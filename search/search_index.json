{"config":{"lang":["es","en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CURSO COMPLETO DE BIG DATA","text":"<p> \"Sin experiencia no hay conocimiento\" </p>"},{"location":"#demos-en-vivo","title":"Demos en Vivo","text":"Observatorio Sismico Global <p>Sismos en tiempo real desde USGS API. Mapa interactivo, filtros por magnitud, alertas de tsunami.</p> <p> Ver en Vivo </p> ISS Tracker <p>Rastrea la Estacion Espacial Internacional en tiempo real. Predictor de pases sobre tu ciudad.</p> <p> Ver en Vivo </p> <p> Estos dashboards se actualizan automaticamente con datos reales de APIs publicas </p>"},{"location":"#el-curso-en-numeros","title":"El Curso en Numeros","text":"230 Horas de contenido 9 Modulos completos 25+ Ejercicios practicos 12+ Dashboards interactivos 30+ Tecnologias"},{"location":"#stack-tecnologico-completo","title":"Stack Tecnologico Completo","text":""},{"location":"#bases-de-datos","title":"Bases de Datos","text":"Tecnologia Nivel Que Aprenderas SQLite Basico Queries SQL, indices, optimizacion PostgreSQL Intermedio Joins complejos, Window Functions, CTEs Oracle Avanzado PL/SQL, procedimientos almacenados DynamoDB Avanzado NoSQL, key-value, serverless"},{"location":"#procesamiento-de-datos","title":"Procesamiento de Datos","text":"Tecnologia Cuando Usarla Escala Pandas Analisis exploratorio &lt; 5 GB Dask Datasets grandes, 1 maquina 5-100 GB Apache Spark Clusters, produccion &gt; 100 GB Spark Streaming Datos en tiempo real Ilimitado"},{"location":"#streaming-y-mensajeria","title":"Streaming y Mensajeria","text":"Tecnologia Proposito Apache Kafka Streaming distribuido, KRaft mode Spark Structured Streaming Procesamiento de streams AWS Kinesis Streaming en la nube"},{"location":"#cloud-e-infraestructura","title":"Cloud e Infraestructura","text":"Tecnologia Que Hace Docker Contenedores, ambientes reproducibles Docker Compose Orquestacion multi-contenedor LocalStack Simulacion AWS local (gratis) Terraform Infraestructura como Codigo AWS S3 Almacenamiento de objetos AWS Lambda Funciones serverless EventBridge Programacion de tareas"},{"location":"#machine-learning-e-ia","title":"Machine Learning e IA","text":"Tecnologia Aplicacion Scikit-learn ML clasico, clustering, clasificacion PCA Reduccion de dimensionalidad K-Means Segmentacion, clustering TensorFlow Deep Learning, redes neuronales MobileNetV2 Transfer Learning, Computer Vision ARIMA/SARIMA Series temporales, forecasting"},{"location":"#nlp-y-text-mining","title":"NLP y Text Mining","text":"Tecnologia Uso NLTK Procesamiento de lenguaje natural TF-IDF Vectorizacion de texto Sentiment Analysis Analisis de sentimiento Jaccard Similarity Similitud entre documentos"},{"location":"#visualizacion","title":"Visualizacion","text":"Tecnologia Tipo Plotly Dashboards interactivos Matplotlib Graficos estaticos Seaborn Visualizacion estadistica Leaflet.js Mapas interactivos Altair Graficos declarativos"},{"location":"#econometria","title":"Econometria","text":"Tecnologia Aplicacion linearmodels Datos de panel Panel OLS Efectos fijos y aleatorios Hausman Test Seleccion de modelo"},{"location":"#modulos-del-curso","title":"Modulos del Curso","text":""},{"location":"#modulo-1-bases-de-datos","title":"Modulo 1: Bases de Datos","text":"<p>SQLite, PostgreSQL, Oracle, migraciones</p> <p>Desde tu primera query SELECT hasta procedimientos almacenados en Oracle. Aprenderas a disenar esquemas, optimizar consultas y migrar entre motores.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-2-limpieza-de-datos-y-etl","title":"Modulo 2: Limpieza de Datos y ETL","text":"<p>Pipeline ETL profesional, QoG Dataset, PostgreSQL</p> <p>Construye un pipeline ETL modular que procesa el dataset Quality of Government (1,289 variables, 194+ paises). Limpieza, transformacion y carga en PostgreSQL.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-3-procesamiento-distribuido","title":"Modulo 3: Procesamiento Distribuido","text":"<p>Dask, Parquet, Cluster Local</p> <p>Procesa datasets grandes sin necesidad de un cluster. Dask te permite escalar pandas a datos que no caben en memoria, usando Parquet y paralelismo local.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-4-machine-learning","title":"Modulo 4: Machine Learning","text":"<p>PCA, K-Means, Transfer Learning, ARIMA/SARIMA</p> <p>Reduccion de dimensionalidad, clustering, clasificacion de imagenes con TensorFlow y series temporales con metodologia Box-Jenkins. Todo con datasets reales.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-5-nlp-y-text-mining","title":"Modulo 5: NLP y Text Mining","text":"<p>NLTK, TF-IDF, Jaccard, Sentiment Analysis</p> <p>Tokenizacion, limpieza de texto, similitud entre documentos, analisis de sentimiento y vectorizacion con TF-IDF.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-6-analisis-de-datos-de-panel","title":"Modulo 6: Analisis de Datos de Panel","text":"<p>Efectos Fijos, Efectos Aleatorios, Hausman Test</p> <p>Analiza datos longitudinales (pais x ano). Replica estudios academicos reales sobre leyes de armas y mortalidad de trafico.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-7-infraestructura-big-data","title":"Modulo 7: Infraestructura Big Data","text":"<p>Docker, Docker Compose, Apache Spark, Cluster Computing</p> <p>Entiende como se construye la infraestructura. Contenedores, orquestacion con Docker Compose, clusters Spark con arquitectura Master-Worker. La base para el Trabajo Final.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-8-streaming-con-kafka","title":"Modulo 8: Streaming con Kafka","text":"<p>Apache Kafka, KRaft, Spark Structured Streaming</p> <p>Streaming en tiempo real con Kafka (modo KRaft, sin ZooKeeper). Productores, consumidores, Spark Structured Streaming y sistema de alertas sismicas.</p> <p>Ver Ejercicios</p>"},{"location":"#modulo-9-cloud-con-localstack","title":"Modulo 9: Cloud con LocalStack","text":"<p>LocalStack, Terraform, AWS (S3, Lambda, DynamoDB)</p> <p>Simula AWS en tu maquina sin costos. Infraestructura como Codigo con Terraform, funciones Lambda serverless y arquitectura Data Lake.</p> <p>Ver Ejercicios</p>"},{"location":"#trabajo-final","title":"Trabajo Final","text":"<p>Docker + Spark + PostgreSQL + Analisis Completo</p> <p>Integra todo lo aprendido en un proyecto de principio a fin. Infraestructura con Docker, ETL con Spark, analisis con tu pregunta de investigacion.</p> <p>Ver Enunciado</p>"},{"location":"#galeria-de-dashboards","title":"Galeria de Dashboards","text":"<p>Todos estos dashboards fueron creados durante el curso:</p> ARIMA PRO Series temporales estilo Bloomberg Ver Dashboard PCA + K-Means Clustering y reduccion dimensional Ver Dashboard Transfer Learning Clasificacion de flores con CNN Ver Dashboard Panel Data QoG Spark + PostgreSQL + ML Ver Dashboard <p>Ver Todos los Dashboards</p>"},{"location":"#para-quien-es-este-curso","title":"Para Quien es Este Curso?","text":"Estudiantes y AutodidactasProfesionales en ActivoEmpresas <ul> <li>Todo el contenido es gratuito y open source</li> <li>Aprende a tu ritmo con ejercicios progresivos</li> <li>Construye un portfolio profesional de proyectos</li> <li>Dashboards reales que puedes mostrar en entrevistas</li> </ul> <ul> <li>Actualiza tus skills a tecnologias modernas</li> <li>De Excel a Spark en semanas, no anos</li> <li>Streaming, Cloud, ML - todo en un solo curso</li> <li>Aplicable inmediatamente en tu trabajo</li> </ul> <ul> <li>Capacitacion in-company disponible</li> <li>Material probado en 230+ horas de clase presencial</li> <li>Casos de uso reales de la industria</li> <li>Consultoria para proyectos especificos</li> </ul>"},{"location":"#como-empezar","title":"Como Empezar","text":"<p>Alumnos del Curso Presencial</p> <p>Lee primero la Guia de Entregas para saber como entregar tus trabajos.</p>"},{"location":"#paso-1-fork-y-clone","title":"Paso 1: Fork y Clone","text":"<pre><code># Haz fork en GitHub (boton arriba a la derecha)\n# Luego clona TU fork:\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\ncd ejercicios-bigdata\n</code></pre>"},{"location":"#paso-2-instala-dependencias","title":"Paso 2: Instala Dependencias","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#paso-3-elige-tu-camino","title":"Paso 3: Elige tu Camino","text":"Si eres... Empieza con... Principiante Ejercicio 1.1: SQLite Intermedio Pipeline ETL QoG Avanzado Streaming con Kafka"},{"location":"#instructor","title":"Instructor","text":"Juan Marcelo Gutierrez Miranda @TodoEconometria 10+ anos en analisis de datos y Big Data. He formado a cientos de profesionales en empresas de toda Latinoamerica y Espana."},{"location":"#servicios-profesionales","title":"Servicios Profesionales","text":"<ul> <li>Capacitacion In-Company: Cursos adaptados a tu equipo y tecnologias</li> <li>Consultoria Big Data: Diseno de pipelines, arquitectura de datos</li> <li>Desarrollo de Dashboards: Visualizaciones interactivas para tu negocio</li> </ul> <p>Contacto:</p> <ul> <li>Email: cursos@todoeconometria.com</li> <li>LinkedIn: Juan Gutierrez</li> <li>Web: www.todoeconometria.com</li> </ul>"},{"location":"#contribuciones","title":"ContribucionesTu Carrera en Big Data Empieza Aqui","text":"<p>Este repositorio es open source. Si encuentras errores o quieres contribuir:</p> <ol> <li>Haz fork del repositorio</li> <li>Crea una rama para tu cambio</li> <li>Envia un Pull Request</li> </ol> <p>230 horas de contenido, 30+ tecnologias, dashboards en tiempo real</p> Comenzar Ahora <p> Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c </p>"},{"location":"faq/","title":"Preguntas Frecuentes (FAQ)","text":"<p>Respuestas a las preguntas mas comunes sobre el curso.</p>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#necesito-experiencia-previa-en-big-data","title":"Necesito experiencia previa en Big Data?","text":"<p>No. El curso empieza desde cero. Solo necesitas:</p> <ul> <li>Conocimientos basicos de Python</li> <li>Saber usar la terminal/consola</li> <li>Ganas de aprender</li> </ul> <p>No tienes Python?</p> <p>Ve a la Guia de Instalacion donde te explicamos como instalar todo desde cero.</p>"},{"location":"faq/#cuanto-tiempo-toma-completar-los-ejercicios","title":"Cuanto tiempo toma completar los ejercicios?","text":"<p>Depende de tu nivel:</p> Nivel Tiempo Total Horas/Semana Duracion Principiante 120-140 horas 10-15h 10-12 semanas Intermedio 60-80 horas 8-10h 6-8 semanas Avanzado 40-50 horas 5-8h 4-5 semanas <p>No hay prisa. Aprende a tu ritmo. Lo importante es entender bien cada concepto.</p>"},{"location":"faq/#los-datos-son-reales-o-sinteticos","title":"Los datos son reales o sinteticos?","text":"<p>Reales. Usamos datasets publicos reales:</p> <ul> <li>NYC Taxi &amp; Limousine Commission (TLC)</li> <li>Weather data de NOAA</li> <li>Otros datasets publicos de Kaggle</li> </ul> <p>Esto te da experiencia con datos del mundo real (sucios, incompletos, grandes).</p>"},{"location":"faq/#puedo-usar-esto-en-mi-portafolio","title":"Puedo usar esto en mi portafolio?","text":"<p>Si! De hecho, te lo recomendamos.</p> <p>Muchos alumnos han conseguido trabajo mostrando:</p> <ul> <li>Sus soluciones de los ejercicios</li> <li>El dashboard que crearon</li> <li>Su fork de GitHub con commits profesionales</li> </ul> <p>Consejo</p> <p>Haz tu fork publico y agrega un README personalizado explicando tu aprendizaje.</p>"},{"location":"faq/#hay-certificado-al-terminar","title":"Hay certificado al terminar?","text":"<p>Para alumnos del curso presencial: Si, certificado de 230 horas.</p> <p>Para autodidactas: No hay certificado oficial, pero tu GitHub es tu certificado. Los empleadores valoran mas ver tu codigo que un PDF.</p> <p>Tu GitHub es tu Certificado</p> <ul> <li>Commits profesionales</li> <li>Codigo bien documentado</li> <li>Proyectos completos</li> <li>Contribuciones activas</li> </ul>"},{"location":"faq/#tecnico","title":"Tecnico","text":""},{"location":"faq/#que-computadora-necesito","title":"Que computadora necesito?","text":"<p>Minimo:</p> <ul> <li>8GB RAM</li> <li>20GB espacio en disco</li> <li>Procesador i5 o equivalente</li> <li>Windows 10+, macOS 10.14+, o Linux (Ubuntu 20.04+)</li> </ul> <p>Recomendado:</p> <ul> <li>16GB RAM</li> <li>50GB espacio en disco SSD</li> <li>Procesador i7 o equivalente</li> </ul> <p>No tienes buenos recursos?</p> <p>Puedes usar Google Colab o GitHub Codespaces (gratis) para trabajar en la nube.</p>"},{"location":"faq/#funciona-en-windowsmaclinux","title":"Funciona en Windows/Mac/Linux?","text":"<p>Si. El curso es compatible con los tres sistemas operativos.</p> <ul> <li>Windows: Preferiblemente Windows 10 o superior</li> <li>macOS: macOS 10.14 (Mojave) o superior</li> <li>Linux: Ubuntu 20.04+, Fedora, Arch, etc.</li> </ul> <p>La Guia de Instalacion tiene instrucciones especificas para cada sistema.</p>"},{"location":"faq/#puedo-usar-otro-ide-en-lugar-de-pycharm","title":"Puedo usar otro IDE en lugar de PyCharm?","text":"<p>Si. PyCharm es recomendado pero no obligatorio.</p> <p>Alternativas:</p> <ul> <li>Visual Studio Code - Ligero y muy popular</li> <li>Jupyter Lab - Excelente para notebooks</li> <li>Sublime Text - Editor de texto avanzado</li> <li>Vim/Emacs - Si eres usuario avanzado</li> </ul> <p>Lo importante es que te sientas comodo con tu herramienta.</p>"},{"location":"faq/#como-descargo-los-datos","title":"Como descargo los datos?","text":"<p>Los datos se descargan automaticamente con un script:</p> <pre><code># Ir a la carpeta de datos\ncd datos\n\n# Ejecutar script de descarga\npython descargar_datos.py\n</code></pre> <p>El script descarga y descomprime automaticamente todos los datasets necesarios.</p> <p>Espacio en disco</p> <p>Los datasets completos ocupan ~5GB. Asegurate de tener suficiente espacio.</p>"},{"location":"faq/#git-y-github","title":"Git y GitHub","text":""},{"location":"faq/#nunca-he-usado-git-es-muy-dificil","title":"Nunca he usado Git. Es muy dificil?","text":"<p>No es dificil, pero requiere practica.</p> <p>Tenemos guias completas paso a paso:</p> <ol> <li>Fork y Clone - Lo mas basico</li> <li>Tu Primer Ejercicio - Workflow completo</li> <li>Comandos Utiles - Referencia rapida</li> </ol> <p>Aprende haciendo</p> <p>La mejor forma de aprender Git es usandolo. Los primeros commits seran raros, pero mejoras rapido.</p>"},{"location":"faq/#que-es-un-fork-por-que-necesito-uno","title":"Que es un Fork? Por que necesito uno?","text":"<p>Fork = Tu copia personal del repositorio en GitHub.</p> <p>Lo necesitas porque:</p> <ul> <li> No puedes modificar el repositorio del profesor directamente</li> <li> El fork es TU espacio para trabajar</li> <li> Puedes sincronizarlo con el original</li> <li> El sistema evalua automaticamente tus PROMPTS en tu fork</li> </ul> <p>Ver guia completa: Fork y Clone</p>"},{"location":"faq/#como-mantengo-mi-fork-actualizado","title":"Como mantengo mi Fork actualizado?","text":"<p>Cuando el profesor agregue ejercicios nuevos, debes sincronizar tu fork.</p> <p>Metodo facil (GitHub Web):</p> <ol> <li>Ve a tu fork en GitHub</li> <li>Click \"Sync fork\" \u2192 \"Update branch\"</li> <li>En tu PC: <code>git pull origin main</code></li> </ol> <p>Metodo completo (Terminal):</p> <pre><code>git checkout main\ngit fetch upstream\ngit merge upstream/main\ngit push origin main\n</code></pre> <p>Ver guia completa: Sincronizar Fork</p>"},{"location":"faq/#hice-un-commit-mal-como-lo-deshago","title":"Hice un commit mal. Como lo deshago?","text":"<p>Antes de hacer push:</p> <pre><code># Deshacer ultimo commit (mantiene cambios)\ngit reset --soft HEAD~1\n\n# Deshacer ultimo commit (descarta cambios)\ngit reset --hard HEAD~1\n</code></pre> <p>Despues de hacer push:</p> <pre><code># Crear nuevo commit que revierte el anterior\ngit revert HEAD\ngit push origin tu-rama\n</code></pre> <p>Evita force push</p> <p>Nunca uses <code>git push --force</code> en ramas compartidas o Pull Requests abiertos.</p>"},{"location":"faq/#ejercicios","title":"Ejercicios","text":""},{"location":"faq/#no-puedo-completar-un-ejercicio-que-hago","title":"No puedo completar un ejercicio. Que hago?","text":"<p>Paso 1: Lee el error cuidadosamente</p> <p>La mayoria de las veces el error te dice exactamente que esta mal.</p> <p>Paso 2: Busca en Google</p> <p>Copia el mensaje de error y buscalo. Probablemente alguien mas ya lo tuvo.</p> <p>Paso 3: Revisa la documentacion</p> <ul> <li>Pandas Docs</li> <li>SQLite Tutorial</li> <li>Python Docs</li> </ul> <p>Paso 4: Pide ayuda</p> <ul> <li>Alumnos presenciales: Consulta en clase</li> <li>Autodidactas: Crea un Issue en GitHub explicando tu problema</li> </ul> <p>Como pedir ayuda</p> <p>Incluye:</p> <ul> <li>Que intentaste hacer</li> <li>Que error obtuviste (mensaje completo)</li> <li>Que ya probaste</li> <li>Tu codigo relevante</li> </ul>"},{"location":"faq/#puedo-hacer-los-ejercicios-en-desorden","title":"Puedo hacer los ejercicios en desorden?","text":"<p>No recomendado. Los ejercicios estan disenados para:</p> <ul> <li>Construir sobre conocimientos previos</li> <li>Aumentar dificultad gradualmente</li> <li>Introducir conceptos en orden logico</li> </ul> <p>Excepcion</p> <p>Si ya tienes experiencia con Python y Pandas, puedes empezar en el NIVEL 2 (Ejercicio 03).</p>"},{"location":"faq/#cuantas-veces-puedo-intentar-un-ejercicio","title":"Cuantas veces puedo intentar un ejercicio?","text":"<p>Las que necesites. No hay limite de intentos.</p> <p>El objetivo es aprender, no aprobar rapidamente.</p> <ul> <li>Puedes actualizar tu fork cuantas veces quieras con <code>git push</code></li> <li>El sistema de evaluacion automatica revisa tu archivo PROMPTS.md</li> <li>Aprendes mas de los errores que de los aciertos</li> </ul>"},{"location":"faq/#puedo-usar-librerias-adicionales","title":"Puedo usar librerias adicionales?","text":"<p>Si, pero:</p> <ol> <li>Justifica por que las necesitas</li> <li>Agregalas a <code>requirements.txt</code></li> <li>Documenta como instalarlas</li> <li>Menciona en PROMPTS.md que librerias usaste</li> </ol> <p>Ejemplo</p> <p>Si usas <code>seaborn</code> para visualizaciones:</p> <pre><code># requirements.txt\npandas==2.0.0\nseaborn==0.12.0  # Para visualizaciones avanzadas\n</code></pre> <p>Y en tu archivo <code>PROMPTS.md</code> menciona: \"Use seaborn para crear graficos mas profesionales\"</p>"},{"location":"faq/#soporte","title":"Soporte","text":""},{"location":"faq/#ofrecen-soporte-si-me-atoro","title":"Ofrecen soporte si me atoro?","text":"<p>Para alumnos del curso presencial:</p> <ul> <li> Soporte completo en las sesiones</li> <li> Consultas por email</li> <li> Revision automatica de entregas</li> </ul> <p>Para autodidactas:</p> <ul> <li> No hay soporte directo</li> <li> Puedes crear Issues en GitHub</li> <li> Comunidad puede ayudarte</li> <li> Documentacion completa disponible</li> </ul>"},{"location":"faq/#como-contacto-al-instructor","title":"Como contacto al instructor?","text":"<p>Para consultas del curso:</p> <ul> <li>GitHub Issues: Crear Issue</li> <li>Email: cursos@todoeconometria.com</li> </ul> <p>Para consultoria empresarial:</p> <ul> <li>Email: cursos@todoeconometria.com</li> <li>LinkedIn: Juan Gutierrez</li> <li>Web: TodoEconometria</li> </ul> <p>Tiempo de respuesta</p> <ul> <li>Alumnos presenciales: 24-48 horas</li> <li>Autodidactas via Issues: Cuando este disponible</li> <li>Empresas: 24 horas</li> </ul>"},{"location":"faq/#problemas-tecnicos","title":"Problemas Tecnicos","text":""},{"location":"faq/#python-no-se-reconoce-como-comando","title":"Python no se reconoce como comando","text":"<p>Windows:</p> <ol> <li>Reinstala Python</li> <li>Marca \"Add Python to PATH\"</li> <li>Reinicia la terminal</li> </ol> <p>macOS/Linux:</p> <p>Usa <code>python3</code> en lugar de <code>python</code>:</p> <pre><code>python3 --version\npip3 install pandas\n</code></pre>"},{"location":"faq/#error-modulenotfounderror","title":"Error: ModuleNotFoundError","text":"<p>Causa: No instalaste las dependencias.</p> <p>Solucion:</p> <pre><code># Activa el entorno virtual\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\n\n# Instala dependencias\npip install -r requirements.txt\n</code></pre>"},{"location":"faq/#git-dice-fatal-not-a-git-repository","title":"Git dice \"fatal: not a git repository\"","text":"<p>Causa: No estas en la carpeta del proyecto.</p> <p>Solucion:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica\ngit status  # Deberia funcionar\n</code></pre>"},{"location":"faq/#no-puedo-hacer-push-permission-denied","title":"No puedo hacer push: \"Permission denied\"","text":"<p>Causa: Problemas de autenticacion con GitHub.</p> <p>Solucion rapida (HTTPS):</p> <pre><code># Cambiar a HTTPS\ngit remote set-url origin https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Intentar push de nuevo\ngit push origin tu-rama\n</code></pre> <p>Solucion permanente (SSH):</p> <p>Configura SSH keys: GitHub SSH Guide</p>"},{"location":"faq/#el-dashboard-no-carga-los-datos","title":"El dashboard no carga los datos","text":"<p>Verificar:</p> <ol> <li> <p>La base de datos existe?    <pre><code>ls datos/taxi.db\n</code></pre></p> </li> <li> <p>Flask esta corriendo?    <pre><code>python app.py\n</code></pre></p> </li> <li> <p>Puerto correcto? (default: 5000)    <pre><code>http://localhost:5000\n</code></pre></p> </li> <li> <p>Revisar consola de errores del navegador (F12)</p> </li> </ol>"},{"location":"faq/#carrera-y-empleo","title":"Carrera y Empleo","text":""},{"location":"faq/#este-curso-me-ayudara-a-conseguir-trabajo","title":"Este curso me ayudara a conseguir trabajo?","text":"<p>Puede ayudar mucho, especialmente si:</p> <ul> <li>Completas todos los ejercicios con calidad</li> <li>Creas un dashboard profesional</li> <li>Documentas bien tu codigo</li> <li>Mantienes un GitHub activo</li> </ul> <p>Que valoran los empleadores</p> <ol> <li>Portfolio de proyectos (tu GitHub)</li> <li>Codigo limpio y documentado</li> <li>Experiencia con datos reales</li> <li>Capacidad de resolver problemas</li> </ol> <p>Un repositorio bien trabajado vale mas que 10 certificados.</p>"},{"location":"faq/#que-trabajos-puedo-conseguir-con-estas-habilidades","title":"Que trabajos puedo conseguir con estas habilidades?","text":"<p>Con las habilidades del curso puedes aplicar a:</p> <ul> <li>Data Analyst - Analisis de datos con Python/SQL</li> <li>Junior Data Scientist - Modelado y analisis avanzado</li> <li>Data Engineer - Pipelines ETL, procesamiento de datos</li> <li>Business Intelligence Developer - Dashboards y reportes</li> <li>Python Developer - Desarrollo backend con datos</li> </ul>"},{"location":"faq/#necesito-un-titulo-universitario","title":"Necesito un titulo universitario?","text":"<p>Depende del empleador.</p> <ul> <li>Empresas tech: Valoran mas el portfolio que el titulo</li> <li>Empresas tradicionales: Pueden requerir titulo</li> <li>Startups: Portfolio &gt; Titulo</li> <li>Freelance: Solo importa tu trabajo</li> </ul> <p>Compensar falta de titulo</p> <ul> <li>Portfolio solido en GitHub</li> <li>Certificaciones relevantes</li> <li>Proyectos personales impresionantes</li> <li>Contribuciones open source</li> </ul>"},{"location":"faq/#otros","title":"Otros","text":""},{"location":"faq/#puedo-compartir-mi-solucion-publicamente","title":"Puedo compartir mi solucion publicamente?","text":"<p>Si, pero considera:</p> <ul> <li>Despues de completar: Compartir despues de recibir tu nota</li> <li>Con creditos: Menciona que es del curso de TodoEconometria</li> <li>No spoilers: No compartas soluciones para ayudar a hacer trampa</li> </ul> <p>Compartir es bueno</p> <p>Compartir tu codigo ayuda a:</p> <ul> <li>Otros a aprender</li> <li>Construir tu marca personal</li> <li>Demostrar habilidades a empleadores</li> </ul>"},{"location":"faq/#el-curso-se-actualiza","title":"El curso se actualiza?","text":"<p>Si. El repositorio se actualiza regularmente con:</p> <ul> <li>Nuevos ejercicios</li> <li>Mejoras en los existentes</li> <li>Actualizaciones de librerias</li> <li>Nuevos datasets</li> <li>Correcciones de bugs</li> </ul> <p>Mant\u00e9n tu fork sincronizado para obtener actualizaciones: Sincronizar Fork</p>"},{"location":"faq/#puedo-contribuir-al-curso","title":"Puedo contribuir al curso?","text":"<p>Si! Las contribuciones son bienvenidas.</p> <p>Puedes contribuir:</p> <ul> <li> Reportando bugs via Issues</li> <li> Mejorando documentacion</li> <li> Sugiriendo nuevos ejercicios</li> <li> Compartiendo tu dashboard en la galeria</li> </ul> <p>Ver seccion de Contribuciones</p>"},{"location":"faq/#donde-puedo-aprender-mas","title":"Donde puedo aprender mas?","text":"<p>Recursos recomendados:</p> <ul> <li>Python for Data Analysis - Libro de Wes McKinney</li> <li>SQL Tutorial - SQL interactivo</li> <li>Dask Tutorial - Tutorial oficial de Dask</li> <li>r/datascience - Comunidad en Reddit</li> </ul> <p>Cursos complementarios:</p> <ul> <li>Python for Data Science (Coursera)</li> <li>SQL for Data Science (DataCamp)</li> <li>Apache Spark (Udacity)</li> </ul>"},{"location":"faq/#preguntas-no-resueltas","title":"Preguntas no Resueltas?","text":"<p>No encontraste tu respuesta?</p> <p>Alumnos presenciales: Consulta en la proxima sesion o envia un email</p> <p>Autodidactas: Crea un Issue en GitHub:</p> <p>Crear Issue</p> <p>Incluye: - Titulo descriptivo - Descripcion detallada de tu pregunta - Contexto (que ejercicio, que intentaste, etc.)</p>"},{"location":"faq/#recursos-adicionales","title":"Recursos Adicionales","text":"<ul> <li>Guia de Inicio - Empezar desde cero</li> <li>Git y GitHub - Guias de Git</li> <li>Ejercicios - Lista de ejercicios</li> <li>Roadmap - Plan de estudio</li> </ul>"},{"location":"infraestructura/","title":"\ud83c\udfd7\ufe0f Infraestructura del Laboratorio","text":"<p>\"Evita que Big Data colapse tu disco principal\"</p> <p>Para realizar este curso, hemos dise\u00f1ado una arquitectura h\u00edbrida que combina la facilidad de uso de Windows con la potencia de servidores Linux aislados en Docker.</p> <p>Esta gu\u00eda te explica qu\u00e9 est\u00e1 pasando \"bajo el cap\u00f3\" cuando ejecutas el script de instalaci\u00f3n.</p>"},{"location":"infraestructura/#arquitectura-hibrida","title":"\ud83e\udde9 Arquitectura H\u00edbrida","text":"<p>Nuestro entorno utiliza tres capas:</p> <ol> <li>Capa de Usuario (T\u00fa): Escribes c\u00f3digo Python en PyCharm/VSCode.</li> <li>Capa de C\u00f3mputo (Docker): Servicios pesados (Spark, Postgres) corren aislados.</li> <li>Capa de Datos (Storage): Los archivos masivos se guardan en un disco dedicado.</li> </ol> <pre><code>graph TD\n    subgraph \"Tu Ordenador\"\n        IDE[PyCharm / VSCode] --&gt;|Ejecuta| PY[Scripts Python]\n        PY --&gt;|Conecta puerto 5432| PG[(PostgreSQL)]\n        PY --&gt;|Conecta puerto 7077| SPARK[Spark Cluster]\n    end\n\n    subgraph \"Docker (Contenedores)\"\n        PG\n        SPARK\n        ADMIN[pgAdmin4]\n    end\n\n    subgraph \"Almacenamiento F\u00edsico\"\n        SSD[SSD Externo / Partici\u00f3n Datos]\n        HDD[Disco Sistema C:]\n    end\n\n    PG --&gt;|Guarda| SSD\n    SPARK --&gt;|Lee| SSD\n    PY -.-&gt;|Junction Link| SSD</code></pre>"},{"location":"infraestructura/#el-dilema-del-almacenamiento-hdd-vs-ssd","title":"\ud83d\udcbe El Dilema del Almacenamiento (HDD vs SSD)","text":"<p>En Proyectos de Datos, el I/O (Velocidad de disco) es cr\u00edtico.</p>"},{"location":"infraestructura/#opcion-a-ssd-externo-modo-pro","title":"\ud83d\ude80 Opci\u00f3n A: SSD Externo (Modo PRO)","text":"<p>Si tienes el laboratorio configurado en un SSD externo (<code>E:\\BIGDATA_LAB_STORAGE</code>): 1.  Velocidad: Docker tiene un carril exclusivo para leer/escribir datos. 2.  Seguridad: Si descargas un dataset de 50GB, tu Windows no se queda sin espacio. 3.  El \"T\u00fanel M\u00e1gico\" (Junction): Windows crea un enlace simb\u00f3lico. T\u00fa ves la carpeta <code>datos/</code> en tu proyecto, pero f\u00edsicamente los bytes est\u00e1n en el SSD externo.</p>"},{"location":"infraestructura/#opcion-b-disco-local","title":"\ud83d\udc22 Opci\u00f3n B: Disco Local","text":"<p>Si usas tu disco principal (<code>C:</code>): - El sistema funcionar\u00e1 igual, pero debes vigilar el espacio libre. - Docker y Windows competir\u00e1n por el uso del disco (performance menor).</p>"},{"location":"infraestructura/#servicios-docker-el-stack","title":"\ud83d\udc33 Servicios Docker (El \"Stack\")","text":"<p>El script <code>setup_cluster.ps1</code> levanta estos servicios autom\u00e1ticamente:</p>"},{"location":"infraestructura/#1-postgresql-el-data-warehouse","title":"1. PostgreSQL (El Data Warehouse)","text":"<ul> <li>Por qu\u00e9 SQL: Aunque procesamos con Spark/Dask, los datos finales \"de valor\" deben residir en una base de datos estructurada para ser consumidos por Dashboards o Analistas.</li> <li>Por qu\u00e9 no Mongo/Cassandra: Para este nivel de datos (Gigabytes estructurados), PostgreSQL es m\u00e1s eficiente en recursos locales y es el est\u00e1ndar de oro para la capa de \"Serving\".</li> <li>Acceso: <code>localhost:5432</code></li> </ul>"},{"location":"infraestructura/#2-apache-spark-el-motor-de-proceso","title":"2. Apache Spark (El Motor de Proceso)","text":"<ul> <li>Master + Worker: Simulamos un cluster real. El \"Master\" reparte tareas y el \"Worker\" las ejecuta.</li> <li>Acceso: <code>localhost:8081</code> (Dashboard)</li> </ul>"},{"location":"infraestructura/#3-pgadmin-4-la-interfaz","title":"3. pgAdmin 4 (La Interfaz)","text":"<ul> <li>Una web para gestionar tu base de datos SQL sin comandos.</li> <li>Acceso: <code>http://localhost:8080</code></li> </ul>"},{"location":"infraestructura/#docker-rescue","title":"\ud83d\ude91 Docker Rescue","text":"<p>\u00bfTe ha pasado que Docker \"no arranca\"? El script de setup incluye un m\u00f3dulo de rescate: 1.  Verifica si el servicio de Windows <code>com.docker.service</code> est\u00e1 corriendo. 2.  Si est\u00e1 detenido, intenta reiniciarlo con permisos de administrador. 3.  Espera a que el motor est\u00e9 listo antes de intentar levantar los contenedores.</p>"},{"location":"infraestructura/#como-iniciar","title":"\ud83d\udee0\ufe0f C\u00f3mo Iniciar","text":"<p>Para levantar el stack de Docker, abre PowerShell como Administrador y ejecuta:</p> <pre><code># Navegar a la carpeta del proyecto\ncd \"C:\\Users\\TU_USUARIO\\Documents\\ejercicios_bigdata\"\n\n# Levantar los servicios\ndocker-compose up -d\n\n# Verificar que est\u00e1n corriendo\ndocker ps\n</code></pre> <p>Servicios disponibles despu\u00e9s de iniciar: - PostgreSQL: <code>localhost:5432</code> - Spark Master UI: <code>localhost:8081</code> - pgAdmin: <code>localhost:8080</code></p>"},{"location":"dashboards/","title":"Galeria de Visualizaciones y Dashboards","text":"<p>Resultados de los algoritmos de Machine Learning y NLP del curso. Cada pagina incluye analisis, graficos y codigo fuente.</p>"},{"location":"dashboards/#machine-learning","title":"Machine Learning","text":""},{"location":"dashboards/#analisis-completos","title":"Analisis completos","text":"<ul> <li>PCA + Clustering K-Means: Dataset Iris -- Reduccion dimensional y agrupamiento sobre el dataset clasico de Fisher</li> <li>Manual PCA estilo FactoMineR -- Replicando el estandar de oro del analisis multivariante en Python</li> </ul>"},{"location":"dashboards/#series-temporales","title":"Series Temporales","text":"<ul> <li>ARIMA/SARIMA - Metodologia Box-Jenkins -- Identificacion, estimacion, diagnostico y pronostico</li> <li>Dashboard ARIMA (interactivo) -- 6 pestanas: serie, descomposicion, ACF/PACF, diagnostico, forecast, radar</li> <li>Dashboard ARIMA PRO -- Tema financiero tipo Bloomberg, KPIs, 7 pestanas, comparativa de modelos</li> <li>Dashboard ARIMA PRO (interactivo) -- Diseno inspirado en OECD Explorer y Portfolio Optimizer</li> </ul>"},{"location":"dashboards/#computer-vision-transfer-learning","title":"Computer Vision (Transfer Learning)","text":"<ul> <li>Clasificacion de Flores con Transfer Learning -- MobileNetV2 + ML tradicional sobre embeddings</li> <li>Dashboard Flores (interactivo) -- t-SNE, comparativa modelos, confusion matrix, radar</li> </ul>"},{"location":"dashboards/#dashboards-interactivos-html","title":"Dashboards interactivos (HTML)","text":"<ul> <li>Dashboard PCA Iris (interactivo) -- Visualizacion completa en pantalla</li> <li>Dashboard FactoMineR (interactivo) -- Circulos de correlacion y biplots</li> </ul>"},{"location":"dashboards/#nlp-y-text-mining","title":"NLP y Text Mining","text":""},{"location":"dashboards/#ejercicios-guiados","title":"Ejercicios guiados","text":"<ul> <li>Ejercicio 01: Anatomia del Texto y Conteo de Palabras -- Frecuencia, tokenizacion y visualizacion</li> <li>Ejercicio 02: Filtro de Stopwords -- Limpieza de ruido semantico</li> </ul>"},{"location":"dashboards/#analisis-completos_1","title":"Analisis completos","text":"<ul> <li>Similitud de Jaccard -- Comparacion de documentos y sistema de recomendacion</li> <li>Vectorizacion y Clustering de Documentos -- TF-IDF, K-Means y analisis de topicos</li> </ul>"},{"location":"dashboards/#analisis-de-datos-de-panel-big-data","title":"Analisis de Datos de Panel (Big Data)","text":""},{"location":"dashboards/#pipeline-completo-spark-postgresql-ml","title":"Pipeline completo: Spark + PostgreSQL + ML","text":"<ul> <li>Modulo 06: QoG - 4 Lineas de Investigacion + ML -- Asia Central, Seguridad Hidrica, Terrorismo, Maghreb, PCA + K-Means</li> <li>Dashboard QoG (interactivo) -- 5 pestanas con graficos Plotly interactivos</li> </ul>"},{"location":"dashboards/#streaming-y-cloud-tiempo-real","title":"Streaming y Cloud (Tiempo Real)","text":""},{"location":"dashboards/#dashboards-en-vivo","title":"Dashboards en vivo","text":"<ul> <li>Observatorio Sismico Global -- Sismos en tiempo real desde USGS API</li> <li>Dashboard Sismos (interactivo) -- Mapa Leaflet con datos en vivo, filtros por categoria</li> <li>ISS Tracker -- Seguimiento de la Estacion Espacial Internacional</li> <li>Dashboard ISS (interactivo) -- Posicion en tiempo real, predictor de pases, trayectoria orbital</li> </ul>"},{"location":"dashboards/#tecnologias","title":"Tecnologias","text":"<ul> <li>APIs en vivo: USGS Earthquakes, Where The ISS At, Open Notify</li> <li>Visualizacion: Leaflet.js, CARTO dark tiles, Lucide icons</li> <li>Actualizacion: Auto-refresh cada 5-30 segundos</li> </ul>"},{"location":"dashboards/#codigo-fuente","title":"Codigo fuente","text":"<p>Los scripts que generan estas visualizaciones estan en:</p> <ul> <li><code>ejercicios/04_machine_learning/</code> -- PCA, K-Means, Silhouette</li> <li><code>ejercicios/05_nlp_text_mining/</code> -- Conteo, limpieza, sentimiento, Jaccard</li> <li><code>ejercicios/06_an\u00e1lisis_datos_de_panel/</code> -- Pipeline QoG con Apache Spark</li> <li><code>ejercicios/07_infraestructura_bigdata/</code> -- Docker Compose, Cluster Spark</li> <li><code>ejercicios/08_streaming_kafka/</code> -- Kafka, Spark Structured Streaming</li> <li><code>ejercicios/09_cloud_localstack/</code> -- LocalStack, Terraform, AWS</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/","title":"Manual de PCA en Python: Estilo FactoMineR","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#replicando-el-estandar-de-oro-del-analisis-multivariante","title":"Replicando el Estandar de Oro del Analisis Multivariante","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#informacion-de-certificacion-y-referencia","title":"Informacion de Certificacion y Referencia","text":"<p>Autor original/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Metodologia: Cursos Avanzados de Big Data, Ciencia de Datos, Desarrollo de aplicaciones con IA &amp; Econometria Aplicada. Hash ID de Certificacion: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Repositorio: https://github.com/TodoEconometria/certificaciones</p> <p>REFERENCIA ACADEMICA PRINCIPAL:</p> <ul> <li>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. CRC Press.</li> <li>Tutorial FactoMineR: http://factominer.free.fr/course/FactoTuto.html</li> <li>Le, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: An R Package for Multivariate Analysis. Journal of Statistical Software, 25(1), 1-18.</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#1-introduccion-por-que-factominer-es-el-estandar-de-oro","title":"1. Introduccion: Por que FactoMineR es el Estandar de Oro","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#11-el-problema-de-los-datos-multivariantes","title":"1.1. El Problema de los Datos Multivariantes","text":"<p>Imagina que tienes datos de 41 atletas que compitieron en decatlon (10 pruebas deportivas). Cada atleta tiene 10 mediciones: tiempos en carreras, distancias en saltos y lanzamientos. La pregunta es: Como podemos \"ver\" patrones en 10 dimensiones?</p> <p>El cerebro humano solo puede visualizar 2 o 3 dimensiones. El Analisis de Componentes Principales (PCA) es la herramienta matematica que nos permite \"comprimir\" esas 10 dimensiones en 2, perdiendo la menor cantidad de informacion posible.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#12-filosofia-factominer-vs-scikit-learn","title":"1.2. Filosofia: FactoMineR vs. Scikit-Learn","text":"<p>Este manual replica la logica de FactoMineR (el estandar de oro en R) usando Python.</p> Aspecto Scikit-learn FactoMineR / Prince Enfoque Machine Learning Exploracion de Datos Objetivo Reducir dimensiones para un modelo predictivo Entender que variables mueven a que individuos Metricas clave <code>explained_variance_ratio_</code> Contribuciones, \\(\\cos^2\\), correlaciones Variables suplementarias No soportadas Soporte nativo Graficos Basicos Circulo de correlacion, biplot <p>La diferencia fundamental: Scikit-learn te dice \"estos son tus datos comprimidos\". FactoMineR te dice \"por que tus datos se comprimieron asi, que variables son responsables, y que tan confiable es la representacion de cada punto\".</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2-antes-de-empezar-tratamiento-de-datos-perdidos","title":"2. ANTES DE EMPEZAR: Tratamiento de Datos Perdidos","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#21-el-error-mas-comun-meto-la-media-y-listo","title":"2.1. El Error Mas Comun: \"Meto la Media y Listo\"","text":"<p>Uno de los errores mas frecuentes en analisis multivariante es imputar datos perdidos con la media sin pensar. Esta practica, aunque comun, puede destruir la estructura real de tus datos y llevarte a conclusiones erroneas.</p> <p>REGLA DE ORO: Antes de imputar cualquier valor perdido, debes entender POR QUE falta ese dato. La estrategia de imputacion depende del mecanismo de perdida.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#22-los-tres-tipos-de-datos-perdidos-rubin-1976","title":"2.2. Los Tres Tipos de Datos Perdidos (Rubin, 1976)","text":"Tipo Nombre Significado Ejemplo Se puede usar la media? MCAR Missing Completely At Random El dato falta por puro azar, sin relacion con ninguna variable Un sensor fallo aleatoriamente SI, pero con precaucion MAR Missing At Random El dato falta por una razon relacionada con OTRAS variables observadas Los jovenes no responden preguntas de salario (pero sabemos su edad) DEPENDE del metodo MNAR Missing Not At Random El dato falta por una razon relacionada con EL MISMO valor perdido Los ricos no declaran su patrimonio PORQUE es alto NO, sesga los resultados"},{"location":"dashboards/02_PCA_FactoMineR_style/#23-por-que-la-media-puede-destruir-tu-analisis","title":"2.3. Por que la Media Puede Destruir tu Analisis","text":"<p>Imagina este escenario en el decatlon:</p> <pre><code>Atleta      | 100m  | Salto Largo | Lanzamiento\n------------|-------|-------------|------------\nVelocista A | 10.5s | 7.8m        | ???\nVelocista B | 10.7s | 7.6m        | ???\nLanzador C  | 11.8s | 6.2m        | 18.5m\nLanzador D  | 12.0s | 6.0m        | 19.2m\n</code></pre> <p>Problema: Los velocistas (A y B) no completaron el lanzamiento (por lesion).</p> <p>Si imputas con la media del lanzamiento: - Media de lanzamiento = (18.5 + 19.2) / 2 = 18.85m - Asignas 18.85m a los velocistas A y B</p> <p>CONSECUENCIA CATASTROFICA: - En el PCA, los velocistas aparecen como buenos lanzadores - El circulo de correlacion mostrara que velocidad y lanzamiento estan correlacionados positivamente - ESTO ES FALSO - en realidad, los velocistas suelen ser peores lanzadores</p> <p>El problema: El dato NO falta al azar (MCAR). Falta porque los velocistas se lesionaron en la prueba de lanzamiento (probablemente por falta de tecnica). Es un caso de MAR o MNAR.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#24-como-diagnosticar-el-tipo-de-dato-perdido","title":"2.4. Como Diagnosticar el Tipo de Dato Perdido","text":"<p>Antes de imputar, realiza estos tests de diagnostico:</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#test-1-patron-visual-de-perdida","title":"Test 1: Patron Visual de Perdida","text":"<pre><code>import missingno as msno\nimport matplotlib.pyplot as plt\n\n# Visualizar patron de datos perdidos\nmsno.matrix(df)\nplt.show()\n\n# Visualizar correlacion entre datos perdidos\nmsno.heatmap(df)\nplt.show()\n</code></pre> <p>Interpretacion: - Si los huecos estan dispersos aleatoriamente: probablemente MCAR - Si los huecos se agrupan en filas/columnas especificas: probablemente MAR o MNAR</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#test-2-test-de-little-mcar-test","title":"Test 2: Test de Little (MCAR Test)","text":"<pre><code># Instalar: pip install pyampute\nfrom pyampute.exploration import mcar_test\n\n# Test estadistico para MCAR\nresult = mcar_test(df)\nprint(f\"p-valor: {result}\")\n# Si p &gt; 0.05: No se rechaza MCAR (los datos PUEDEN ser aleatorios)\n# Si p &lt; 0.05: Se rechaza MCAR (los datos NO son aleatorios)\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#test-3-comparar-grupos-datos-completos-vs-incompletos","title":"Test 3: Comparar Grupos (Datos Completos vs Incompletos)","text":"<pre><code># Crear indicador de dato perdido\ndf['tiene_na'] = df['lanzamiento'].isna()\n\n# Comparar medias de otras variables\nprint(df.groupby('tiene_na')['100m'].mean())\nprint(df.groupby('tiene_na')['salto_largo'].mean())\n\n# Si las medias son MUY diferentes, el dato NO es MCAR\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#25-estrategias-de-imputacion-segun-el-tipo","title":"2.5. Estrategias de Imputacion Segun el Tipo","text":"Tipo de Perdida Estrategia Recomendada Estrategia a EVITAR MCAR (aleatorio puro) Media, mediana, KNN, MICE - MAR (depende de otras variables) KNN, MICE, Regresion multiple Media simple (sesga) MNAR (depende del valor perdido) Modelos especificos, Analisis de sensibilidad Media, KNN, MICE (todos sesgan)"},{"location":"dashboards/02_PCA_FactoMineR_style/#26-alternativas-a-la-media-codigo-python","title":"2.6. Alternativas a la Media (Codigo Python)","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-1-eliminar-filas-con-na-solo-si-son-pocas-y-mcar","title":"Opcion 1: Eliminar Filas con NA (solo si son pocas y MCAR)","text":"<pre><code># Solo si tienes POCOS datos perdidos (&lt;5%) y son MCAR\ndf_limpio = df.dropna()\nprint(f\"Filas eliminadas: {len(df) - len(df_limpio)}\")\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-2-imputacion-por-knn-respeta-la-estructura-local","title":"Opcion 2: Imputacion por KNN (respeta la estructura local)","text":"<pre><code>from sklearn.impute import KNNImputer\n\n# KNN busca los K vecinos mas similares y promedia sus valores\nimputer = KNNImputer(n_neighbors=5)\ndf_imputado = pd.DataFrame(\n    imputer.fit_transform(df[columnas_numericas]),\n    columns=columnas_numericas\n)\n</code></pre> <p>Ventaja: Un velocista con NA en lanzamiento recibira el valor de otros velocistas similares, no el promedio global.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-3-imputacion-multiple-mice-el-estandar-de-oro","title":"Opcion 3: Imputacion Multiple (MICE) - El Estandar de Oro","text":"<pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# MICE: Multiple Imputation by Chained Equations\nimputer = IterativeImputer(max_iter=10, random_state=42)\ndf_imputado = pd.DataFrame(\n    imputer.fit_transform(df[columnas_numericas]),\n    columns=columnas_numericas\n)\n</code></pre> <p>Ventaja: Usa TODAS las variables para predecir el valor perdido mediante regresiones iterativas.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#opcion-4-imputacion-por-grupo-cuando-hay-categorias","title":"Opcion 4: Imputacion por Grupo (cuando hay categorias)","text":"<pre><code># Si sabes que el dato depende de una categoria\ndf['lanzamiento'] = df.groupby('tipo_atleta')['lanzamiento'].transform(\n    lambda x: x.fillna(x.mean())\n)\n</code></pre> <p>Ejemplo: Imputar el lanzamiento de un velocista con la media de otros velocistas, no con la media global.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#27-checklist-antes-de-hacer-pca","title":"2.7. Checklist ANTES de Hacer PCA","text":"<p>Antes de ejecutar tu analisis PCA, verifica:</p> <ul> <li> Cuantos NA tengo? <code>df.isna().sum()</code> - Si &gt; 20% en una variable, considera eliminarla</li> <li> Donde estan los NA? <code>msno.matrix(df)</code> - Busca patrones</li> <li> Son aleatorios? Test de Little o comparacion de grupos</li> <li> Que metodo uso?</li> <li>MCAR + pocos NA: Media o eliminar filas</li> <li>MAR: KNN o MICE</li> <li>MNAR: Consulta con experto del dominio o analisis de sensibilidad</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#28-ejemplo-practico-que-hacer-en-el-decatlon","title":"2.8. Ejemplo Practico: Que Hacer en el Decatlon","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\n\n# Cargar datos\ndf = pd.read_csv('decathlon.csv')\n\n# 1. Diagnostico\nprint(\"Datos perdidos por columna:\")\nprint(df.isna().sum())\n\n# 2. Si hay NA, verificar patron\nif df.isna().any().any():\n    # Comparar atletas con/sin NA\n    df['tiene_na'] = df.isna().any(axis=1)\n    print(\"\\nComparacion de medias (con NA vs sin NA):\")\n    print(df.groupby('tiene_na')[['100m', 'Long.jump']].mean())\n\n    # Si las medias son similares, probablemente MCAR -&gt; podemos imputar\n    # Si las medias son muy diferentes, cuidado -&gt; usar KNN o MICE\n\n    # 3. Imputar con KNN (respeta estructura)\n    imputer = KNNImputer(n_neighbors=3)\n    columnas_activas = ['100m', 'Long.jump', 'Shot.put', 'High.jump',\n                        '400m', '110m.hurdle', 'Discus', 'Pole.vault',\n                        'Javeline', '1500m']\n\n    df[columnas_activas] = imputer.fit_transform(df[columnas_activas])\n    print(\"\\nImputacion completada con KNN\")\n\n# 4. Ahora SI podemos hacer PCA\nfrom prince import PCA\npca = PCA(n_components=5, rescale_with_std=True)\npca.fit(df[columnas_activas])\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#29-resumen-la-regla-de-los-3-pasos","title":"2.9. Resumen: La Regla de los 3 Pasos","text":"<pre><code>PASO 1: DIAGNOSTICAR\n   \"Por que falta este dato?\"\n   -&gt; MCAR, MAR, o MNAR?\n\nPASO 2: DECIDIR\n   \"Puedo imputar sin sesgar?\"\n   -&gt; Si MNAR: NO imputes con media\n   -&gt; Si MAR: Usa KNN o MICE\n   -&gt; Si MCAR: Media es aceptable\n\nPASO 3: DOCUMENTAR\n   \"Que hice y por que?\"\n   -&gt; Reporta el % de NA\n   -&gt; Reporta el metodo usado\n   -&gt; Reporta si los resultados cambian\n</code></pre> <p>MORALEJA: Meter la media \"porque si\" es como llenar un hueco en un puzzle con una pieza de otro puzzle. Puede que encaje, pero la imagen final estara distorsionada.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#210-fundamental-por-que-el-0-es-valido-y-no-siempre-hay-que-imputar","title":"2.10. FUNDAMENTAL: Por Que el 0 es Valido y NO Siempre Hay que Imputar","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#2101-el-problema-de-fondo-confundir-base-completa-con-base-correcta","title":"2.10.1. El Problema de Fondo: Confundir \"Base Completa\" con \"Base Correcta\"","text":"<p>Uno de los errores conceptuales mas graves que cometen los estudiantes (y muchos profesionales) es pensar que:</p> <p>FALSO: \"Una base de datos debe estar completa para ser analizada\"</p> <p>Esta creencia viene de la programacion (donde <code>NULL</code> causa errores) y del machine learning (donde muchos algoritmos no aceptan <code>NaN</code>). Pero en analisis de datos reales, especialmente en encuestas y Big Data, los datos faltantes son INFORMACION, no un problema a \"arreglar\".</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#la-verdad-incomoda","title":"La Verdad Incomoda:","text":"Creencia Erronea Realidad \"Debo llenar todos los NA\" NO. Debes entender POR QUE faltan \"Una base completa es mejor\" NO. Una base con imputaciones incorrectas es PEOR que una base con NA \"El 0 significa que no hay dato\" DEPENDE. En encuestas, 0 puede ser una respuesta valida \"NaN y 0 son lo mismo\" FALSO. Son conceptualmente diferentes"},{"location":"dashboards/02_PCA_FactoMineR_style/#2102-la-diferencia-fundamental-0-vs-nan-vs-no-respuesta","title":"2.10.2. La Diferencia Fundamental: 0 vs NaN vs \"No Respuesta\"","text":"<p>Imagina una encuesta sobre ingresos y gastos:</p> Pregunta Respuesta Persona A Respuesta Persona B Respuesta Persona C \"Cuantos cigarrillos fuma al dia?\" 0 (no fuma) NaN (se nego a responder) 20 (fuma 20) \"Cuanto gasto en tabaco este mes?\" 0 (coherente: no fuma) NaN (no respondio) 150 (coherente: fuma)"},{"location":"dashboards/02_PCA_FactoMineR_style/#analisis-de-cada-caso","title":"Analisis de Cada Caso:","text":"<p>Persona A - El 0 es VALIDO: - <code>cigarrillos = 0</code> significa \"NO FUMA\", es una respuesta real - <code>gasto_tabaco = 0</code> es coherente con no fumar - IMPUTAR estos 0 con la media seria CATASTROFICO: convertirias a un no fumador en fumador promedio</p> <p>Persona B - El NaN es INFORMATIVO: - <code>cigarrillos = NaN</code> significa \"NO QUISO RESPONDER\" - Puede ser porque:   - Fuma ilegalmente (menor de edad)   - Le da verguenza admitir que fuma mucho   - Simplemente no quiere compartir esa informacion - IMPUTAR con la media asume que es un fumador promedio, lo cual puede ser FALSO</p> <p>Persona C - Dato Completo: - Respondio todo, no hay ambiguedad</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2103-conceptos-de-muestreo-por-que-los-datos-faltantes-son-parte-del-diseno","title":"2.10.3. Conceptos de Muestreo: Por Que los Datos Faltantes son Parte del Diseno","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#el-contexto-de-las-encuestas","title":"El Contexto de las Encuestas","text":"<p>Cuando trabajamos con encuestas (surveys) o bases de datos de Big Data, los datos faltantes son una consecuencia natural del proceso de recoleccion. Segun la teoria de muestreo (ver TodoEconometria - Estrategias de Muestreo):</p> <p>Muestreo Aleatorio Simple: La seleccion de la muestra representativa es altamente complicada. Por ejemplo, si queremos estudiar personas con diabetes tipo 2, no todos responderan todas las preguntas, y eso es esperado y valido.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#los-tres-escenarios-reales","title":"Los Tres Escenarios Reales:","text":"<p>Escenario 1: Preguntas de Control (Filtros)</p> <p>En encuestas bien disenadas, hay preguntas que intencionalmente generan <code>NaN</code> en otras variables:</p> <pre><code>P1: \"Tiene automovil?\"\n    [ ] Si  -&gt; Pasar a P2\n    [ ] No  -&gt; Saltar a P5\n\nP2: \"Que marca es su automovil?\" (Solo si respondio Si en P1)\n    _____________\n</code></pre> <p>Resultado en la base de datos:</p> Persona Tiene_Auto Marca_Auto Juan Si Toyota Maria No NaN <p>Pregunta: Debemos imputar la marca de auto de Maria?</p> <p>Respuesta: NO. El <code>NaN</code> es correcto. Maria no tiene auto, por lo tanto no puede tener marca. Imputar \"Toyota\" (la media/moda) seria absurdo.</p> <p>Escenario 2: No Respuesta por Sensibilidad</p> <p>Algunas preguntas son sensibles:</p> <pre><code>P10: \"Cual es su ingreso mensual?\"\n     [ ] Menos de $1000\n     [ ] $1000 - $3000\n     [ ] $3000 - $5000\n     [ ] Mas de $5000\n     [ ] Prefiero no responder\n</code></pre> <p>Resultado:</p> Persona Ingreso Pedro \\(1000-\\)3000 Ana NaN (prefiere no responder) <p>Pregunta: Debemos imputar el ingreso de Ana con la mediana?</p> <p>Respuesta: DEPENDE del tipo de perdida: - Si Ana es rica y no quiere declararlo: MNAR (el dato falta porque es alto) - Si Ana respondio al azar \"no responder\": MCAR (aleatorio)</p> <p>Imputar con la mediana en caso MNAR introducira sesgo hacia abajo (asumimos que los ricos que no responden tienen ingresos promedio).</p> <p>Escenario 3: Datos de Big Data con Sensores</p> <p>En bases de datos de IoT, sensores, o logs:</p> <pre><code>Sensor de Temperatura:\nHora  | Temperatura\n------|------------\n10:00 | 25.3\u00b0C\n10:05 | 25.5\u00b0C\n10:10 | NaN  (sensor fallo)\n10:15 | 26.1\u00b0C\n</code></pre> <p>Pregunta: Debemos imputar 10:10 con la media de 10:05 y 10:15?</p> <p>Respuesta: DEPENDE: - Si el sensor falla aleatoriamente: MCAR -&gt; Imputacion lineal es razonable - Si el sensor falla cuando hace mucho calor (se sobrecalienta): MNAR -&gt; Imputar con la media subestimara la temperatura real</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2104-el-error-conceptual-necesito-datos-completos-para-analizar","title":"2.10.4. El Error Conceptual: \"Necesito Datos Completos para Analizar\"","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#mito-1-los-algoritmos-no-aceptan-nan","title":"Mito 1: \"Los algoritmos no aceptan NaN\"","text":"<p>Realidad: - Muchos algoritmos SI aceptan NaN: arboles de decision, Random Forest, XGBoost - Los que no aceptan NaN (regresion lineal, PCA clasico) tienen versiones robustas:   - PCA con <code>prince</code> acepta NA   - Regresion con <code>statsmodels</code> tiene <code>missing='drop'</code></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#mito-2-mas-datos-mejor-modelo","title":"Mito 2: \"Mas datos = Mejor modelo\"","text":"<p>Realidad: - Datos incorrectos (imputados mal) son peores que menos datos correctos - Ejemplo:</p> <pre><code># Opcion A: Imputar con media (MALO si es MNAR)\ndf['ingreso'].fillna(df['ingreso'].mean())  # 1000 filas, pero con sesgo\n\n# Opcion B: Eliminar filas con NA (BUENO si son pocas)\ndf.dropna(subset=['ingreso'])  # 800 filas, pero sin sesgo\n</code></pre> <p>Cual es mejor? Depende del % de NA: - Si NA &lt; 5%: Eliminar es seguro - Si NA &gt; 20%: Investigar por que faltan antes de decidir</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2105-caso-practico-encuesta-de-habitos-de-consumo","title":"2.10.5. Caso Practico: Encuesta de Habitos de Consumo","text":"<p>Imagina esta base de datos:</p> Persona Fuma Cigarrillos_Dia Gasto_Tabaco Bebe_Alcohol Copas_Semana A No 0 0 Si 3 B Si 10 50 No 0 C Si NaN NaN Si NaN D No 0 0 No 0"},{"location":"dashboards/02_PCA_FactoMineR_style/#analisis-correcto","title":"Analisis Correcto:","text":"<p>Persona A: - <code>Cigarrillos_Dia = 0</code> es VALIDO (no fuma) - <code>Gasto_Tabaco = 0</code> es VALIDO (coherente) - NO IMPUTAR</p> <p>Persona B: - <code>Copas_Semana = 0</code> es VALIDO (no bebe) - NO IMPUTAR</p> <p>Persona C: - <code>Cigarrillos_Dia = NaN</code> es PROBLEMATICO - Opciones:   1. Eliminar fila (si es la unica con NA)   2. Imputar con KNN (buscar fumadores similares)   3. Dejar como NA y usar algoritmos que lo soporten</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#analisis-incorrecto-comun-en-principiantes","title":"Analisis INCORRECTO (Comun en Principiantes):","text":"<pre><code># ERROR: Imputar TODO con la media\ndf.fillna(df.mean())\n</code></pre> <p>Consecuencia: - Persona A (no fumador) ahora tiene <code>Cigarrillos_Dia = 5</code> (media de B y C) - Persona D (no fumador, no bebedor) ahora fuma y bebe - La base de datos esta DESTRUIDA</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2106-reglas-de-oro-para-estudiantes","title":"2.10.6. Reglas de Oro para Estudiantes","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#regla-1-antes-de-tocar-un-na-preguntate","title":"Regla 1: Antes de Tocar un NA, Preguntate:","text":"<pre><code>1. Por que falta este dato?\n   - Pregunta de filtro? -&gt; NA es CORRECTO, NO imputar\n   - No respuesta? -&gt; Investigar tipo (MCAR/MAR/MNAR)\n   - Error tecnico? -&gt; Puede ser MCAR\n\n2. Es un 0 o un NaN?\n   - 0 = Respuesta valida (\"no tengo\", \"no hago\")\n   - NaN = Ausencia de respuesta\n\n3. Que pasa si imputo mal?\n   - Sesgo en resultados\n   - Conclusiones erroneas\n   - Modelo invalido\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#regla-2-workflow-de-decision","title":"Regla 2: Workflow de Decision","text":"<pre><code>PASO 1: Contar NA\n   df.isna().sum()\n   -&gt; Si &lt; 5%: Considera eliminar filas\n   -&gt; Si &gt; 20%: Investiga patron\n\nPASO 2: Visualizar patron\n   import missingno as msno\n   msno.matrix(df)\n   -&gt; Aleatorio? -&gt; MCAR\n   -&gt; Agrupado? -&gt; MAR/MNAR\n\nPASO 3: Test estadistico\n   from pyampute.exploration import mcar_test\n   mcar_test(df)\n   -&gt; p &gt; 0.05: MCAR (puedes imputar)\n   -&gt; p &lt; 0.05: MAR/MNAR (cuidado)\n\nPASO 4: Decidir estrategia\n   - MCAR + pocos NA: Media o eliminar\n   - MAR: KNN o MICE\n   - MNAR: NO imputar o modelo especifico\n   - Pregunta de filtro: DEJAR como NA\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#regla-3-documentar-siempre","title":"Regla 3: Documentar SIEMPRE","text":"<pre><code># BUENA PRACTICA\nprint(\"=\"*50)\nprint(\"REPORTE DE DATOS FALTANTES\")\nprint(\"=\"*50)\nprint(f\"Total de observaciones: {len(df)}\")\nprint(f\"Variables con NA: {df.isna().any().sum()}\")\nprint(f\"\\nDetalle por variable:\")\nprint(df.isna().sum())\nprint(f\"\\nPorcentaje de NA:\")\nprint((df.isna().sum() / len(df) * 100).round(2))\nprint(\"\\nEstrategia aplicada:\")\nprint(\"- Variables con 0 valido: ['Cigarrillos_Dia', 'Copas_Semana']\")\nprint(\"- Variables imputadas con KNN: ['Ingreso']\")\nprint(\"- Filas eliminadas: 3 (0.5% del total)\")\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2107-ejemplo-completo-codigo-correcto-vs-incorrecto","title":"2.10.7. Ejemplo Completo: Codigo Correcto vs Incorrecto","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#codigo-incorrecto-principiante","title":"Codigo INCORRECTO (Principiante):","text":"<pre><code>import pandas as pd\n\n# Cargar datos\ndf = pd.read_csv('encuesta.csv')\n\n# ERROR: Imputar TODO con la media\ndf = df.fillna(df.mean())\n\n# ERROR: No verificar que paso\nprint(\"Listo, base completa!\")\n</code></pre> <p>Problemas: 1. No distingue entre 0 valido y NaN 2. No verifica tipo de perdida 3. No documenta que hizo 4. Puede haber destruido la estructura real</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#codigo-correcto-profesional","title":"Codigo CORRECTO (Profesional):","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# 1. Cargar y explorar\ndf = pd.read_csv('encuesta.csv')\nprint(\"DIAGNOSTICO INICIAL\")\nprint(\"=\"*50)\nprint(df.info())\nprint(\"\\nDatos faltantes:\")\nprint(df.isna().sum())\n\n# 2. Visualizar patron\nmsno.matrix(df)\nplt.title(\"Patron de Datos Faltantes\")\nplt.show()\n\n# 3. Separar variables con 0 valido de variables con NA\n# Variables donde 0 es valido (no imputar)\nvars_con_cero_valido = ['Cigarrillos_Dia', 'Gasto_Tabaco', 'Copas_Semana']\n\n# Variables donde NA debe tratarse\nvars_a_imputar = ['Ingreso', 'Edad', 'Horas_Trabajo']\n\n# 4. Verificar coherencia (0 en variables dependientes)\n# Ejemplo: Si no fuma, gasto en tabaco debe ser 0\nmask_no_fuma = df['Fuma'] == 'No'\nassert (df.loc[mask_no_fuma, 'Cigarrillos_Dia'] == 0).all(), \\\n    \"ERROR: Hay no fumadores con cigarrillos &gt; 0\"\n\n# 5. Imputar solo variables numericas que lo necesiten\nif df[vars_a_imputar].isna().any().any():\n    print(\"\\nAplicando KNN Imputer a:\", vars_a_imputar)\n    imputer = KNNImputer(n_neighbors=5)\n    df[vars_a_imputar] = imputer.fit_transform(df[vars_a_imputar])\n    print(\"Imputacion completada\")\n\n# 6. Documentar\nprint(\"\\n\" + \"=\"*50)\nprint(\"REPORTE FINAL\")\nprint(\"=\"*50)\nprint(f\"Filas totales: {len(df)}\")\nprint(f\"Variables con 0 valido (NO imputadas): {vars_con_cero_valido}\")\nprint(f\"Variables imputadas con KNN: {vars_a_imputar}\")\nprint(f\"NA restantes: {df.isna().sum().sum()}\")\n</code></pre>"},{"location":"dashboards/02_PCA_FactoMineR_style/#2108-resumen-la-filosofia-del-analista-de-datos","title":"2.10.8. Resumen: La Filosofia del Analista de Datos","text":"<p>\"No es tu trabajo llenar huecos. Es tu trabajo ENTENDER por que hay huecos.\"</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#principios-fundamentales","title":"Principios Fundamentales:","text":"<ol> <li>Los datos faltantes son informacion, no un error</li> <li>El 0 es una respuesta valida, no un dato faltante</li> <li>NaN significa \"no se\", y a veces \"no se\" es la respuesta correcta</li> <li>Imputar mal es peor que no imputar</li> <li>Documenta TODO lo que hagas con los NA</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#checklist-final-antes-de-hacer-pca-o-cualquier-analisis","title":"Checklist Final Antes de Hacer PCA (o Cualquier Analisis):","text":"<ul> <li> He identificado todas las variables con 0 valido?</li> <li> He verificado que los 0 sean coherentes con otras variables?</li> <li> He diagnosticado el tipo de perdida (MCAR/MAR/MNAR)?</li> <li> He decidido una estrategia justificada para cada variable?</li> <li> He documentado cuantos NA habia y que hice con ellos?</li> <li> He verificado que los resultados tengan sentido despues de imputar?</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#3-el-dataset-decathlon-contexto-y-preguntas-de-investigacion","title":"3. El Dataset: Decathlon - Contexto y Preguntas de Investigacion","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#31-el-escenario-real-que-problema-queremos-resolver","title":"3.1. El Escenario Real: Que Problema Queremos Resolver?","text":"<p>Imagina que eres el director tecnico de una federacion de atletismo. Tienes datos de 41 atletas que compitieron en decatlon (10 pruebas) en dos competiciones diferentes: los Juegos Olimpicos y el Meeting de Decastar.</p> <p>Te enfrentas a varias preguntas de negocio/investigacion:</p> Pregunta Por que importa Existen \"perfiles\" de atletas? (velocistas vs lanzadores vs completos) Para disenar programas de entrenamiento especializados Que pruebas estan relacionadas entre si? Para optimizar el entrenamiento cruzado Los atletas olimpicos tienen un perfil diferente a los de Decastar? Para entender que caracteriza a los de elite Que pruebas son \"redundantes\" (miden lo mismo)? Para simplificar las evaluaciones Que atletas tienen perfiles atipicos? Para identificar talentos unicos o detectar anomalias <p>CLAVE: No aplicamos PCA \"porque si\" o \"porque esta de moda\". Lo aplicamos porque tenemos preguntas especificas que esta tecnica puede responder de forma eficiente y valida.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#32-por-que-pca-es-la-herramienta-adecuada-y-no-otra","title":"3.2. Por que PCA es la Herramienta Adecuada (y no otra)","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#el-problema-matematico","title":"El Problema Matematico","text":"<p>Tenemos 10 variables numericas (las pruebas). Visualizar 10 dimensiones es imposible. Pero:</p> <ul> <li>No queremos perder informacion importante</li> <li>Queremos ver patrones (grupos de atletas, relaciones entre pruebas)</li> <li>Queremos reducir la complejidad sin destruir la estructura</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#por-que-no-otras-tecnicas","title":"Por que NO otras tecnicas?","text":"Tecnica Por que NO es adecuada aqui Regresion No tenemos una variable \"objetivo\" que predecir. Queremos explorar, no predecir. Clustering (K-Means) Agrupa individuos, pero no explica QUE variables los diferencian ni como se relacionan las variables entre si. Correlacion simple Solo compara variables de 2 en 2. Con 10 variables, tendriamos 45 correlaciones. Imposible de interpretar. t-SNE / UMAP Buenos para visualizar, pero no dan interpretacion de las dimensiones (son \"cajas negras\")."},{"location":"dashboards/02_PCA_FactoMineR_style/#por-que-si-usar-pca","title":"Por que SI usar PCA?","text":"Ventaja de PCA Aplicacion en Decathlon Reduce dimensiones preservando varianza De 10 pruebas a 2-3 dimensiones interpretables Identifica que variables \"van juntas\" Descubrir que 100m, salto largo y 400m estan correlacionadas Detecta individuos atipicos Identificar atletas con perfiles unicos Permite variables suplementarias Ver si \"Competicion\" explica diferencias sin contaminar el analisis Es interpretable Cada eje tiene un significado (ej: \"velocidad vs resistencia\")"},{"location":"dashboards/02_PCA_FactoMineR_style/#33-el-contexto-deportivo-del-decathlon","title":"3.3. El Contexto Deportivo del Decathlon","text":"<p>El decatlon es conocido como \"la prueba reina del atletismo\" porque evalua al atleta mas completo. Consta de 10 pruebas en 2 dias:</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#dia-1","title":"Dia 1","text":"Prueba Tipo Habilidad Principal 100m Carrera Velocidad explosiva Salto largo Salto Potencia de piernas Lanzamiento de peso Lanzamiento Fuerza bruta Salto alto Salto Tecnica + potencia 400m Carrera Velocidad-resistencia"},{"location":"dashboards/02_PCA_FactoMineR_style/#dia-2","title":"Dia 2","text":"Prueba Tipo Habilidad Principal 110m vallas Carrera Velocidad + tecnica Lanzamiento de disco Lanzamiento Fuerza + tecnica Salto con pertiga Salto Tecnica avanzada Lanzamiento de jabalina Lanzamiento Fuerza + coordinacion 1500m Carrera Resistencia pura <p>Hipotesis previa (antes del PCA): Esperamos encontrar que las pruebas se agrupen por \"tipo de habilidad\": velocidad (100m, 400m, vallas), fuerza (peso, disco, jabalina), y tecnica (pertiga, salto alto). El PCA nos dira si esta hipotesis es correcta.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#34-las-dos-competiciones-por-que-es-importante","title":"3.4. Las Dos Competiciones: Por que es Importante?","text":"Competicion Nivel Caracteristicas Juegos Olimpicos Elite mundial Solo los mejores del mundo. Alta presion. Maxima preparacion. Meeting de Decastar Alto nivel europeo Competidores de elite, pero no necesariamente los mejores del mundo. <p>Pregunta clave: Los atletas olimpicos tienen un perfil de rendimiento diferente? O simplemente son \"mejores en todo\"?</p> <ul> <li>Si son \"mejores en todo\", estaran en la misma direccion que los de Decastar pero mas lejos del origen.</li> <li>Si tienen un perfil diferente, estaran en una zona distinta del mapa.</li> </ul> <p>Esta pregunta se responde usando <code>Competition</code> como variable suplementaria: no participa en el calculo del PCA, pero la proyectamos para ver si los grupos se separan.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#35-estructura-de-los-datos","title":"3.5. Estructura de los Datos","text":"Tipo de Variable Variables Rol en el Analisis Activas 100m, Long.jump, Shot.put, High.jump, 400m, 110m.hurdle, Discus, Pole.vault, Javeline, 1500m Construyen los ejes PCA Suplementarias Cuantitativas Rank, Points Se proyectan pero NO construyen ejes Suplementarias Cualitativas Competition (Decastar/OlympicG) Para colorear y segmentar"},{"location":"dashboards/02_PCA_FactoMineR_style/#36-concepto-clave-variables-activas-vs-suplementarias","title":"3.6. Concepto Clave: Variables Activas vs. Suplementarias","text":"<p>Este es uno de los conceptos mas poderosos de FactoMineR:</p> <p>Variables Activas: Participan en el calculo matematico de los componentes principales. Son las que \"construyen\" los ejes. En nuestro caso, las 10 pruebas deportivas.</p> <p>Variables Suplementarias: NO participan en el calculo, pero se proyectan sobre el espacio factorial para ver como se relacionan con las dimensiones encontradas.</p> <p>Por que es util? Imagina que quieres saber si el tipo de competicion (Juegos Olimpicos vs Decastar) influye en el perfil de rendimiento de los atletas. Si incluyes \"Competition\" como variable activa, contaminas el analisis (introduces una variable categorica en un calculo numerico). Pero si la usas como suplementaria, puedes ver si los grupos se separan sin haber forzado esa separacion.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#37-otros-datasets-clasicos-de-pca-y-sus-contextos","title":"3.7. Otros Datasets Clasicos de PCA y sus Contextos","text":"<p>Para que entiendas que PCA se aplica en muchos dominios, aqui tienes otros datasets clasicos:</p> Dataset Contexto Pregunta de Investigacion Variables Iris (Fisher, 1936) Botanica Pueden las medidas de petalos/sepalos distinguir especies de flores? 4 medidas de flores, 3 especies Wine (UCI) Enologia Que propiedades quimicas caracterizan vinos de diferentes regiones? 13 propiedades quimicas, 3 origenes Breast Cancer (UCI) Medicina Que medidas de celulas distinguen tumores benignos de malignos? 30 medidas de nucleos celulares MNIST (LeCun) Vision artificial Pueden reducirse imagenes de 784 pixeles a pocas dimensiones? 784 pixeles, 10 digitos Encuestas sociales Sociologia Que dimensiones latentes explican las opiniones politicas? Muchas preguntas de encuesta <p>Patron comun: En todos los casos, tenemos muchas variables numericas y queremos descubrir estructura latente (dimensiones ocultas que explican la variacion).</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#38-checklist-antes-de-aplicar-pca-preguntate","title":"3.8. Checklist: Antes de Aplicar PCA, Preguntate...","text":"<ul> <li> Tengo una pregunta clara? \"Quiero explorar patrones\" es valido. \"Quiero predecir Y\" no es para PCA.</li> <li> Mis variables son numericas? PCA requiere variables cuantitativas (o convertibles a numericas).</li> <li> Tengo suficientes observaciones? Regla practica: al menos 5-10 observaciones por variable.</li> <li> Las variables estan en escalas comparables? Si no, debo estandarizar (lo hace Prince por defecto).</li> <li> He tratado los datos perdidos? (Ver seccion 2)</li> <li> Tengo variables suplementarias para validar? Muy util para interpretar los resultados.</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#4-resultados-del-analisis-panel-de-graficos","title":"4. Resultados del Analisis: Panel de Graficos","text":"<p>A continuacion se presenta el panel completo de graficos generados por el script <code>02_PCA_FactoMineR_style.py</code>:</p> <p></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#5-interpretacion-detallada-de-cada-grafico","title":"5. Interpretacion Detallada de Cada Grafico","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#41-scree-plot-grafico-de-sedimentacion","title":"4.1. SCREE PLOT (Grafico de Sedimentacion)","text":"<p>Ubicacion: Panel superior izquierdo</p> <p>Que muestra: La varianza explicada por cada componente principal.</p> <p></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#lectura-de-los-resultados","title":"Lectura de los Resultados:","text":"Dimension Varianza Explicada Varianza Acumulada Dim.1 16.7% 16.7% Dim.2 14.5% 31.2% Dim.3 12.8% 44.0% Dim.4 11.9% 55.9% Dim.5 9.6% 65.5%"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion","title":"Interpretacion:","text":"<ol> <li> <p>La linea punteada gris (Umbral = 10%) representa la contribucion esperada si todas las variables fueran independientes (100% / 10 variables = 10%). Las dimensiones por encima de esta linea aportan mas informacion que el promedio.</p> </li> <li> <p>Las primeras 4 dimensiones tienen autovalores mayores a 1 (Regla de Kaiser), lo que sugiere retener 4 componentes.</p> </li> <li> <p>La curva roja (acumulada) muestra que con 5 dimensiones capturamos el 65.5% de la variabilidad total. Esto es relativamente bajo, lo que indica que el decatlon es un deporte multidimensional donde no hay un unico \"factor\" que explique todo el rendimiento.</p> </li> </ol> <p>Insight Deportivo: A diferencia de deportes como natacion (donde la velocidad lo explica casi todo), el decatlon requiere multiples habilidades independientes: velocidad, fuerza, resistencia, tecnica. Por eso ninguna dimension domina claramente.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#42-circulo-de-correlacion-el-grafico-mas-importante","title":"4.2. CIRCULO DE CORRELACION (El Grafico Mas Importante)","text":"<p>Ubicacion: Panel superior centro</p> <p>Este es el grafico mas importante de FactoMineR. Muestra como las variables se relacionan entre si y con los ejes.</p> <p></p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#como-leer-el-circulo","title":"Como Leer el Circulo:","text":"Situacion en el Grafico Significado Flecha cerca del circulo (borde) Variable bien representada en el plano Flecha corta (cerca del centro) Variable mal representada, su varianza esta en otras dimensiones Flechas en la misma direccion Variables positivamente correlacionadas Flechas opuestas (180 grados) Variables negativamente correlacionadas Flechas perpendiculares (90 grados) Variables no correlacionadas"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion-de-nuestros-resultados","title":"Interpretacion de Nuestros Resultados:","text":"<p>Grupo 1 - Cuadrante Inferior Izquierdo (Velocidad/Agilidad): - <code>Long.jump</code> (salto largo): Flecha larga hacia la izquierda - <code>400m</code>: Similar direccion - <code>110m.hurdle</code>: Similar direccion</p> <p>Interpretacion: Estas pruebas estan correlacionadas positivamente entre si. Un atleta bueno en salto largo tiende a ser bueno en 400m y vallas. Esto tiene sentido: todas requieren velocidad explosiva y potencia de piernas.</p> <p>Grupo 2 - Cuadrante Superior Izquierdo (Resistencia): - <code>1500m</code>: Flecha hacia arriba-izquierda - <code>Shot.put</code> (lanzamiento de peso): Hacia arriba</p> <p>Interpretacion: El 1500m (resistencia) y el lanzamiento de peso aparecen en direcciones diferentes al grupo de velocidad. Esto sugiere que la resistencia es una habilidad distinta de la velocidad explosiva.</p> <p>Grupo 3 - Cuadrante Inferior Derecho: - <code>Pole.vault</code> (salto con pertiga): Flecha larga hacia abajo - <code>Discus</code>: Similar direccion</p> <p>Interpretacion: Estas pruebas tecnicas forman su propio grupo.</p> <p>Variable Especial - <code>Long.jump</code>: - Es la flecha mas larga hacia la izquierda - Correlacion con Dim.1: -0.74 (muy alta)</p> <p>Conclusion: El salto largo es la variable que mejor define la primera dimension. Un atleta con alta puntuacion en Dim.1 (derecha del mapa) tiende a tener peor salto largo.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#43-mapa-de-individuos","title":"4.3. MAPA DE INDIVIDUOS","text":"<p>Ubicacion: Panel superior derecho</p> <p>Muestra donde se ubica cada atleta en el espacio de las dos primeras dimensiones, coloreado por la variable suplementaria <code>Competition</code>.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#lectura-del-grafico","title":"Lectura del Grafico:","text":"<ul> <li>Puntos azules: Atletas de la competencia Decastar</li> <li>Puntos rojos: Atletas de los Juegos Olimpicos (OlympicG)</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_1","title":"Interpretacion:","text":"<ol> <li> <p>Dispersion general: Los atletas se distribuyen ampliamente en el plano, sin formar grupos muy compactos. Esto confirma que hay diversidad de perfiles en el decatlon.</p> </li> <li> <p>Separacion por competicion: Observa que los puntos rojos (Olimpicos) tienden a estar mas dispersos y algunos estan en posiciones extremas (ej. BERNARD en el extremo derecho). Los atletas Olimpicos muestran mayor variabilidad en sus perfiles.</p> </li> <li> <p>Atletas destacados:</p> </li> <li>BERNARD (extremo derecho): Perfil muy particular, diferente al promedio</li> <li>Zsivoczky (abajo): Otro perfil atipico</li> <li> <p>BOURGUIGNON (arriba izquierda): Perfil diferente</p> </li> <li> <p>Interpretacion combinada con el circulo:</p> </li> <li>Los atletas a la izquierda del mapa tienden a ser buenos en <code>Long.jump</code>, <code>400m</code>, <code>110m.hurdle</code> (pruebas de velocidad)</li> <li>Los atletas arriba tienden a ser buenos en <code>Shot.put</code>, <code>1500m</code> (fuerza/resistencia)</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#44-contribuciones-de-variables-a-dim1","title":"4.4. CONTRIBUCIONES DE VARIABLES A DIM.1","text":"<p>Ubicacion: Panel inferior izquierdo</p> <p>Este grafico responde a la pregunta: Que variables \"construyeron\" el primer eje?</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#resultados","title":"Resultados:","text":"Variable Contribucion a Dim.1 Long.jump 32.9% 1500m 19.9% 400m 12.0% 110m.hurdle 10.8% High.jump 7.6% Javeline 6.6% Shot.put 5.0% 100m 4.0% Pole.vault 1.1% Discus 0.1%"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_2","title":"Interpretacion:","text":"<ul> <li>La linea roja punteada marca el umbral de contribucion esperada (10% = 100%/10 variables)</li> <li>Las barras rojas (sobre el umbral) son las variables que realmente definen esa dimension</li> <li>Las barras azules (bajo el umbral) contribuyen menos de lo esperado</li> </ul> <p>Conclusion para Dim.1: Esta dimension esta dominada por <code>Long.jump</code> (32.9%), seguida de <code>1500m</code> (19.9%) y <code>400m</code> (12.0%). Esto sugiere que Dim.1 representa un continuo entre atletas explosivos (buenos en salto largo, 400m) vs atletas de resistencia (1500m).</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#45-contribuciones-de-variables-a-dim2","title":"4.5. CONTRIBUCIONES DE VARIABLES A DIM.2","text":"<p>Ubicacion: Panel inferior centro</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#resultados_1","title":"Resultados:","text":"Variable Contribucion a Dim.2 Shot.put 34.1% Pole.vault 30.7% 1500m 10.6% High.jump 10.1% Javeline 8.7% Discus 3.0% Long.jump 1.2% 100m 0.9% 400m 0.7% 110m.hurdle 0.2%"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_3","title":"Interpretacion:","text":"<p>Conclusion para Dim.2: Esta dimension esta dominada por <code>Shot.put</code> (34.1%) y <code>Pole.vault</code> (30.7%). Representa un contraste entre atletas de lanzamiento (fuerza bruta) vs atletas tecnicos (salto con pertiga).</p> <p>Nota importante: Observa que <code>Shot.put</code> y <code>Pole.vault</code> apuntan en direcciones opuestas en el circulo de correlacion. Esto significa que: - Atletas arriba en el mapa: Buenos en lanzamiento de peso - Atletas abajo en el mapa: Buenos en salto con pertiga</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#46-biplot-individuos-variables","title":"4.6. BIPLOT (Individuos + Variables)","text":"<p>Ubicacion: Panel inferior derecho</p> <p>El biplot combina el mapa de individuos con las flechas de variables, permitiendo ver directamente que variables caracterizan a que atletas.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#como-leerlo","title":"Como Leerlo:","text":"<ol> <li>Proyecta mentalmente cada atleta sobre cada flecha de variable</li> <li>Si la proyeccion cae en la direccion de la flecha: el atleta tiene valor alto en esa variable</li> <li>Si la proyeccion cae en la direccion opuesta: el atleta tiene valor bajo en esa variable</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#ejemplo-de-interpretacion","title":"Ejemplo de Interpretacion:","text":"<ul> <li>BERNARD (extremo derecho):</li> <li>Proyeccion sobre <code>Long.jump</code> (izquierda): opuesta = mal salto largo</li> <li> <p>Proyeccion sobre <code>Shot.put</code> (arriba): perpendicular = promedio</p> </li> <li> <p>Atletas arriba-izquierda:</p> </li> <li>Buenos en <code>1500m</code> y <code>Shot.put</code></li> <li>Perfil de resistencia y fuerza</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#6-metricas-avanzadas-de-factominer","title":"6. Metricas Avanzadas de FactoMineR","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#51-calidad-de-representacion-cos2","title":"5.1. Calidad de Representacion (\\(\\cos^2\\))","text":"<p>El \\(\\cos^2\\) mide que tan bien representado esta un punto en el plano factorial.</p> \\[\\cos^2 = \\frac{\\text{distancia al origen en el plano}^2}{\\text{distancia al origen en el espacio original}^2}\\]"},{"location":"dashboards/02_PCA_FactoMineR_style/#interpretacion_4","title":"Interpretacion:","text":"Valor de \\(\\cos^2\\) Significado Cercano a 1 El punto esta perfectamente representado en el plano Cercano a 0 El punto esta mal representado, su informacion esta en otras dimensiones"},{"location":"dashboards/02_PCA_FactoMineR_style/#resultados-para-los-atletas-top-5-mejor-representados","title":"Resultados para los Atletas (Top 5 mejor representados):","text":"Atleta \\(\\cos^2\\) en Dim1+Dim2 Interpretacion Bourguignon 0.755 75.5% de su variabilidad capturada BERNARD 0.725 72.5% de su variabilidad capturada Warners 0.717 71.7% de su variabilidad capturada Lorenzo 0.679 67.9% de su variabilidad capturada Clay 0.638 63.8% de su variabilidad capturada <p>Aplicacion practica: Si vas a interpretar la posicion de un atleta especifico, primero verifica su \\(\\cos^2\\). Si es bajo (&lt; 0.5), su posicion en el mapa puede ser enganosa.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#52-contribuciones","title":"5.2. Contribuciones","text":"<p>Las contribuciones indican cuanto aporta cada individuo/variable a la construccion de un eje.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#para-variables","title":"Para Variables:","text":"<p>Ya vimos que <code>Long.jump</code> contribuye 32.9% a Dim.1. Esto significa que si eliminaramos el salto largo del analisis, la primera dimension cambiaria drasticamente.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#para-individuos","title":"Para Individuos:","text":"Atleta Contribucion a Dim.1 BERNARD 12.82% Warners 12.42% Clay 8.62% <p>Interpretacion: BERNARD y Warners son atletas \"influyentes\" que \"tiran\" el eje hacia sus posiciones. Son atletas con perfiles extremos.</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#7-tabla-de-equivalencias-rpython","title":"7. Tabla de Equivalencias R/Python","text":"<p>Para quienes vienen de R y quieren replicar analisis de FactoMineR:</p> FactoMineR (R) Prince (Python) Descripcion <code>res.pca$eig</code> <code>pca.eigenvalues_summary</code> Tabla de autovalores <code>res.pca$ind$coord</code> <code>pca.row_coordinates(df)</code> Coordenadas de individuos <code>res.pca$ind$contrib</code> <code>pca.row_contributions_</code> Contribuciones de individuos <code>res.pca$ind$cos2</code> <code>pca.row_cosine_similarities(df)</code> Cos2 de individuos <code>res.pca$var$coord</code> <code>pca.column_coordinates</code> Coordenadas de variables <code>res.pca$var$cor</code> <code>pca.column_correlations</code> Correlaciones de variables <code>res.pca$var$contrib</code> <code>pca.column_contributions_</code> Contribuciones de variables <code>res.pca$var$cos2</code> <code>pca.column_cosine_similarities_</code> Cos2 de variables"},{"location":"dashboards/02_PCA_FactoMineR_style/#8-conclusiones-del-analisis-del-decathlon","title":"8. Conclusiones del Analisis del Decathlon","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#71-hallazgos-principales","title":"7.1. Hallazgos Principales","text":"<ol> <li> <p>El decatlon es multidimensional: Ninguna dimension explica mas del 17% de la varianza. Se necesitan al menos 4 componentes para capturar el 56% de la informacion.</p> </li> <li> <p>Dimension 1 - Velocidad/Agilidad vs Resistencia:</p> </li> <li>Dominada por: <code>Long.jump</code> (32.9%), <code>1500m</code> (19.9%), <code>400m</code> (12.0%)</li> <li> <p>Separa atletas explosivos de atletas de resistencia</p> </li> <li> <p>Dimension 2 - Fuerza vs Tecnica:</p> </li> <li>Dominada por: <code>Shot.put</code> (34.1%), <code>Pole.vault</code> (30.7%)</li> <li> <p>Separa atletas de lanzamiento de atletas tecnicos</p> </li> <li> <p>Variable suplementaria (Competition):</p> </li> <li>Los atletas olimpicos muestran mayor variabilidad en sus perfiles</li> <li>No hay una separacion clara entre competiciones, lo que sugiere que el nivel de competencia no determina el tipo de perfil</li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#72-implicaciones-practicas","title":"7.2. Implicaciones Practicas","text":"<ul> <li>Para entrenadores: Identificar en que cuadrante esta un atleta ayuda a disenar programas de entrenamiento personalizados</li> <li>Para analistas: Las contribuciones revelan que variables son mas importantes para caracterizar el rendimiento global</li> </ul>"},{"location":"dashboards/02_PCA_FactoMineR_style/#9-archivos-generados","title":"9. Archivos Generados","text":"Archivo Descripcion <code>02_PCA_FactoMineR_style.py</code> Codigo Python completo <code>02_PCA_FactoMineR_style.md</code> Este manual <code>02_PCA_FactoMineR_graficos.png</code> Panel de 6 graficos <code>02_PCA_circulo_correlacion.png</code> Circulo de correlacion individual <code>02_PCA_resultados_FactoMineR.xlsx</code> Excel con 7 hojas de resultados"},{"location":"dashboards/02_PCA_FactoMineR_style/#10-ejercicio-propuesto-para-el-alumno","title":"10. Ejercicio Propuesto para el Alumno","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#preguntas-de-comprension","title":"Preguntas de Comprension:","text":"<ol> <li> <p>Si un atleta tiene coordenada negativa en Dim.1, que puedes inferir sobre su salto largo?</p> </li> <li> <p>Por que <code>Discus</code> tiene contribucion casi nula (0.1%) a Dim.1?</p> </li> <li> <p>Si el \\(\\cos^2\\) de un atleta en el plano Dim1-Dim2 es 0.30, deberiamos confiar en su posicion en el mapa? Por que?</p> </li> <li> <p>Observando el biplot, que tipo de perfil tiene el atleta YURKOV (arriba en el mapa)?</p> </li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#respuestas","title":"Respuestas:","text":"<ol> <li> <p>Tiene buen salto largo (la correlacion de Long.jump con Dim.1 es negativa: -0.74)</p> </li> <li> <p>Porque la variabilidad del Discus esta representada principalmente en Dim.3 (contribucion 39.4%), no en Dim.1</p> </li> <li> <p>No deberiamos confiar mucho. Solo el 30% de su variabilidad esta capturada en ese plano. El 70% restante esta en otras dimensiones.</p> </li> <li> <p>YURKOV tiene valores altos en las variables que apuntan hacia arriba (<code>Shot.put</code>, <code>1500m</code>, <code>High.jump</code>): perfil de fuerza y resistencia.</p> </li> </ol>"},{"location":"dashboards/02_PCA_FactoMineR_style/#11-referencias-academicas","title":"11. Referencias Academicas","text":""},{"location":"dashboards/02_PCA_FactoMineR_style/#referencia-principal-del-tutorial","title":"Referencia Principal del Tutorial","text":"<p>FactoMineR Tutorial Husson, F., &amp; Josse, J. http://factominer.free.fr/course/FactoTuto.html \"This course presents the main methods used in Exploratory Data Analysis with the FactoMineR package.\"</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#articulo-original-de-factominer","title":"Articulo Original de FactoMineR","text":"<p>Le, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: An R Package for Multivariate Analysis. Journal of Statistical Software, 25(1), 1-18. DOI: 10.18637/jss.v025.i01</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#libro-de-referencia","title":"Libro de Referencia","text":"<p>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. 2<sup>nd</sup> Edition. CRC Press. ISBN: 978-1138196346</p>"},{"location":"dashboards/02_PCA_FactoMineR_style/#informacion-institucional","title":"Informacion Institucional","text":"<p>Autor/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Area: Big Data, Ciencia de Datos &amp; Econometria Aplicada.</p> <p>Hash ID de Certificacion: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code></p>"},{"location":"dashboards/02_pca_iris_clustering/","title":"PCA + Clustering K-Means: Dataset Iris","text":"<p>Autor: @TodoEconometria | Profesor: Juan Marcelo Gutierrez Miranda</p>"},{"location":"dashboards/02_pca_iris_clustering/#tabla-de-contenidos","title":"\ud83d\udcda Tabla de Contenidos","text":"<ol> <li>Introducci\u00f3n</li> <li>El Dataset Iris: Un Cl\u00e1sico del Machine Learning</li> <li>Por Qu\u00e9 Combinar PCA + Clustering</li> <li>An\u00e1lisis de Componentes Principales (PCA)</li> <li>Clustering K-Means</li> <li>Interpretaci\u00f3n de Resultados</li> <li>Conclusiones y Recomendaciones</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>Este documento presenta un an\u00e1lisis completo del famoso dataset Iris combinando dos t\u00e9cnicas fundamentales del Machine Learning no supervisado:</p> <ul> <li>PCA (Principal Component Analysis): Reducci\u00f3n de dimensionalidad</li> <li>K-Means Clustering: Agrupaci\u00f3n de observaciones</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#objetivos-del-analisis","title":"\ud83c\udfaf Objetivos del An\u00e1lisis","text":"<ol> <li>Reducir las 4 dimensiones originales a 2 dimensiones principales</li> <li>Identificar grupos naturales en los datos (especies de flores)</li> <li>Visualizar patrones y relaciones en un espacio 2D</li> <li>Validar si el clustering no supervisado puede descubrir las 3 especies conocidas</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#2-el-dataset-iris-un-clasico-del-machine-learning","title":"2. El Dataset Iris: Un Cl\u00e1sico del Machine Learning","text":""},{"location":"dashboards/02_pca_iris_clustering/#historia-y-contexto","title":"\ud83d\udcd6 Historia y Contexto","text":"<p>El dataset Iris fue introducido por Ronald Fisher en 1936 en su paper seminal:</p> <p>Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.</p> <p>Es uno de los datasets m\u00e1s utilizados en:</p> <ul> <li>Ense\u00f1anza de Machine Learning</li> <li>Validaci\u00f3n de algoritmos de clasificaci\u00f3n</li> <li>Ejemplos de visualizaci\u00f3n de datos</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#descripcion-del-dataset","title":"\ud83c\udf38 Descripci\u00f3n del Dataset","text":"Caracter\u00edstica Descripci\u00f3n Observaciones 150 flores Especies 3 (Setosa, Versicolor, Virginica) Variables 4 medidas en cent\u00edmetros Distribuci\u00f3n 50 flores por especie (balanceado)"},{"location":"dashboards/02_pca_iris_clustering/#variables-medidas","title":"\ud83d\udccf Variables Medidas","text":"<ol> <li>Sepal Length (Largo del s\u00e9palo)</li> <li>Sepal Width (Ancho del s\u00e9palo)</li> <li>Petal Length (Largo del p\u00e9talo)</li> <li>Petal Width (Ancho del p\u00e9talo)</li> </ol> <p>NOTA BOT\u00c1NICA: El s\u00e9palo es la parte verde que protege la flor antes de abrirse. El p\u00e9talo es la parte colorida de la flor.</p>"},{"location":"dashboards/02_pca_iris_clustering/#analisis-exploratorio-de-datos-eda","title":"\ud83d\udcca An\u00e1lisis Exploratorio de Datos (EDA)","text":""},{"location":"dashboards/02_pca_iris_clustering/#por-que-es-importante-este-dataset","title":"\ud83d\udd0d \u00bfPor Qu\u00e9 es Importante Este Dataset?","text":"<ol> <li>Tama\u00f1o Manejable: 150 observaciones son suficientes para aprender sin ser abrumadoras</li> <li>Bien Balanceado: 50 flores de cada especie (no hay desbalance de clases)</li> <li>Separabilidad: Una especie (Setosa) es linealmente separable, las otras dos se superponen ligeramente</li> <li>Multivariate: 4 variables permiten practicar t\u00e9cnicas de reducci\u00f3n de dimensionalidad</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#3-por-que-combinar-pca-clustering","title":"3. Por Qu\u00e9 Combinar PCA + Clustering","text":""},{"location":"dashboards/02_pca_iris_clustering/#el-problema-de-la-dimensionalidad","title":"\ud83e\udd14 El Problema de la Dimensionalidad","text":"<p>Cuando tenemos m\u00e1s de 3 dimensiones, es imposible visualizar los datos directamente:</p> <ul> <li>1D: L\u00ednea (f\u00e1cil)</li> <li>2D: Plano (f\u00e1cil)</li> <li>3D: Espacio 3D (posible pero dif\u00edcil)</li> <li>4D+: \u274c Imposible de visualizar</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#la-solucion-pca-clustering","title":"\ud83d\udca1 La Soluci\u00f3n: PCA + Clustering","text":"<pre><code>Datos Originales (4D)\n        \u2193\n    PCA (Reducci\u00f3n)\n        \u2193\nDatos Reducidos (2D) \u2190 Ahora podemos VISUALIZAR\n        \u2193\n    K-Means (Agrupaci\u00f3n)\n        \u2193\n  Clusters Identificados\n</code></pre>"},{"location":"dashboards/02_pca_iris_clustering/#ventajas-de-esta-combinacion","title":"\u2705 Ventajas de Esta Combinaci\u00f3n","text":"Ventaja Explicaci\u00f3n Visualizaci\u00f3n PCA reduce a 2D para graficar Reducci\u00f3n de Ruido PCA elimina varianza no informativa Mejor Clustering K-Means funciona mejor en espacios de menor dimensi\u00f3n Interpretabilidad Podemos ver y entender los clusters en 2D"},{"location":"dashboards/02_pca_iris_clustering/#4-analisis-de-componentes-principales-pca","title":"4. An\u00e1lisis de Componentes Principales (PCA)","text":""},{"location":"dashboards/02_pca_iris_clustering/#que-es-pca","title":"\ud83c\udfaf \u00bfQu\u00e9 es PCA?","text":"<p>PCA es una t\u00e9cnica que:</p> <ol> <li>Encuentra las direcciones de m\u00e1xima varianza en los datos</li> <li>Proyecta los datos en esas direcciones (componentes principales)</li> <li>Reduce la dimensionalidad manteniendo la mayor informaci\u00f3n posible</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#resultados-del-pca-en-iris","title":"\ud83d\udcca Resultados del PCA en Iris","text":""},{"location":"dashboards/02_pca_iris_clustering/#varianza-explicada","title":"Varianza Explicada","text":"Dimensi\u00f3n Autovalor Varianza (%) Varianza Acumulada (%) Dim.1 ~2.92 ~73% ~73% Dim.2 ~0.91 ~23% ~96% Dim.3 ~0.15 ~4% ~99% Dim.4 ~0.02 ~1% ~100% <p>INTERPRETACI\u00d3N: Las primeras 2 dimensiones capturan ~96% de la varianza total. Esto significa que podemos reducir de 4D a 2D perdiendo solo ~4% de informaci\u00f3n.</p>"},{"location":"dashboards/02_pca_iris_clustering/#regla-de-kaiser","title":"Regla de Kaiser","text":"<p>La Regla de Kaiser dice: Retener componentes con autovalor &gt; 1</p> <ul> <li>Dim.1: Autovalor = 2.92 \u2705 (Retener)</li> <li>Dim.2: Autovalor = 0.91 \u26a0\ufe0f (Casi 1, retener para visualizaci\u00f3n)</li> <li>Dim.3: Autovalor = 0.15 \u274c (Descartar)</li> <li>Dim.4: Autovalor = 0.02 \u274c (Descartar)</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#interpretacion-de-las-dimensiones","title":"\ud83d\udd0d Interpretaci\u00f3n de las Dimensiones","text":""},{"location":"dashboards/02_pca_iris_clustering/#dimension-1-73-de-varianza","title":"Dimensi\u00f3n 1 (~73% de varianza)","text":"<p>Variables que m\u00e1s contribuyen:</p> <ul> <li>Petal Length (~42%)</li> <li>Petal Width (~42%)</li> </ul> <p>Interpretaci\u00f3n:</p> <p>Dim.1 representa el \"tama\u00f1o del p\u00e9talo\". Flores con valores altos en Dim.1 tienen p\u00e9talos grandes; valores bajos tienen p\u00e9talos peque\u00f1os.</p>"},{"location":"dashboards/02_pca_iris_clustering/#dimension-2-23-de-varianza","title":"Dimensi\u00f3n 2 (~23% de varianza)","text":"<p>Variables que m\u00e1s contribuyen:</p> <ul> <li>Sepal Width (~72%)</li> </ul> <p>Interpretaci\u00f3n:</p> <p>Dim.2 representa el \"ancho del s\u00e9palo\". Flores con valores altos en Dim.2 tienen s\u00e9palos anchos; valores bajos tienen s\u00e9palos estrechos.</p>"},{"location":"dashboards/02_pca_iris_clustering/#circulo-de-correlacion","title":"\ud83d\udcc8 C\u00edrculo de Correlaci\u00f3n","text":"<p>El c\u00edrculo de correlaci\u00f3n muestra c\u00f3mo las variables originales se relacionan con las dimensiones principales:</p> <pre><code>           Dim.2 (Sepal Width)\n                 \u2191\n                 |\n    Sepal Width  |\n         \u2191       |\n         |       |\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Dim.1 (Petal Size)\n         |       |\n         |   Petal Length \u2192\n         |   Petal Width \u2192\n         |\n</code></pre> <p>Observaciones:</p> <ul> <li>Petal Length y Petal Width est\u00e1n muy correlacionadas (flechas en la misma direcci\u00f3n)</li> <li>Sepal Width es casi perpendicular a las medidas de p\u00e9talo (baja correlaci\u00f3n)</li> <li>Sepal Length est\u00e1 entre ambas dimensiones</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#5-clustering-k-means","title":"5. Clustering K-Means","text":""},{"location":"dashboards/02_pca_iris_clustering/#que-es-k-means","title":"\ud83c\udfaf \u00bfQu\u00e9 es K-Means?","text":"<p>K-Means es un algoritmo de clustering que:</p> <ol> <li>Divide los datos en K grupos (clusters)</li> <li>Minimiza la distancia de cada punto a su centroide</li> <li>Itera hasta convergencia</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#determinacion-del-numero-optimo-de-clusters","title":"\ud83d\udd22 Determinaci\u00f3n del N\u00famero \u00d3ptimo de Clusters","text":""},{"location":"dashboards/02_pca_iris_clustering/#metodo-del-codo-elbow-method","title":"M\u00e9todo del Codo (Elbow Method)","text":"<p>Graficamos la inercia (suma de distancias al cuadrado) vs K:</p> <pre><code>Inercia\n  \u2502\n  \u2502 \u25cf\n  \u2502   \u25cf\n  \u2502     \u25cf  \u2190 \"Codo\" en K=3\n  \u2502       \u25cf\n  \u2502         \u25cf\n  \u2502           \u25cf\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 K\n    2  3  4  5  6  7\n</code></pre> <p>Interpretaci\u00f3n: El \"codo\" est\u00e1 en K=3, sugiriendo 3 clusters.</p>"},{"location":"dashboards/02_pca_iris_clustering/#silhouette-score","title":"Silhouette Score","text":"<p>El Silhouette Score mide qu\u00e9 tan bien separados est\u00e1n los clusters:</p> <ul> <li>Valor: Entre -1 y 1</li> <li>Interpretaci\u00f3n:</li> <li>Cercano a 1: Clusters bien separados \u2705</li> <li>Cercano a 0: Clusters superpuestos \u26a0\ufe0f</li> <li>Negativo: Puntos mal asignados \u274c</li> </ul> <p>Resultado para Iris: Silhouette Score \u2248 0.55 (buena separaci\u00f3n)</p>"},{"location":"dashboards/02_pca_iris_clustering/#resultados-del-clustering","title":"\ud83d\udcca Resultados del Clustering","text":""},{"location":"dashboards/02_pca_iris_clustering/#confusion-matrix-clusters-vs-especies-reales","title":"Confusion Matrix: Clusters vs Especies Reales","text":"Cluster 0 Cluster 1 Cluster 2 Setosa 50 0 0 Versicolor 0 48 2 Virginica 0 14 36 <p>Observaciones:</p> <ul> <li>Setosa: Perfectamente separada (100% en Cluster 0)</li> <li>Versicolor: Mayormente en Cluster 1 (96%)</li> <li>Virginica: Mayormente en Cluster 2 (72%), pero con superposici\u00f3n con Versicolor</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#pureza-de-clusters","title":"Pureza de Clusters","text":"<p>La pureza mide el porcentaje de observaciones correctamente agrupadas:</p> <pre><code>Pureza = (50 + 48 + 36) / 150 = 89.3%\n</code></pre> <p>INTERPRETACI\u00d3N: El algoritmo K-Means logr\u00f3 identificar correctamente las especies en 89.3% de los casos, sin conocer las etiquetas reales. Esto es excelente para un m\u00e9todo no supervisado.</p>"},{"location":"dashboards/02_pca_iris_clustering/#visualizacion-de-clusters","title":"\ud83c\udfa8 Visualizaci\u00f3n de Clusters","text":"<p>En el espacio 2D del PCA, los clusters se ven as\u00ed:</p> <pre><code>     Dim.2\n       \u2191\n       \u2502     \u25cf Cluster 2 (Virginica)\n       \u2502    \u25cf\u25cf\u25cf\n       \u2502   \u25cf\u25cf\u25cf\u25cf\n       \u2502  \u25cf\u25cf\u25cf\u25cf\n       \u2502 \u25cf\u25cf\u25cf\u25cf  \u25a0\u25a0\u25a0 Cluster 1 (Versicolor)\n       \u2502\u25cf\u25cf\u25cf   \u25a0\u25a0\u25a0\u25a0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Dim.1\n       \u2502\n       \u2502  \u25b2\u25b2\u25b2\n       \u2502 \u25b2\u25b2\u25b2\u25b2\u25b2\n       \u2502\u25b2\u25b2\u25b2\u25b2\u25b2\u25b2  Cluster 0 (Setosa)\n       \u2502\n</code></pre> <p>Centroides (marcados con X):</p> <ul> <li>Cluster 0: (-2.7, 0.3) \u2192 Setosa</li> <li>Cluster 1: (0.3, -0.5) \u2192 Versicolor</li> <li>Cluster 2: (1.7, 0.2) \u2192 Virginica</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#6-interpretacion-de-resultados","title":"6. Interpretaci\u00f3n de Resultados","text":""},{"location":"dashboards/02_pca_iris_clustering/#panel-completo-pca-clustering-k-means","title":"Panel Completo: PCA + Clustering K-Means","text":""},{"location":"dashboards/02_pca_iris_clustering/#analisis-por-especie","title":"\ud83d\udd2c An\u00e1lisis por Especie","text":""},{"location":"dashboards/02_pca_iris_clustering/#setosa-cluster-0","title":"Setosa (Cluster 0)","text":"<p>Caracter\u00edsticas:</p> <ul> <li>Petal Length: Muy peque\u00f1o (~1.5 cm)</li> <li>Petal Width: Muy peque\u00f1o (~0.2 cm)</li> <li>Sepal Width: Relativamente grande</li> </ul> <p>Posici\u00f3n en PCA:</p> <ul> <li>Dim.1: Valores muy negativos (p\u00e9talos peque\u00f1os)</li> <li>Dim.2: Valores positivos (s\u00e9palos anchos)</li> </ul> <p>Separabilidad: \u2705 Perfecta (100% correctamente agrupada)</p>"},{"location":"dashboards/02_pca_iris_clustering/#versicolor-cluster-1","title":"Versicolor (Cluster 1)","text":"<p>Caracter\u00edsticas:</p> <ul> <li>Petal Length: Mediano (~4.3 cm)</li> <li>Petal Width: Mediano (~1.3 cm)</li> <li>Sepal Width: Mediano</li> </ul> <p>Posici\u00f3n en PCA:</p> <ul> <li>Dim.1: Valores cercanos a 0 (p\u00e9talos medianos)</li> <li>Dim.2: Valores ligeramente negativos</li> </ul> <p>Separabilidad: \u26a0\ufe0f Buena (96% correctamente agrupada, 4% confundida con Virginica)</p>"},{"location":"dashboards/02_pca_iris_clustering/#virginica-cluster-2","title":"Virginica (Cluster 2)","text":"<p>Caracter\u00edsticas:</p> <ul> <li>Petal Length: Grande (~5.5 cm)</li> <li>Petal Width: Grande (~2.0 cm)</li> <li>Sepal Width: Mediano</li> </ul> <p>Posici\u00f3n en PCA:</p> <ul> <li>Dim.1: Valores muy positivos (p\u00e9talos grandes)</li> <li>Dim.2: Valores cercanos a 0</li> </ul> <p>Separabilidad: \u26a0\ufe0f Moderada (72% correctamente agrupada, 28% confundida con Versicolor)</p>"},{"location":"dashboards/02_pca_iris_clustering/#metricas-de-evaluacion","title":"\ud83d\udcca M\u00e9tricas de Evaluaci\u00f3n","text":"M\u00e9trica Valor Interpretaci\u00f3n Silhouette Score 0.55 Buena separaci\u00f3n entre clusters Davies-Bouldin Index 0.66 Clusters compactos y separados (menor es mejor) Calinski-Harabasz Index 561.63 Alta separaci\u00f3n entre clusters (mayor es mejor) Pureza 89.3% Alta concordancia con especies reales"},{"location":"dashboards/02_pca_iris_clustering/#por-que-versicolor-y-virginica-se-superponen","title":"\ud83c\udfaf \u00bfPor Qu\u00e9 Versicolor y Virginica se Superponen?","text":"<p>Raz\u00f3n Biol\u00f3gica:</p> <ul> <li>Versicolor y Virginica son especies evolutivamente m\u00e1s cercanas</li> <li>Comparten caracter\u00edsticas morfol\u00f3gicas similares</li> <li>Setosa es m\u00e1s distinta (probablemente de un linaje diferente)</li> </ul> <p>Raz\u00f3n Estad\u00edstica:</p> <ul> <li>Las medidas de p\u00e9talo de Versicolor y Virginica tienen rangos superpuestos</li> <li>No existe una frontera clara en el espacio de 4 dimensiones</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#7-conclusiones-y-recomendaciones","title":"7. Conclusiones y Recomendaciones","text":""},{"location":"dashboards/02_pca_iris_clustering/#conclusiones-principales","title":"\u2705 Conclusiones Principales","text":"<ol> <li>PCA es Efectivo:</li> <li>Reduce de 4D a 2D manteniendo 96% de la informaci\u00f3n</li> <li> <p>Las 2 primeras dimensiones son suficientes para visualizaci\u00f3n y clustering</p> </li> <li> <p>Las Medidas de P\u00e9talo son Clave:</p> </li> <li>Petal Length y Petal Width son las variables m\u00e1s discriminantes</li> <li> <p>Dim.1 (que representa el tama\u00f1o del p\u00e9talo) explica 73% de la varianza</p> </li> <li> <p>K-Means Funciona Bien:</p> </li> <li>Identifica correctamente las 3 especies en 89.3% de los casos</li> <li>Setosa es perfectamente separable</li> <li> <p>Versicolor y Virginica tienen cierta superposici\u00f3n natural</p> </li> <li> <p>Validaci\u00f3n del M\u00e9todo No Supervisado:</p> </li> <li>Sin conocer las etiquetas, K-Means descubre los 3 grupos naturales</li> <li>Esto valida que las especies tienen diferencias morfol\u00f3gicas reales</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#lecciones-para-estudiantes","title":"\ud83c\udf93 Lecciones para Estudiantes","text":""},{"location":"dashboards/02_pca_iris_clustering/#leccion-1-la-importancia-de-la-reduccion-de-dimensionalidad","title":"Lecci\u00f3n 1: La Importancia de la Reducci\u00f3n de Dimensionalidad","text":"<p>ANTES DE PCA: 4 variables \u2192 Dif\u00edcil de visualizar \u2192 Dif\u00edcil de interpretar</p> <p>DESPU\u00c9S DE PCA: 2 dimensiones \u2192 F\u00e1cil de visualizar \u2192 Patrones claros</p> <p>Moraleja: No siempre necesitas todas las variables. A veces, menos es m\u00e1s.</p>"},{"location":"dashboards/02_pca_iris_clustering/#leccion-2-el-clustering-no-supervisado-puede-descubrir-estructura-real","title":"Lecci\u00f3n 2: El Clustering No Supervisado Puede Descubrir Estructura Real","text":"<p>SIN ETIQUETAS: K-Means encuentra 3 grupos</p> <p>CON ETIQUETAS: Hay 3 especies reales</p> <p>COINCIDENCIA: 89.3%</p> <p>Moraleja: Los datos tienen estructura natural. Los algoritmos pueden encontrarla.</p>"},{"location":"dashboards/02_pca_iris_clustering/#leccion-3-no-todos-los-grupos-son-perfectamente-separables","title":"Lecci\u00f3n 3: No Todos los Grupos son Perfectamente Separables","text":"<p>Setosa: 100% separable</p> <p>Versicolor/Virginica: Superposici\u00f3n natural</p> <p>Moraleja: En datos reales, la superposici\u00f3n es normal. No esperes clusters perfectos.</p>"},{"location":"dashboards/02_pca_iris_clustering/#leccion-4-validar-validar-validar","title":"Lecci\u00f3n 4: Validar, Validar, Validar","text":"<p>M\u00e9todo del Codo: Sugiere K=3</p> <p>Silhouette Score: Confirma K=3</p> <p>Pureza: Valida que K=3 es correcto</p> <p>Moraleja: Usa m\u00faltiples m\u00e9tricas para validar tus decisiones.</p>"},{"location":"dashboards/02_pca_iris_clustering/#recomendaciones-practicas","title":"\ud83d\udd27 Recomendaciones Pr\u00e1cticas","text":""},{"location":"dashboards/02_pca_iris_clustering/#para-clasificacion-de-especies-de-iris","title":"Para Clasificaci\u00f3n de Especies de Iris","text":"<ol> <li>Enfocarse en medidas de p\u00e9talo (son las m\u00e1s discriminantes)</li> <li>Usar PCA para visualizaci\u00f3n (reduce complejidad sin perder informaci\u00f3n)</li> <li>K=3 es \u00f3ptimo (validado por m\u00faltiples m\u00e9tricas)</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#para-analisis-de-datos-similares","title":"Para An\u00e1lisis de Datos Similares","text":"<ol> <li>Siempre hacer EDA primero (entender distribuciones y correlaciones)</li> <li>Estandarizar antes de PCA (variables en diferentes escalas sesgan resultados)</li> <li>Validar n\u00famero de clusters (no asumir K, usar Elbow + Silhouette)</li> <li>Comparar con ground truth (si est\u00e1 disponible, como en este caso)</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#extensiones-posibles","title":"\ud83d\ude80 Extensiones Posibles","text":"<ol> <li>Otros Algoritmos de Clustering:</li> <li>DBSCAN (para clusters de forma arbitraria)</li> <li>Hierarchical Clustering (para dendrogramas)</li> <li> <p>Gaussian Mixture Models (para clusters probabil\u00edsticos)</p> </li> <li> <p>Clasificaci\u00f3n Supervisada:</p> </li> <li>Usar las especies conocidas para entrenar un clasificador</li> <li> <p>Comparar con clustering no supervisado</p> </li> <li> <p>An\u00e1lisis de Variables Suplementarias:</p> </li> <li>Agregar informaci\u00f3n de ubicaci\u00f3n geogr\u00e1fica</li> <li>Agregar informaci\u00f3n de temporada de recolecci\u00f3n</li> </ol>"},{"location":"dashboards/02_pca_iris_clustering/#referencias","title":"\ud83d\udcda Referencias","text":""},{"location":"dashboards/02_pca_iris_clustering/#papers-originales","title":"Papers Originales","text":"<ul> <li>Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.</li> <li> <p>El paper original que introdujo el dataset Iris</p> </li> <li> <p>Anderson, E. (1935). The irises of the Gaspe Peninsula. Bulletin of the American Iris Society, 59, 2-5.</p> </li> <li>El bot\u00e1nico que recolect\u00f3 los datos originales</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#libros-de-referencia","title":"Libros de Referencia","text":"<ul> <li>Husson, F., L\u00ea, S., &amp; Pag\u00e8s, J. (2017). Exploratory Multivariate Analysis by Example Using R. CRC Press.</li> <li> <p>Referencia principal para PCA estilo FactoMineR</p> </li> <li> <p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.</p> </li> <li>Cap\u00edtulos sobre PCA y Clustering</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#articulos-tecnicos","title":"Art\u00edculos T\u00e9cnicos","text":"<ul> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> <li> <p>Documentaci\u00f3n de las librer\u00edas utilizadas</p> </li> <li> <p>Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.</p> </li> <li>M\u00e9todo del Silhouette Score</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#recursos-adicionales","title":"\ud83d\udd17 Recursos Adicionales","text":""},{"location":"dashboards/02_pca_iris_clustering/#tutoriales-online","title":"Tutoriales Online","text":"<ul> <li>Scikit-learn: PCA Tutorial</li> <li>Scikit-learn: K-Means Tutorial</li> <li>FactoMineR Tutorial</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#datasets-similares","title":"Datasets Similares","text":"<ul> <li>Wine Dataset: 178 vinos, 13 variables qu\u00edmicas, 3 clases</li> <li>Breast Cancer Dataset: 569 tumores, 30 variables, 2 clases (maligno/benigno)</li> <li>Digits Dataset: 1797 im\u00e1genes de d\u00edgitos, 64 p\u00edxeles, 10 clases</li> </ul> <p>Autor: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Fecha: Enero 2026 Licencia: Uso educativo con atribuci\u00f3n</p>"},{"location":"dashboards/02_pca_iris_clustering/#preguntas-frecuentes-faq","title":"\ud83d\udcac Preguntas Frecuentes (FAQ)","text":""},{"location":"dashboards/02_pca_iris_clustering/#por-que-estandarizar-antes-de-pca","title":"\u00bfPor qu\u00e9 estandarizar antes de PCA?","text":"<p>Respuesta: Porque PCA es sensible a la escala de las variables. Si una variable tiene valores mucho mayores que otra (ej: ingresos en miles vs edad en decenas), dominar\u00e1 la varianza y sesgar\u00e1 los resultados.</p>"},{"location":"dashboards/02_pca_iris_clustering/#cuantas-componentes-debo-retener","title":"\u00bfCu\u00e1ntas componentes debo retener?","text":"<p>Respuesta: Depende del objetivo:</p> <ul> <li>Visualizaci\u00f3n: 2-3 componentes</li> <li>Regla de Kaiser: Componentes con autovalor &gt; 1</li> <li>Varianza Acumulada: Retener hasta alcanzar 80-95% de varianza</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#k-means-siempre-encuentra-los-clusters-correctos","title":"\u00bfK-Means siempre encuentra los clusters correctos?","text":"<p>Respuesta: No. K-Means tiene limitaciones:</p> <ul> <li>Asume clusters esf\u00e9ricos</li> <li>Sensible a inicializaci\u00f3n (usar <code>n_init</code> alto)</li> <li>Requiere especificar K de antemano</li> </ul>"},{"location":"dashboards/02_pca_iris_clustering/#que-pasa-si-tengo-mas-de-3-especies","title":"\u00bfQu\u00e9 pasa si tengo m\u00e1s de 3 especies?","text":"<p>Respuesta: El proceso es el mismo:</p> <ol> <li>Usar Elbow + Silhouette para determinar K \u00f3ptimo</li> <li>Validar con m\u00e9tricas (pureza, confusion matrix)</li> <li>Visualizar en 2D con PCA (aunque haya m\u00e1s de 3 clusters)</li> </ol> <p>FIN DEL DOCUMENTO</p>"},{"location":"dashboards/04_series_temporales_arima/","title":"Series Temporales: ARIMA/SARIMA (Box-Jenkins)","text":"<p>Analisis completo de series temporales siguiendo la metodologia Box-Jenkins, desde la identificacion del modelo hasta el pronostico con intervalos de confianza.</p> <p>Abrir Dashboard Interactivo</p>"},{"location":"dashboards/04_series_temporales_arima/#contenido","title":"Contenido","text":"<p>El dashboard presenta 6 pestanas interactivas:</p> Pestana Contenido Serie Original Pasajeros aereos mensuales 1949-1960 (144 observaciones) Descomposicion Tendencia + Estacionalidad + Residuo (multiplicativa) ACF / PACF Autocorrelacion y autocorrelacion parcial de la serie diferenciada Diagnostico Residuos, histograma, ACF residual, Q-Q plot Pronostico Final Serie original + ajuste SARIMA + forecast 12 meses + IC 95% Metricas Radar RMSE, MAE, MAPE, R2 del modelo"},{"location":"dashboards/04_series_temporales_arima/#metodologia-box-jenkins","title":"Metodologia Box-Jenkins","text":"<ol> <li>Identificacion: ACF/PACF para determinar ordenes (p, d, q)(P, D, Q)[s]</li> <li>Estimacion: Ajuste por maxima verosimilitud</li> <li>Diagnostico: Tests de Ljung-Box, Jarque-Bera sobre residuos</li> <li>Pronostico: Forecast con intervalos de confianza al 95%</li> </ol> <p>Modelo seleccionado: SARIMA(1,1,0)(0,1,0)[12] \u2014 AIC = -445.41</p>"},{"location":"dashboards/04_series_temporales_arima/#codigo-fuente","title":"Codigo fuente","text":"<ul> <li>Script completo: <code>ejercicios/04_machine_learning/07_series_temporales_arima/</code></li> <li>Guia teorica: <code>07_series_temporales_arima/README.md</code></li> </ul>"},{"location":"dashboards/04_similitud_jaccard/","title":"\ud83e\uddf6 An\u00e1lisis de Similitud: El Mystery del Portal Web","text":"<p>\"Un sistema de recomendaci\u00f3n es como un bibliotecario que sabe exactamente qu\u00e9 revista te va a gustar sin haber le\u00eddo el contenido completo, solo mirando las palabras que se repiten. \ud83e\udd18\"</p>"},{"location":"dashboards/04_similitud_jaccard/#el-desafio-del-portal","title":"\ud83c\udfaf El Desaf\u00edo del Portal","text":"<p>Imagina que gestionas un portal din\u00e1mico. Tu jefe te ha puesto un reto: \"Agrupa estos art\u00edculos autom\u00e1ticamente. No tengo tiempo para leerlos todos.\"</p> <p></p> <p>Para resolverlo, usamos el \u00cdndice de Jaccard, una herramienta matem\u00e1tica que convierte el texto en \"conjuntos\" y mide cu\u00e1nto se solapan. \u26a1</p> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#el-corazon-del-algoritmo","title":"\ud83c\udfd7\ufe0f El Coraz\u00f3n del Algoritmo","text":"<p>La magia ocurre comparando lo que los documentos comparten frente a todo lo que dicen.</p> Atributo Explicaci\u00f3n Visual Intersecci\u00f3n Las palabras que aparecen en AMBOS textos. \u2694\ufe0f Uni\u00f3n Todas las palabras \u00fanicas de AMBOS textos. \ud83c\udf0c Resultado Un n\u00famero entre 0 (desconocidos) y 1 (almas gemelas). \ud83e\udd18 <p></p>"},{"location":"dashboards/04_similitud_jaccard/#la-formula-matematica","title":"La F\u00f3rmula Matem\u00e1tica","text":""},{"location":"dashboards/04_similitud_jaccard/#resultados-reales-generados-por-tu-script","title":"\ud83c\udfb8 Resultados Reales (Generados por tu Script)","text":"<p>Aqu\u00ed es donde la teor\u00eda se encuentra con la realidad. Al ejecutar <code>04_similitud_jaccard.py</code>, el sistema \"ve\" el portal as\u00ed:</p>"},{"location":"dashboards/04_similitud_jaccard/#1-el-mapa-de-calor-del-saber","title":"1. El Mapa de Calor del Saber","text":"<p>En esta matriz, los colores c\u00e1lidos (rojos) indican alta similitud. Observa c\u00f3mo se forman cuadrados en la diagonal. \u00a1Eso son tus categor\u00edas de F\u00fatbol, Tecnolog\u00eda y Cocina detectadas autom\u00e1ticamente! \u26a1</p> <p></p> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#2-la-prueba-del-algoritmo-clustermap","title":"2. La Prueba del Algoritmo (Clustermap)","text":"<p>\u00bfPuede la inteligencia artificial agrupar los temas sin ayuda? El Dendrograma (el \u00e1rbol lateral) nos dice que s\u00ed. Los art\u00edculos de la misma tem\u00e1tica se \"buscan\" y se agrupan en ramas comunes. \ud83d\udc80</p> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#3-clustermap-con-valores-numericos","title":"3. Clustermap con Valores Num\u00e9ricos","text":"<p>Otra vista del mismo agrupamiento, ahora con los valores exactos de similitud en cada celda:</p> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#aplicaciones-en-el-mundo-real","title":"\u2694\ufe0f Aplicaciones en el Mundo Real","text":"<p>No es solo un ejercicio acad\u00e9mico. Esta t\u00e9cnica se usa cada segundo en:</p> <ul> <li>\ud83c\udff4\u200d\u2620\ufe0f Detecci\u00f3n de Plagio: Comparar entregas de alumnos para ver si comparten \"demasiado\" vocabulario.</li> <li>\ud83d\udef8 Recomendadores: \"Si le\u00edste sobre el nuevo CPU, te recomiendo este art\u00edculo sobre memoria RAM\".</li> <li>\u26d3\ufe0f SEO y Buscadores: Para entender si dos p\u00e1ginas hablan de lo mismo y evitar contenido duplicado.</li> </ul> <p></p>"},{"location":"dashboards/04_similitud_jaccard/#reflexion-final-para-el-alumno","title":"\ud83c\udf11 Reflexi\u00f3n Final para el Alumno","text":"<p>Mira las gr\u00e1ficas que se han guardado en tu carpeta:</p> <ol> <li>\u00bfVes alg\u00fan punto rojo fuera de la diagonal? Eso indicar\u00eda que dos temas diferentes comparten palabras.</li> <li>\u00bfQu\u00e9 pasar\u00eda si el corpus fuera de 10,000 documentos? El mapa de calor se volver\u00eda ilegible, pero el Clustermap seguir\u00eda d\u00e1ndonos la estructura. \ud83e\udd18</li> </ol> <p>Hash de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Autor: Juan Marcelo Gutierrez Miranda (@TodoEconometria)</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/","title":"\ud83c\udf93 Gu\u00eda Formativa: Vectorizaci\u00f3n y Clustering de Documentos","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#analisis-de-topicos-con-inteligencia-artificial-nlp-machine-learning","title":"\ud83d\ude80 An\u00e1lisis de T\u00f3picos con Inteligencia Artificial (NLP &amp; Machine Learning)","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#informacion-de-certificacion-y-referencia","title":"\ud83d\udcdd Informaci\u00f3n de Certificaci\u00f3n y Referencia","text":"<p>Autor original/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda Metodolog\u00eda: Cursos Avanzados de Big Data, Ciencia de Datos, Desarrollo de aplicaciones con IA &amp; Econometr\u00eda Aplicada. Hash ID de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Repositorio: https://github.com/TodoEconometria/certificaciones </p> <p>REFERENCIA ACAD\u00c9MICA:</p> <ul> <li>McKinney, W. (2012). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media.</li> <li>Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362.</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#1-introduccion-al-laboratorio","title":"\ud83c\udfdb\ufe0f 1. Introducci\u00f3n al Laboratorio","text":"<p>En el mundo del Big Data, la mayor parte de la informaci\u00f3n es no estructurada (textos, correos, noticias). El reto fundamental es: \u00bfC\u00f3mo puede una computadora entender que dos documentos hablan de lo mismo sin leerlos?</p> <p>Este ejercicio implementa un pipeline completo de Ciencia de Datos para agrupar autom\u00e1ticamente 1,200 documentos en espa\u00f1ol, utilizando una combinaci\u00f3n de t\u00e9cnicas matem\u00e1ticas avanzadas para transformar el lenguaje humano en estructuras que las m\u00e1quinas pueden procesar.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#2-fundamentos-teoricos-que-usamos-y-por-que","title":"\ud83e\udde0 2. Fundamentos Te\u00f3ricos (\u00bfQu\u00e9 usamos y por qu\u00e9?)","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#a-vectorizacion-el-puente-entre-texto-y-matematicas","title":"A. Vectorizaci\u00f3n: El Puente entre Texto y Matem\u00e1ticas","text":"<p>Las computadoras no entienden palabras, solo n\u00fameros. Usamos TF-IDF (Term Frequency - Inverse Document Frequency) por su capacidad de discernir la relevancia.</p> <ul> <li>\u00bfQu\u00e9 es?: Un valor estad\u00edstico que busca medir qu\u00e9 tan importante es una palabra para un documento dentro de una colecci\u00f3n (corpus).</li> <li>\u00bfC\u00f3mo funciona?:     $\\(TF(t, d) = \\frac{\\text{Conteo de la palabra } t \\text{ en documento } d}{\\text{Total de palabras en } d}\\)$     $\\(IDF(t) = \\log\\left(\\frac{\\text{Total de documentos}}{\\text{Documentos que contienen la palabra } t}\\right)\\)$</li> <li>\u00bfPor qu\u00e9 lo usamos?: A diferencia del simple conteo (Bak-of-Words), el TF-IDF penaliza palabras que aparecen en todos lados (como \"el\", \"que\", \"es\") y premia palabras tem\u00e1ticas (\"procesador\", \"inversi\u00f3n\", \"vacuna\").</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#b-k-means-el-cerebro-del-agrupamiento","title":"B. K-Means: El Cerebro del Agrupamiento","text":"<p>Para el Aprendizaje No Supervisado, el algoritmo de K-Means es el est\u00e1ndar de oro para encontrar patrones sin etiquetas previas.</p> <ul> <li>El Proceso:<ol> <li>Define \\(k\\) puntos aleatorios (centroides).</li> <li>Asigna cada documento al centroide m\u00e1s cercano (usando distancia euclidiana en el espacio vectorial).</li> <li>Recalcula el centro del grupo y repite hasta que los grupos se estabilizan.</li> </ol> </li> <li>Por qu\u00e9 lo usamos: Es extremadamente eficiente para grandes vol\u00famenes de datos y nos permite segmentar el mercado, noticias o documentos legales de forma autom\u00e1tica.</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#c-pca-visualizando-el-hiperespacio","title":"C. PCA: Visualizando el Hiperespacio","text":"<p>Nuestra matriz TF-IDF tiene cientos de dimensiones (una por cada palabra \u00fanica). El ser humano solo puede ver en 2D o 3D.</p> <ul> <li>\u00bfQu\u00e9 significa?: Principal Component Analysis \"comprime\" la informaci\u00f3n. Busca las direcciones (componentes) donde los datos var\u00edan m\u00e1s y proyecta todo sobre ellas.</li> <li>Utilidad Did\u00e1ctica: Sin PCA, el clustering ser\u00eda una lista de n\u00fameros abstractos. Con PCA, podemos \"ver\" la separaci\u00f3n de conceptos en una gr\u00e1fica.</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#3-interpretacion-de-lo-que-estamos-viendo","title":"\ud83d\udcca 3. Interpretaci\u00f3n de lo que estamos viendo","text":"<p>Al ejecutar el c\u00f3digo, se genera la visualizaci\u00f3n <code>05_visualizacion_clustering.png</code>.</p> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#como-leer-este-grafico","title":"\u00bfC\u00f3mo leer este gr\u00e1fico?","text":"<ol> <li>Cercan\u00eda es Similitud: Dos puntos que est\u00e1n pegados significan documentos que comparten palabras clave y, por lo tanto, temas.</li> <li>Dispersi\u00f3n de Clusters:<ul> <li>Si los grupos est\u00e1n muy separados, el algoritmo ha tenido \u00e9xito total identificando temas \u00fanicos.</li> <li>Si hay solapamiento, indica que hay documentos que comparten vocabulario de varios temas (ej. un art\u00edculo sobre \"Tecnolog\u00eda en la Salud\").</li> </ul> </li> <li>Los Ejes (PCA): El Eje X (Componente 1) suele capturar la diferencia m\u00e1s grande entre los temas (ej. t\u00e9rminos m\u00e9dicos vs t\u00e9rminos financieros).</li> </ol>"},{"location":"dashboards/05_vectorizacion_y_clustering/#palabras-dominantes-por-cluster","title":"Palabras Dominantes por Cluster","text":"<p>Cada cluster tiene un \"perfil sem\u00e1ntico\" definido por las palabras que m\u00e1s contribuyen a su identidad:</p> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#seleccion-del-k-optimo-elbow-y-silhouette","title":"Selecci\u00f3n del k \u00d3ptimo (Elbow y Silhouette)","text":"<p>\u00bfC\u00f3mo sabemos cu\u00e1ntos clusters crear? Dos m\u00e9todos complementarios:</p> <ul> <li>M\u00e9todo del Codo (Elbow): Buscamos el punto donde la inercia deja de decrecer significativamente.</li> <li>Coeficiente de Silueta: Un valor cercano a 1 indica clusters bien separados.</li> </ul> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#analisis-de-silueta-por-cluster","title":"An\u00e1lisis de Silueta por Cluster","text":"<p>El an\u00e1lisis granular muestra la cohesi\u00f3n interna de cada cluster. Un ancho uniforme indica clusters bien definidos:</p> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#validacion-temas-reales-vs-clusters","title":"Validaci\u00f3n: Temas Reales vs Clusters","text":"<p>Los diagramas de Venn muestran la coincidencia entre los temas reales del corpus y los clusters detectados autom\u00e1ticamente. Un solapamiento del 100% confirma que K-Means ha identificado correctamente los t\u00f3picos:</p> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#4-caso-practico-clustering-del-titanic","title":"\ud83d\udea2 4. Caso Pr\u00e1ctico: Clustering del Titanic","text":"<p>Para demostrar que K-Means funciona m\u00e1s all\u00e1 del texto, aplicamos el mismo algoritmo al dataset Titanic con caracter\u00edsticas demogr\u00e1ficas (edad, tarifa, clase):</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#seleccion-de-k-para-el-titanic","title":"Selecci\u00f3n de k para el Titanic","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#clustering-de-pasajeros-pca","title":"Clustering de Pasajeros (PCA)","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#perfilado-de-clusters","title":"Perfilado de Clusters","text":"<p>Cada cluster revela un perfil de pasajero diferente (edad promedio, tarifa, tasa de supervivencia):</p> <p></p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#5-guia-didactica-paso-a-paso","title":"\ud83d\udc68\u200d\ud83c\udfeb 5. Gu\u00eda Did\u00e1ctica: Paso a Paso","text":""},{"location":"dashboards/05_vectorizacion_y_clustering/#paso-1-generacion-del-corpus","title":"Paso 1: Generaci\u00f3n del Corpus","text":"<p>Creamos 1,200 documentos sint\u00e9ticos. En una formaci\u00f3n real, esto simula la ingesta de datos de una API o una base de datos SQL.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#paso-2-limpieza-y-tokenizacion","title":"Paso 2: Limpieza y Tokenizaci\u00f3n","text":"<p>Aunque el script es directo, en NLP real eliminar\u00edamos puntuaci\u00f3n, convertir\u00edamos a min\u00fasculas y quitar\u00edamos Stop Words (palabras vac\u00edas) para que el modelo no se distraiga con ruido.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#paso-3-entrenamiento-del-modelo","title":"Paso 3: Entrenamiento del Modelo","text":"<p>El comando <code>kmeans.fit(tfidf_matrix)</code> es donde ocurre la \"magia\". El modelo \"aprende\" la estructura latente de los datos sin ayuda humana.</p>"},{"location":"dashboards/05_vectorizacion_y_clustering/#referencias-y-citas-academicas","title":"\ud83d\udcda Referencias y Citas Acad\u00e9micas","text":"<p>Para profundizar en la metodolog\u00eda, se recomienda la consulta de las siguientes fuentes fundamentales:</p> <ul> <li>McKinney, W. (2012). Python for Data Analysis. O'Reilly Media. (Referencia para manipulaci\u00f3n de matrices).</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR. (Documentaci\u00f3n oficial del framework utilizado).</li> <li>Manning, C. D., et al. (2008). Introduction to Information Retrieval. Cambridge University Press. (Teor\u00eda base de TF-IDF).</li> </ul>"},{"location":"dashboards/05_vectorizacion_y_clustering/#informacion-institucional","title":"\ud83c\udf93 Informaci\u00f3n Institucional","text":"<p>Autor/Referencia: @TodoEconometria Profesor: Juan Marcelo Gutierrez Miranda \u00c1rea: Big Data, Ciencia de Datos &amp; Econometr\u00eda Aplicada.  </p> <p>Hash ID de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code></p>"},{"location":"dashboards/06_analisis_panel_qog/","title":"Modulo 06: Analisis de Datos de Panel (QoG)","text":"<p>Dashboard interactivo con los resultados del pipeline Big Data sobre el Quality of Government Standard Dataset (University of Gothenburg, Enero 2024).</p> <p>Abrir Dashboard Interactivo</p>"},{"location":"dashboards/06_analisis_panel_qog/#contenido","title":"Contenido","text":"<p>El dashboard presenta 5 pestanas interactivas:</p> Pestana Linea de investigacion Paises Asia Central Evolucion institucional post-sovietica KAZ, UZB, TKM, KGZ, TJK Seguridad Hidrica Crisis del Mar de Aral KAZ, UZB, TKM, AFG, IRN Terrorismo Estabilidad politica y fragilidad estatal 13 paises (Europa + Asia Central + Oriente Medio) Maghreb Autoritarismo vs democracia, Primaveras Arabes DZA, MAR, TUN, LBY, EGY, MRT ML: Clusters PCA Clasificacion de 28 paises (K-Means + PCA) 28 paises de todas las lineas"},{"location":"dashboards/06_analisis_panel_qog/#infraestructura","title":"Infraestructura","text":"<ul> <li>Procesamiento: Apache Spark 3.5.4 (cluster Docker: 1 master + 2 workers)</li> <li>Almacenamiento: PostgreSQL 15 + Parquet</li> <li>Clustering: scikit-learn (K-Means k=5 + PCA 2 componentes)</li> <li>Visualizacion: Plotly (graficos interactivos)</li> <li>Dataset: 924 observaciones (28 paises x 33 anios), 40 variables</li> </ul>"},{"location":"dashboards/06_analisis_panel_qog/#variables-principales","title":"Variables principales","text":"Variable Fuente Que mide <code>vdem_polyarchy</code> V-Dem Indice de democracia electoral <code>ti_cpi</code> Transparency International Percepcion de corrupcion <code>wbgi_cce</code> World Bank Control de corrupcion <code>wbgi_rle</code> World Bank Estado de derecho <code>wbgi_pse</code> World Bank Estabilidad politica <code>wdi_gdppc</code> World Bank PIB per capita (USD 2015) <code>undp_hdi</code> UNDP Indice de Desarrollo Humano <code>ffp_fsi</code> Fund for Peace Indice de fragilidad estatal"},{"location":"dashboards/06_analisis_panel_qog/#codigo-fuente","title":"Codigo fuente","text":"<ul> <li>Pipeline ETL + ML: <code>ejercicios/06_an\u00e1lisis_datos_de_panel/</code></li> <li>Material teorico: <code>ejercicios/07_infraestructura_bigdata/</code></li> </ul>"},{"location":"dashboards/EJERCICIO_01_CONTEO/","title":"\ud83e\uddf6 Ejercicio 1: Anatom\u00eda del Texto y el Ritmo de las Palabras \ud83c\udfb8","text":"<p>\"Check out Guitar George, he knows-all the chords... \ud83e\udd18\" Envolvi\u00e9ndote en el ritmo de los Sultans of Swing, aprenderemos a diseccionar el lenguaje para encontrar su melod\u00eda oculta.</p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#el-ritual-de-inicio-objetivos","title":"\ud83c\udfaf El Ritual de Inicio (Objetivos)","text":"<p>Convertir un murmullo de palabras en una sinfon\u00eda de datos. En este primer paso del NLP (Procesamiento de Lenguaje Natural), aprender\u00e1s a:</p> <ol> <li>Exponer la Estructura: Entender que el texto no es solo letras, sino una arquitectura que debe ser unificada (Merge). \u26d3\ufe0f</li> <li>Normalizar la Frecuencia: Doblegar el texto a min\u00fasculas para que el algoritmo no se confunda entre \"\ud83c\udfb8 Solo\" y \"solo\". \u26a1</li> <li>Tokenizar el Caos: Usar expresiones regulares (<code>re.findall</code>) como un bistur\u00ed para separar cada t\u00e9rmino (token) del ruido ambiental. \u2694\ufe0f</li> <li>Contar el Pulso: Usar <code>Counter</code> para medir qu\u00e9 tan fuerte late cada palabra en el corpus. \ud83c\udf0c</li> </ol>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#la-maquina-de-diseccion","title":"\ud83c\udfd7\ufe0f La M\u00e1quina de Disecci\u00f3n","text":"<p>Imagina una trituradora de alta precisi\u00f3n que toma un pergamino antiguo y lo convierte en bloques de datos puros.</p> <p></p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#el-purgatorio-del-procesamiento","title":"El Purgatorio del Procesamiento","text":"<ol> <li>Unificaci\u00f3n: Juntamos todas las frases en un solo bloque de acero textual.</li> <li>Normalizaci\u00f3n: Aplicamos <code>lower()</code> para estandarizar la se\u00f1al. \u26a1</li> <li>Tokenizaci\u00f3n: Extraemos los tokens, eliminando la puntuaci\u00f3n que no aporta al \"riff\" principal.</li> </ol>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#resultados-del-directo-python-output","title":"\ud83c\udfb8 Resultados del Directo (Python Output)","text":"<p>Al ejecutar <code>01_conteo_palabras.py</code>, ver\u00e1s surgir el Top 10 de palabras que dominan el escenario.</p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#el-grafico-de-la-verdad","title":"El Gr\u00e1fico de la Verdad","text":"<p>Aqu\u00ed es donde visualizamos el espectro de frecuencias. \u00bfVes esas barras gigantes? Son las palabras que m\u00e1s se repiten.</p>"},{"location":"dashboards/EJERCICIO_01_CONTEO/#reflexion-de-backstage","title":"\u2694\ufe0f Reflexi\u00f3n de Backstage","text":"<ul> <li>\u00bfVes el ruido? Art\u00edculos como \"el\", \"la\" o \"de\" suelen dominar el gr\u00e1fico. Son como los acoples de un amplificador: est\u00e1n ah\u00ed, pero no son la melod\u00eda. \ud83c\udfc1</li> <li>Sultans of Swing: Al igual que en un buen solo de Knopfler, cada palabra tiene su lugar, pero algunas (las stopwords) aparecen demasiado y tapan el verdadero mensaje. \ud83c\udfb8</li> </ul> <p>Hash de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Master of Ceremonies: Juan Marcelo Gutierrez Miranda (@TodoEconometria) Vibe: Rock &amp; Data \ud83e\udd18\u26a1</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/","title":"\u26a1 Ejercicio 2: El Filtro de Ruido (Anti-Stopwords) \ud83d\udc80","text":"<p>\"You hear the guitar, it's a-clean and it's a-pure...\" \ud83c\udfb8 Como pasar de una distorsi\u00f3n ca\u00f3tica a un solo cristalino, en este ejercicio filtraremos el ruido del lenguaje para que brille la esencia.</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-desafio-limpiar-la-mezcla","title":"\ud83c\udfaf El Desaf\u00edo: Limpiar la Mezcla","text":"<p>En el ejercicio anterior vimos que las palabras m\u00e1s comunes son \"basura sem\u00e1ntica\" (stopwords). Tu misi\u00f3n es eliminarlas para que las palabras con peso real salgan a la superficie.</p> <ol> <li>Stopword Filtering: Activar el pedal de filtro para ignorar palabras como \"el\", \"es\", \"y\". \u26d3\ufe0f</li> <li>Impacto Visual: Comparar el \"Antes\" y el \"Despu\u00e9s\" para ver c\u00f3mo emerge el verdadero significado. \u26a1</li> <li>An\u00e1lisis de Sentimiento Primitivo: Al limpiar el ruido, palabras como \"fant\u00e1stico\" o \"terrible\" toman el protagonismo. \ud83e\udd18</li> </ol>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-pedal-de-filtro-stopword-filter","title":"\ud83c\udfd7\ufe0f El Pedal de Filtro (Stopword Filter)","text":"<p>Imagina que cada palabra com\u00fan es un acopio de est\u00e1tica. Nuestro algoritmo act\u00faa como un pedal de noise gate que solo deja pasar las frecuencias de alto impacto.</p> <p></p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-ritual-de-limpieza","title":"El Ritual de Limpieza","text":"<ul> <li>Input: Un texto sucio lleno de art\u00edculos y preposiciones.</li> <li>Filtro: Una lista negra de palabras prohibidas (<code>stopwords_es</code>). \u2694\ufe0f</li> <li>Output: Una se\u00f1al pura donde cada palabra cuenta una historia.</li> </ul>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#comparativa-del-escenario-python-data","title":"\ud83c\udfb8 Comparativa del Escenario (Python Data)","text":"<p>Al ejecutar <code>02_limpieza_texto.py</code>, ver\u00e1s el contraste brutal entre los dos mundos.</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#el-antes-vs-el-despues","title":"El Antes vs El Despu\u00e9s","text":"<p>Observa c\u00f3mo en el gr\u00e1fico izquierdo predominan las palabras vac\u00edas, mientras que en el derecho aparece el sentimiento puro.</p>"},{"location":"dashboards/EJERCICIO_02_LIMPIEZA/#profecia-para-el-analista","title":"\ud83c\udf11 Profec\u00eda para el Analista","text":"<ul> <li>El Silencio es Poder: Al eliminar el 70% de las palabras que no sirven, el an\u00e1lisis se vuelve 100% m\u00e1s preciso. \ud83c\udf0c</li> <li>Sultans of Data: Ahora que has limpiado la pista, estamos listos para el siguiente nivel: el an\u00e1lisis de sentimiento y la similitud. \ud83e\udd18</li> </ul> <p>Hash de Certificaci\u00f3n: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Master of Ceremonies: Juan Marcelo Gutierrez Miranda (@TodoEconometria) Vibe: Heavy Clean Sound \ud83c\udfb8\u26a1</p>"},{"location":"dashboards/arima-sarima-pro/","title":"Dashboard PRO: Series Temporales ARIMA/SARIMA","text":"<p>Dashboard interactivo con dise\u00f1o profesional inspirado en terminales financieras tipo Bloomberg, para el analisis completo de series temporales siguiendo la metodologia Box-Jenkins.</p> <p>Abrir Dashboard PRO</p>"},{"location":"dashboards/arima-sarima-pro/#caracteristicas-del-dashboard","title":"Caracteristicas del Dashboard","text":"Elemento Descripcion Tema Oscuro tipo terminal financiera (inspirado Bloomberg/OECD Explorer) KPIs Cards con metricas en tiempo real: RMSE, MAE, MAPE, R2, AIC Pestanas 7 secciones interactivas con transiciones suaves Responsivo Adaptable a diferentes tamanos de pantalla"},{"location":"dashboards/arima-sarima-pro/#contenido-por-pestana","title":"Contenido por Pestana","text":"# Pestana Descripcion 1 Serie Original Pasajeros aereos 1949-1960 con media movil 12M 2 Descomposicion Tendencia + Estacionalidad + Residuo (multiplicativa) 3 ACF / PACF Autocorrelacion de la serie diferenciada (d=1, D=1, s=12) 4 Diagnostico Residuos, histograma, ACF residual, Q-Q plot 5 Pronostico Serie + ajuste SARIMA + forecast 24 meses + IC 95% 6 Metricas Radar Visualizacion polar de las metricas normalizadas 7 Comparativa Comparacion de diferentes ordenes SARIMA"},{"location":"dashboards/arima-sarima-pro/#inspiracion-de-diseno","title":"Inspiracion de Diseno","text":"<p>Este dashboard fue creado siguiendo las mejores practicas de visualizacion financiera:</p> <ul> <li>OECD Pension Explorer - Plotly App oficial de la OECD</li> <li>Portfolio Optimizer - Panel/HoloViz Gallery</li> <li>Dash Bootstrap Templates - Temas profesionales para Dash</li> </ul>"},{"location":"dashboards/arima-sarima-pro/#metodologia-box-jenkins","title":"Metodologia Box-Jenkins","text":"<ol> <li>Identificacion: Analisis ACF/PACF para determinar ordenes (p, d, q)(P, D, Q)[s]</li> <li>Estimacion: Ajuste por maxima verosimilitud con SARIMAX</li> <li>Diagnostico: Tests de Ljung-Box, Jarque-Bera, Q-Q plot</li> <li>Pronostico: Forecast con intervalos de confianza al 95%</li> </ol>"},{"location":"dashboards/arima-sarima-pro/#codigo-fuente","title":"Codigo Fuente","text":"<ul> <li>Script de ejercicio: <code>ejercicios/04_machine_learning/07_series_temporales_arima/</code></li> <li>Exportador dashboard: <code>.profesor/soluciones/TRABAJO_FINAL/export_arima_pro.py</code></li> <li>Guia teorica: Series Temporales ARIMA</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Referencias academicas:</p> <ul> <li>Box, G.E.P. &amp; Jenkins, G.M. (1976). Time Series Analysis: Forecasting and Control. Holden-Day.</li> <li>Hyndman, R.J. &amp; Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3<sup>rd</sup> ed.). OTexts.</li> <li>Hamilton, J.D. (1994). Time Series Analysis. Princeton University Press.</li> </ul>"},{"location":"dashboards/dashboard_iss_tracker/","title":"ISS Tracker - Estacion Espacial Internacional","text":"<p>Seguimiento en tiempo real de la Estacion Espacial Internacional</p>"},{"location":"dashboards/dashboard_iss_tracker/#descripcion","title":"Descripcion","text":"<p>Este dashboard permite rastrear la posicion de la Estacion Espacial Internacional (ISS) en tiempo real, ver su trayectoria orbital, y predecir cuando pasara sobre tu ubicacion.</p>"},{"location":"dashboards/dashboard_iss_tracker/#caracteristicas","title":"Caracteristicas","text":"<ul> <li>Mapa en tiempo real con la posicion actual de la ISS</li> <li>Trayectoria orbital pasada y futura</li> <li>Predictor de pases: ingresa tu ciudad y descubre cuando ver la ISS</li> <li>Datos en vivo: latitud, longitud, altitud, velocidad</li> <li>Tripulacion actual: astronautas a bordo de la ISS</li> </ul>"},{"location":"dashboards/dashboard_iss_tracker/#ver-dashboard","title":"Ver Dashboard","text":"Abrir ISS Tracker"},{"location":"dashboards/dashboard_iss_tracker/#como-ver-la-iss","title":"Como Ver la ISS","text":"<p>La ISS es visible a simple vista cuando pasa sobre tu ubicacion durante la noche. Se ve como una estrella brillante movi\u00e9ndose rapidamente a traves del cielo.</p> <p>Consejos: 1. Usa el predictor del dashboard para saber cuando pasara 2. Busca un lugar oscuro, lejos de luces de la ciudad 3. Mira hacia la direccion indicada (N, NE, E, etc.) 4. La ISS aparecera como un punto brillante movi\u00e9ndose de horizonte a horizonte</p>"},{"location":"dashboards/dashboard_iss_tracker/#datos-de-la-iss","title":"Datos de la ISS","text":"Caracteristica Valor Altitud ~420 km sobre la Tierra Velocidad 27,600 km/h (7.66 km/s) Periodo orbital 92.68 minutos Orbitas por dia 15.5 Inclinacion 51.6 grados Tama\u00f1o 109m x 73m (campo de futbol)"},{"location":"dashboards/dashboard_iss_tracker/#apis-utilizadas","title":"APIs Utilizadas","text":"<ul> <li>Where The ISS At: Posicion en tiempo real</li> <li>Open Notify: Astronautas y posicion</li> <li>Nominatim: Geocodificacion de direcciones</li> </ul>"},{"location":"dashboards/dashboard_iss_tracker/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>LocalStack: Simulacion de AWS (S3, Lambda, DynamoDB)</li> <li>Terraform: Infraestructura como Codigo</li> <li>Kinesis: Streaming de posiciones</li> <li>Plotly: Visualizaciones interactivas</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria</p>"},{"location":"dashboards/dashboard_sismos_global/","title":"Observatorio Sismico Global","text":"<p>Dashboard interactivo de actividad sismica mundial en tiempo real</p>"},{"location":"dashboards/dashboard_sismos_global/#descripcion","title":"Descripcion","text":"<p>Este dashboard muestra la actividad sismica de las ultimas 24 horas utilizando datos del USGS Earthquake Hazards Program (Servicio Geologico de Estados Unidos).</p>"},{"location":"dashboards/dashboard_sismos_global/#caracteristicas","title":"Caracteristicas","text":"<ul> <li>Mapa global interactivo con todos los sismos detectados</li> <li>Estadisticas en tiempo real: total de sismos, magnitud maxima, promedio</li> <li>Timeline de actividad por hora</li> <li>Top 10 regiones con mayor actividad sismica</li> <li>Clasificacion por magnitud: Micro, Menor, Ligero, Moderado, Mayor</li> </ul>"},{"location":"dashboards/dashboard_sismos_global/#ver-dashboard","title":"Ver Dashboard","text":"Abrir Observatorio Sismico"},{"location":"dashboards/dashboard_sismos_global/#fuente-de-datos","title":"Fuente de Datos","text":"<p>Los datos provienen de la API publica del USGS:</p> <ul> <li>URL: earthquake.usgs.gov</li> <li>Formato: GeoJSON</li> <li>Actualizacion: Cada minuto</li> <li>Cobertura: Global</li> </ul>"},{"location":"dashboards/dashboard_sismos_global/#escala-de-magnitud","title":"Escala de Magnitud","text":"Categoria Magnitud Color Descripcion MICRO &lt; 2.5 Verde No se siente, solo detectado por instrumentos MENOR 2.5 - 4.0 Azul Generalmente no se siente, pero se registra LIGERO 4.0 - 5.0 Naranja Se siente, danos menores poco probables MODERADO 5.0 - 7.0 Rojo Puede causar danos significativos MAYOR &gt; 7.0 Morado Terremoto mayor, danos severos potenciales"},{"location":"dashboards/dashboard_sismos_global/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Apache Kafka: Streaming de datos en tiempo real</li> <li>Spark Structured Streaming: Procesamiento distribuido</li> <li>PostgreSQL: Almacenamiento de alertas</li> <li>Plotly: Visualizaciones interactivas</li> <li>USGS API: Fuente de datos oficial</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria</p>"},{"location":"dashboards/flores-transfer-learning/","title":"Dashboard: Clasificaci\u00f3n de Flores con Transfer Learning","text":""},{"location":"dashboards/flores-transfer-learning/#descripcion","title":"Descripci\u00f3n","text":"<p>Pipeline de Computer Vision que clasifica im\u00e1genes de flores usando Transfer Learning con MobileNetV2.</p> <p>\u00bfQu\u00e9 es Transfer Learning? En lugar de entrenar una red neuronal desde cero (necesitar\u00edamos millones de im\u00e1genes), usamos una red ya entrenada en ImageNet y la adaptamos a nuestro problema.</p>"},{"location":"dashboards/flores-transfer-learning/#pipeline","title":"Pipeline","text":"<pre><code>1. DESCARGA          2. EMBEDDINGS         3. CLASIFICACI\u00d3N      4. VISUALIZACI\u00d3N\n   3,670 flores         MobileNetV2           ML tradicional        Dashboard\n   5 clases             1280 features         KNN/SVM/RF            Plotly\n</code></pre>"},{"location":"dashboards/flores-transfer-learning/#resultados","title":"Resultados","text":"Modelo Accuracy SVM 89.9% Random Forest 86.5% KNN 86.2%"},{"location":"dashboards/flores-transfer-learning/#visualizaciones","title":"Visualizaciones","text":"<p>El dashboard incluye 4 pesta\u00f1as interactivas:</p> <ol> <li>t-SNE: Proyecci\u00f3n 2D de los embeddings - flores similares aparecen juntas</li> <li>Comparativa: Barras con accuracy de cada modelo</li> <li>Confusion Matrix: Aciertos/errores por clase (porcentajes)</li> <li>Distribuci\u00f3n: Radar chart del dataset</li> </ol>"},{"location":"dashboards/flores-transfer-learning/#ver-dashboard","title":"Ver Dashboard","text":"Abrir Dashboard Interactivo"},{"location":"dashboards/flores-transfer-learning/#ejecutar-el-ejercicio","title":"Ejecutar el Ejercicio","text":"<pre><code>cd ejercicios/04_machine_learning/flores_transfer_learning/\npip install -r requirements.txt\npython 01_flores_transfer_learning.py\n</code></pre> <p>Requisitos: TensorFlow (GPU recomendado pero funciona en CPU)</p> <p>Curso: Big Data con Python - De Cero a Producci\u00f3n Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Referencias acad\u00e9micas:</p> <ul> <li>Sandler, M., et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR.</li> <li>Yosinski, J., et al. (2014). How transferable are features in deep neural networks? NeurIPS.</li> <li>van der Maaten, L. &amp; Hinton, G. (2008). Visualizing Data using t-SNE. JMLR.</li> </ul>"},{"location":"ejercicios/","title":"Ejercicios","text":"<p>Lista completa de todos los ejercicios disponibles en el curso.</p>"},{"location":"ejercicios/#roadmap-de-ejercicios","title":"Roadmap de Ejercicios","text":""},{"location":"ejercicios/#modulo-1-bases-de-datos","title":"Modulo 1: Bases de Datos","text":"# Ejercicio Tecnologia Nivel Estado 1.1 Introduccion SQLite SQLite + Pandas Basico Disponible 2.1 PostgreSQL HR PostgreSQL Intermedio Disponible 2.2 PostgreSQL Jardineria PostgreSQL Intermedio Disponible 2.3 Migracion SQLite a PostgreSQL PostgreSQL + Python Intermedio Disponible 3.1 Oracle HR Oracle Database Avanzado Disponible 5.1 Analisis Excel/Python Pandas + Excel Basico Disponible"},{"location":"ejercicios/#modulo-2-limpieza-de-datos-y-etl","title":"Modulo 2: Limpieza de Datos y ETL","text":"# Ejercicio Tecnologia Nivel Estado 02 Pipeline ETL QoG PostgreSQL + Pandas Avanzado Disponible"},{"location":"ejercicios/#modulo-3-procesamiento-distribuido","title":"Modulo 3: Procesamiento Distribuido","text":"# Ejercicio Tecnologia Nivel Estado 03 Procesamiento Distribuido con Dask Dask + Parquet Intermedio Disponible"},{"location":"ejercicios/#modulo-4-machine-learning","title":"Modulo 4: Machine Learning","text":"# Ejercicio Tecnologia Nivel Estado 04 Machine Learning (PCA, K-Means) Scikit-Learn, PCA, K-Means Avanzado Disponible 04.2 Transfer Learning Flores TensorFlow, MobileNetV2 Avanzado Disponible ARIMA Series Temporales ARIMA/SARIMA statsmodels, Box-Jenkins Avanzado Disponible"},{"location":"ejercicios/#modulo-5-nlp-y-text-mining","title":"Modulo 5: NLP y Text Mining","text":"# Ejercicio Tecnologia Nivel Estado 05 NLP y Text Mining NLTK, TF-IDF, Jaccard, Sentimiento Avanzado Disponible"},{"location":"ejercicios/#modulo-6-analisis-de-datos-de-panel","title":"Modulo 6: Analisis de Datos de Panel","text":"# Ejercicio Tecnologia Nivel Estado 06 Analisis de Datos de Panel linearmodels, Panel OLS, Altair Avanzado Disponible"},{"location":"ejercicios/#modulo-7-infraestructura-big-data","title":"Modulo 7: Infraestructura Big Data","text":"# Ejercicio Tecnologia Nivel Estado 07 Infraestructura Big Data Docker Compose, Apache Spark Intermedio-Avanzado Disponible"},{"location":"ejercicios/#modulo-8-streaming-con-kafka","title":"Modulo 8: Streaming con Kafka","text":"# Ejercicio Tecnologia Nivel Estado 08 Streaming con Kafka Apache Kafka, Spark Streaming, KRaft Avanzado Disponible"},{"location":"ejercicios/#modulo-9-cloud-con-localstack","title":"Modulo 9: Cloud con LocalStack","text":"# Ejercicio Tecnologia Nivel Estado 09 Cloud con LocalStack LocalStack, Terraform, AWS Avanzado Disponible"},{"location":"ejercicios/#trabajo-final","title":"Trabajo Final","text":"# Ejercicio Tecnologia Nivel Estado TF Proyecto Final Integrador Docker + Spark + PostgreSQL + QoG Avanzado Disponible"},{"location":"ejercicios/#modulo-1-bases-de-datos_1","title":"MODULO 1: Bases de Datos","text":""},{"location":"ejercicios/#ejercicio-11-introduccion-a-sqlite","title":"Ejercicio 1.1: Introduccion a SQLite","text":"<p>Detalles</p> <ul> <li>Nivel: Basico</li> <li>Dataset: NYC Taxi (muestra 10MB)</li> <li>Tecnologias: SQLite, Pandas</li> </ul> <p>Que aprenderas:</p> <ul> <li>Cargar datos CSV a base de datos SQLite</li> <li>Queries SQL basicas (SELECT, WHERE, GROUP BY)</li> <li>Optimizacion con indices</li> <li>Exportar resultados a CSV</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-21-postgresql-con-bd-hr","title":"Ejercicio 2.1: PostgreSQL con BD HR","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Base de Datos: HR (Human Resources) de Oracle</li> <li>Tecnologias: PostgreSQL, SQL</li> </ul> <p>Que aprenderas:</p> <ul> <li>Instalar y configurar PostgreSQL</li> <li>Cargar bases de datos desde scripts SQL</li> <li>Consultas complejas con multiples JOINs</li> <li>Funciones especificas de PostgreSQL</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-22-postgresql-jardineria","title":"Ejercicio 2.2: PostgreSQL Jardineria","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Base de Datos: Sistema de ventas de jardineria</li> <li>Tecnologias: PostgreSQL, Window Functions</li> </ul> <p>Que aprenderas:</p> <ul> <li>Analisis de ventas con SQL</li> <li>Agregaciones complejas (GROUP BY, HAVING)</li> <li>Window Functions para rankings</li> <li>Vistas materializadas</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-23-migracion-sqlite-a-postgresql","title":"Ejercicio 2.3: Migracion SQLite a PostgreSQL","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Tecnologias: SQLite, PostgreSQL, Python</li> </ul> <p>Que aprenderas:</p> <ul> <li>Diferencias entre motores de BD</li> <li>Migrar esquemas y datos</li> <li>Adaptar tipos de datos</li> <li>Validar integridad</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-31-oracle-con-bd-hr","title":"Ejercicio 3.1: Oracle con BD HR","text":"<p>Avanzado</p> <ul> <li>Nivel: Avanzado</li> <li>Base de Datos: HR en Oracle nativo</li> <li>Tecnologias: Oracle Database, PL/SQL</li> </ul> <p>Que aprenderas:</p> <ul> <li>Instalar Oracle Database XE</li> <li>Sintaxis especifica de Oracle</li> <li>PL/SQL (procedimientos, funciones)</li> <li>Secuencias y triggers</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#ejercicio-51-analisis-excelpython","title":"Ejercicio 5.1: Analisis Excel/Python","text":"<p>Detalles</p> <ul> <li>Nivel: Basico-Intermedio</li> <li>Tecnologias: Python, Pandas, Excel</li> </ul> <p>Que aprenderas:</p> <ul> <li>Leer archivos Excel con Python</li> <li>Analisis exploratorio de datos (EDA)</li> <li>Visualizaciones con matplotlib/seaborn</li> <li>Automatizar analisis</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-2-limpieza-de-datos-y-etl_1","title":"MODULO 2: Limpieza de Datos y ETL","text":""},{"location":"ejercicios/#pipeline-etl-profesional-quality-of-government","title":"Pipeline ETL Profesional - Quality of Government","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Dataset: QoG (1289 variables, 194+ paises)</li> <li>Tecnologias: PostgreSQL, Pandas, psycopg2</li> </ul> <p>Que aprenderas:</p> <ul> <li>Disenar arquitectura ETL modular</li> <li>Trabajar con PostgreSQL para analisis longitudinal</li> <li>Limpiar datasets complejos (&gt;1000 variables)</li> <li>Preparar datos de panel para econometria</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-3-procesamiento-distribuido_1","title":"MODULO 3: Procesamiento Distribuido","text":""},{"location":"ejercicios/#procesamiento-distribuido-con-dask","title":"Procesamiento Distribuido con Dask","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio</li> <li>Tecnologias: Dask, Parquet, LocalCluster</li> </ul> <p>Que aprenderas:</p> <ul> <li>Configurar un Cluster Local con Dask</li> <li>Leer archivos Parquet de forma particionada</li> <li>Ejecutar agregaciones complejas en paralelo</li> <li>Comparar rendimiento vs Pandas</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-4-machine-learning_1","title":"MODULO 4: Machine Learning","text":""},{"location":"ejercicios/#machine-learning-en-big-data","title":"Machine Learning en Big Data","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: Scikit-Learn, PCA, K-Means</li> <li>Scripts: PCA Iris, FactoMineR, Breast Cancer, Wine, TF-IDF</li> </ul> <p>Que aprenderas:</p> <ul> <li>Reduccion de dimensionalidad con PCA</li> <li>Clustering con K-Means y Hierarchical Clustering</li> <li>Interpretacion de componentes principales</li> <li>Perfilado de clusters</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#transfer-learning-clasificacion-de-flores","title":"Transfer Learning: Clasificacion de Flores","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: TensorFlow, MobileNetV2, Scikit-Learn</li> <li>Dataset: TensorFlow Flowers (3,670 imagenes, 5 clases)</li> </ul> <p>Que aprenderas:</p> <ul> <li>Transfer Learning con redes pre-entrenadas (ImageNet)</li> <li>Extraccion de embeddings con CNNs</li> <li>Clasificacion de imagenes con ML tradicional (KNN, SVM, Random Forest)</li> <li>Visualizacion t-SNE de espacios de alta dimension</li> </ul> <p>Ver Dashboard Interactivo</p>"},{"location":"ejercicios/#series-temporales-arimasarima","title":"Series Temporales: ARIMA/SARIMA","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Dataset: AirPassengers (144 observaciones, 1949-1960)</li> <li>Tecnologias: statsmodels, Metodologia Box-Jenkins</li> </ul> <p>Que aprenderas:</p> <ul> <li>Metodologia Box-Jenkins completa (Identificacion, Estimacion, Diagnostico, Pronostico)</li> <li>Modelos ARIMA y SARIMA con estacionalidad</li> <li>ACF/PACF para identificacion de ordenes</li> <li>Diagnostico de residuos y pronosticos</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-5-nlp-y-text-mining_1","title":"MODULO 5: NLP y Text Mining","text":""},{"location":"ejercicios/#nlp-y-text-mining","title":"NLP y Text Mining","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: NLTK, TF-IDF, Jaccard, Sentiment Analysis</li> <li>Scripts: Conteo, Limpieza, Sentimiento, Similitud</li> </ul> <p>Que aprenderas:</p> <ul> <li>Tokenizacion y limpieza de texto</li> <li>Eliminacion de stopwords</li> <li>Similitud de Jaccard entre documentos</li> <li>Analisis de sentimiento por lexicon</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-6-analisis-de-datos-de-panel_1","title":"MODULO 6: Analisis de Datos de Panel","text":""},{"location":"ejercicios/#analisis-de-datos-de-panel","title":"Analisis de Datos de Panel","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Datasets: Guns (leyes de armas), Fatalities (mortalidad trafico)</li> <li>Tecnologias: linearmodels, Panel OLS, Altair</li> </ul> <p>Que aprenderas:</p> <ul> <li>Datos de panel: estructura pais x anio</li> <li>Efectos Fijos vs Efectos Aleatorios</li> <li>Two-Way Fixed Effects</li> <li>Test de Hausman para seleccion de modelo</li> <li>Odds Ratios y Efectos Marginales</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-7-infraestructura-big-data_1","title":"MODULO 7: Infraestructura Big Data","text":""},{"location":"ejercicios/#infraestructura-big-data-docker-y-spark","title":"Infraestructura Big Data: Docker y Spark","text":"<p>Detalles</p> <ul> <li>Nivel: Intermedio-Avanzado</li> <li>Tipo: Teorico-Conceptual con ejemplos practicos</li> <li>Tecnologias: Docker, Docker Compose, Apache Spark</li> </ul> <p>Que aprenderas:</p> <ul> <li>Docker: contenedores, imagenes, Dockerfile, orquestacion con Compose</li> <li>Redes, volumenes, healthchecks, patrones de produccion</li> <li>Apache Spark: arquitectura Master-Worker, cluster con Docker</li> <li>SparkSession, Lazy Evaluation, DAG, optimizador Catalyst</li> <li>Spark + PostgreSQL via JDBC</li> <li>De Standalone a produccion (Kubernetes, EMR, Dataproc)</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-8-streaming-con-kafka_1","title":"MODULO 8: Streaming con Kafka","text":""},{"location":"ejercicios/#streaming-con-apache-kafka","title":"Streaming con Apache Kafka","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: Apache Kafka (KRaft), Python, Spark Streaming</li> <li>API: USGS Earthquakes (tiempo real)</li> </ul> <p>Que aprenderas:</p> <ul> <li>Arquitectura de Kafka: Brokers, Topics, Partitions</li> <li>Modo KRaft (sin ZooKeeper)</li> <li>Productores y Consumidores en Python</li> <li>Spark Structured Streaming</li> <li>Sistema de alertas en tiempo real</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#modulo-9-cloud-con-localstack_1","title":"MODULO 9: Cloud con LocalStack","text":""},{"location":"ejercicios/#cloud-con-localstack-y-terraform","title":"Cloud con LocalStack y Terraform","text":"<p>Detalles</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: LocalStack, Terraform, AWS (S3, Lambda, DynamoDB)</li> <li>API: ISS Tracker (tiempo real)</li> </ul> <p>Que aprenderas:</p> <ul> <li>Cloud Computing: IaaS, PaaS, SaaS</li> <li>Simular AWS localmente con LocalStack</li> <li>Infraestructura como Codigo con Terraform</li> <li>Funciones Lambda serverless</li> <li>Arquitectura Data Lake (Medallion)</li> </ul> <p>Ver Ejercicio Completo</p>"},{"location":"ejercicios/#trabajo-final_1","title":"TRABAJO FINAL","text":""},{"location":"ejercicios/#proyecto-final-pipeline-de-big-data-con-docker","title":"Proyecto Final: Pipeline de Big Data con Docker","text":"<p>Proyecto Integrador</p> <ul> <li>Nivel: Avanzado</li> <li>Tecnologias: Docker, Apache Spark, PostgreSQL, QoG</li> <li>Evaluacion: Infraestructura 30% + ETL 25% + Analisis 25% + Reflexion IA 20%</li> </ul> <p>Que haras:</p> <ul> <li>Construir infraestructura Docker (Spark + PostgreSQL)</li> <li>Disenar y ejecutar un pipeline ETL con Apache Spark</li> <li>Analizar datos QoG con pregunta de investigacion propia</li> <li>Documentar tu proceso de aprendizaje con IA</li> </ul> <p>Ver Enunciado Completo</p>"},{"location":"ejercicios/#datasets-utilizados","title":"Datasets Utilizados","text":""},{"location":"ejercicios/#nyc-taxi-limousine-commission-tlc","title":"NYC Taxi &amp; Limousine Commission (TLC)","text":"<ul> <li>Fuente: NYC Open Data</li> <li>Periodo: 2021</li> <li>Registros: 10M+ viajes</li> </ul>"},{"location":"ejercicios/#quality-of-government-qog","title":"Quality of Government (QoG)","text":"<ul> <li>Fuente: Universidad de Gotemburgo</li> <li>Variables: 1289 indicadores de calidad institucional</li> <li>Paises: 194+ con datos desde 1946</li> </ul>"},{"location":"ejercicios/#airpassengers","title":"AirPassengers","text":"<ul> <li>Fuente: Box &amp; Jenkins (1976)</li> <li>Periodo: 1949-1960 (144 observaciones mensuales)</li> <li>Uso: Series temporales ARIMA/SARIMA</li> </ul>"},{"location":"ejercicios/#como-trabajar-los-ejercicios","title":"Como Trabajar los Ejercicios","text":""},{"location":"ejercicios/#flujo-recomendado","title":"Flujo Recomendado","text":"<ol> <li>Leer el enunciado completo - No empieces a codear sin leer todo</li> <li>Entender los objetivos - Que se espera que logres?</li> <li>Crear rama de trabajo - <code>git checkout -b tu-apellido-ejercicio-XX</code></li> <li>Trabajar en pasos pequenos - No intentes hacerlo todo de una vez</li> <li>Probar frecuentemente - Ejecuta tu codigo cada vez que completes una parte</li> <li>Hacer commits regulares - Guarda tu progreso frecuentemente</li> <li>Subir con git push - Cuando completes, el sistema evalua tu PROMPTS.md</li> </ol>"},{"location":"ejercicios/#proximos-pasos","title":"Proximos Pasos","text":"<p>Empieza con el primer ejercicio:</p> <p>Ejercicio 01: Introduccion SQLite</p> <p>O salta al proyecto final:</p> <p>Trabajo Final: Pipeline Big Data</p>"},{"location":"ejercicios/01-introduccion-sqlite/","title":"Ejercicio 01: Introduccion a SQLite","text":"<p>Aprende a cargar y consultar datos usando SQLite y Pandas.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#informacion-general","title":"Informacion General","text":"Campo Valor Nivel \ud83d\udfe2 Basico Tiempo estimado 2-3 horas Tecnologias Python, SQLite, Pandas Dataset NYC Taxi (muestra 10MB) Prerequisitos Python basico, conocimientos de SQL basico"},{"location":"ejercicios/01-introduccion-sqlite/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio, seras capaz de:</p> <ul> <li> Cargar archivos CSV grandes a SQLite usando chunks</li> <li> Crear y gestionar bases de datos SQLite</li> <li> Ejecutar queries SQL basicas y avanzadas</li> <li> Optimizar rendimiento con indices</li> <li> Exportar resultados de queries a CSV</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#el-problema","title":"El Problema","text":"<p>Tienes un archivo CSV con 100,000 registros de viajes de taxi de NYC. Si intentas cargarlo todo en memoria con Pandas, tu computadora puede quedarse sin memoria.</p> <p>Tu mision: Cargar estos datos a una base de datos SQLite de forma eficiente y realizar analisis.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#dataset","title":"Dataset","text":""},{"location":"ejercicios/01-introduccion-sqlite/#nyc-taxi-trip-records","title":"NYC Taxi Trip Records","text":"<ul> <li>Archivo: <code>datos/muestra_taxi.csv</code></li> <li>Tamano: ~10 MB</li> <li>Registros: ~100,000 viajes</li> <li>Periodo: Enero 2021</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#estructura-de-datos","title":"Estructura de Datos","text":"<pre><code>Columnas:\n- tpep_pickup_datetime    # Fecha/hora de inicio del viaje\n- tpep_dropoff_datetime   # Fecha/hora de fin del viaje\n- passenger_count         # Numero de pasajeros\n- trip_distance           # Distancia en millas\n- pickup_longitude        # Longitud de origen\n- pickup_latitude         # Latitud de origen\n- dropoff_longitude       # Longitud de destino\n- dropoff_latitude        # Latitud de destino\n- payment_type            # Tipo de pago (1=Credit, 2=Cash, ...)\n- fare_amount             # Tarifa base\n- tip_amount              # Propina\n- total_amount            # Total pagado\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#tareas","title":"Tareas","text":""},{"location":"ejercicios/01-introduccion-sqlite/#tarea-1-cargar-csv-a-sqlite-en-chunks","title":"Tarea 1: Cargar CSV a SQLite en Chunks","text":"<p>Objetivo: Cargar el CSV completo a SQLite sin quedarte sin memoria.</p> <p>Requisitos:</p> <ul> <li>Usar <code>pandas.read_csv()</code> con parametro <code>chunksize</code></li> <li>Procesar el CSV en chunks de 10,000 registros</li> <li>Insertar cada chunk en la tabla <code>trips</code></li> <li>Mostrar progreso de carga</li> </ul> <p>Ejemplo de codigo inicial:</p> <pre><code>import sqlite3\nimport pandas as pd\n\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"\n    Carga un CSV grande a SQLite en chunks\n\n    Args:\n        csv_path: Ruta al archivo CSV\n        db_path: Ruta a la base de datos SQLite\n        chunksize: Numero de registros por chunk\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # TODO: Implementar carga por chunks\n    # Pista: usa pd.read_csv con chunksize\n    # y itera sobre los chunks\n\n    conn.close()\n</code></pre> <p>Pista</p> <pre><code>chunks = pd.read_csv(csv_path, chunksize=chunksize)\nfor i, chunk in enumerate(chunks):\n    # Insertar chunk en SQLite\n    chunk.to_sql('trips', conn, if_exists='append', index=False)\n    print(f\"Chunk {i+1} cargado\")\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#tarea-2-crear-indices","title":"Tarea 2: Crear Indices","text":"<p>Objetivo: Optimizar queries agregando indices a columnas frecuentemente consultadas.</p> <p>Requisitos:</p> <ul> <li>Crear indice en <code>tpep_pickup_datetime</code></li> <li>Crear indice en <code>payment_type</code></li> <li>Medir tiempo de query antes y despues de indices</li> </ul> <p>Ejemplo:</p> <pre><code>def crear_indices(db_path):\n    \"\"\"\n    Crea indices para optimizar queries\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # TODO: Crear indices\n    # Ejemplo:\n    # cursor.execute(\"CREATE INDEX idx_pickup ON trips(tpep_pickup_datetime)\")\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#tarea-3-queries-de-analisis","title":"Tarea 3: Queries de Analisis","text":"<p>Objetivo: Extraer insights de los datos usando SQL.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#query-1-ingresos-promedio-por-hora-del-dia","title":"Query 1: Ingresos Promedio por Hora del Dia","text":"<p>Calcular el ingreso promedio para cada hora del dia.</p> <p>SQL esperado:</p> <pre><code>SELECT\n    strftime('%H', tpep_pickup_datetime) as hora,\n    AVG(total_amount) as promedio_ingreso,\n    COUNT(*) as num_viajes\nFROM trips\nGROUP BY hora\nORDER BY hora\n</code></pre> <p>Resultado esperado:</p> <pre><code>hora  promedio_ingreso  num_viajes\n00    15.23             2340\n01    14.89             1982\n02    16.45             1657\n...\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#query-2-distribucion-de-metodos-de-pago","title":"Query 2: Distribucion de Metodos de Pago","text":"<p>Calcular el porcentaje de cada metodo de pago.</p> <pre><code>SELECT\n    payment_type,\n    COUNT(*) as total,\n    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM trips), 2) as porcentaje\nFROM trips\nGROUP BY payment_type\nORDER BY total DESC\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#query-3-top-10-rutas-mas-rentables","title":"Query 3: Top 10 Rutas Mas Rentables","text":"<p>Encontrar las rutas (pickup \u2192 dropoff) con mayor ingreso promedio.</p> <p>Desafio</p> <p>Esta query requiere agrupar por coordenadas redondeadas. Piensa como hacerlo.</p>"},{"location":"ejercicios/01-introduccion-sqlite/#tarea-4-exportar-resultados","title":"Tarea 4: Exportar Resultados","text":"<p>Objetivo: Guardar los resultados de tus analisis en archivos CSV.</p> <pre><code>def exportar_resultados(db_path, query, output_path):\n    \"\"\"\n    Ejecuta una query y exporta a CSV\n\n    Args:\n        db_path: Ruta a la BD SQLite\n        query: Query SQL a ejecutar\n        output_path: Ruta del CSV de salida\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # TODO: Ejecutar query y exportar\n    # Pista: usa pd.read_sql_query() y df.to_csv()\n\n    conn.close()\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#criterios-de-evaluacion","title":"Criterios de Evaluacion","text":""},{"location":"ejercicios/01-introduccion-sqlite/#funcionalidad-40-puntos","title":"Funcionalidad (40 puntos)","text":"<ul> <li> Carga completa de datos sin errores (10 pts)</li> <li> Indices creados correctamente (10 pts)</li> <li> Queries ejecutan y devuelven resultados correctos (15 pts)</li> <li> Exportacion funciona (5 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#codigo-limpio-30-puntos","title":"Codigo Limpio (30 puntos)","text":"<ul> <li> Funciones bien documentadas (10 pts)</li> <li> Codigo legible y organizado (10 pts)</li> <li> Manejo de errores (10 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#rendimiento-20-puntos","title":"Rendimiento (20 puntos)","text":"<ul> <li> Carga eficiente con chunks (10 pts)</li> <li> Indices mejoran rendimiento medible (10 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#analisis-10-puntos","title":"Analisis (10 puntos)","text":"<ul> <li> Interpretacion de resultados (5 pts)</li> <li> Insights adicionales (5 pts)</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#entregables","title":"Entregables","text":"<p>Debes entregar subiendo a tu fork (git push):</p> <ol> <li>Codigo Python: <code>01_cargar_sqlite.py</code></li> <li>Base de datos: <code>datos/taxi.db</code> (NO subir a GitHub, muy grande)</li> <li>Resultados: Carpeta <code>resultados/</code> con CSVs exportados</li> <li>Analisis: <code>ANALISIS.md</code> con tus hallazgos</li> </ol>"},{"location":"ejercicios/01-introduccion-sqlite/#estructura-de-carpeta","title":"Estructura de carpeta","text":"<pre><code>entregas/01_sqlite/tu_apellido_nombre/\n\u251c\u2500\u2500 01_cargar_sqlite.py\n\u251c\u2500\u2500 resultados/\n\u2502   \u251c\u2500\u2500 ingresos_por_hora.csv\n\u2502   \u251c\u2500\u2500 distribucion_pagos.csv\n\u2502   \u2514\u2500\u2500 top_rutas.csv\n\u2514\u2500\u2500 ANALISIS.md\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#ejemplo-de-solucion-parcial","title":"Ejemplo de Solucion (Parcial)","text":"<p>Codigo de Ejemplo</p> <pre><code>import sqlite3\nimport pandas as pd\nimport time\n\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"Carga CSV a SQLite en chunks\"\"\"\n    conn = sqlite3.connect(db_path)\n\n    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n\n    for i, chunk in enumerate(chunks):\n        chunk.to_sql('trips', conn, if_exists='append', index=False)\n        print(f\"Chunk {i+1} cargado ({len(chunk)} registros)\")\n\n    conn.close()\n    print(\"Carga completa!\")\n\ndef crear_indices(db_path):\n    \"\"\"Crea indices para optimizacion\"\"\"\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    print(\"Creando indices...\")\n    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_pickup ON trips(tpep_pickup_datetime)\")\n    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_payment ON trips(payment_type)\")\n\n    conn.commit()\n    conn.close()\n    print(\"Indices creados!\")\n\ndef analizar_ingresos_por_hora(db_path):\n    \"\"\"Query: Ingresos promedio por hora\"\"\"\n    conn = sqlite3.connect(db_path)\n\n    query = \"\"\"\n        SELECT\n            strftime('%H', tpep_pickup_datetime) as hora,\n            ROUND(AVG(total_amount), 2) as promedio_ingreso,\n            COUNT(*) as num_viajes\n        FROM trips\n        GROUP BY hora\n        ORDER BY hora\n    \"\"\"\n\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return df\n\nif __name__ == \"__main__\":\n    # Rutas\n    csv_path = \"datos/muestra_taxi.csv\"\n    db_path = \"datos/taxi.db\"\n\n    # 1. Cargar datos\n    print(\"=== CARGANDO DATOS ===\")\n    start = time.time()\n    cargar_datos_sqlite(csv_path, db_path)\n    print(f\"Tiempo: {time.time() - start:.2f} segundos\\n\")\n\n    # 2. Crear indices\n    print(\"=== CREANDO INDICES ===\")\n    crear_indices(db_path)\n    print()\n\n    # 3. Analisis\n    print(\"=== ANALISIS: INGRESOS POR HORA ===\")\n    df = analizar_ingresos_por_hora(db_path)\n    print(df)\n\n    # Exportar\n    df.to_csv(\"resultados/ingresos_por_hora.csv\", index=False)\n    print(\"\\nResultados exportados!\")\n</code></pre>"},{"location":"ejercicios/01-introduccion-sqlite/#preguntas-para-reflexion","title":"Preguntas para Reflexion","text":"<p>Responde en tu <code>ANALISIS.md</code>:</p> <ol> <li> <p>Rendimiento:</p> <ul> <li>Cuanto tiempo tomo cargar 100,000 registros?</li> <li>Cuanto mejoraron los indices el tiempo de query?</li> </ul> </li> <li> <p>Insights:</p> <ul> <li>A que hora del dia los taxis ganan mas?</li> <li>Cual es el metodo de pago mas comun?</li> <li>Que patron observas en los datos?</li> </ul> </li> <li> <p>Mejoras:</p> <ul> <li>Como optimizarias aun mas la carga?</li> <li>Que otros analisis se podrian hacer?</li> <li>Que limitaciones tiene SQLite para este dataset?</li> </ul> </li> </ol>"},{"location":"ejercicios/01-introduccion-sqlite/#recursos-adicionales","title":"Recursos Adicionales","text":""},{"location":"ejercicios/01-introduccion-sqlite/#documentacion","title":"Documentacion","text":"<ul> <li>SQLite Python Tutorial</li> <li>Pandas to_sql Documentation</li> <li>SQL Tutorial</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#tutoriales","title":"Tutoriales","text":"<ul> <li>Working with Large CSV Files</li> <li>SQLite Indexes</li> </ul>"},{"location":"ejercicios/01-introduccion-sqlite/#problemas-comunes","title":"Problemas Comunes","text":"Error: MemoryError al cargar CSV <p>Causa: Intentando cargar todo el archivo de una vez.</p> <p>Solucion: Usa chunks: <pre><code>chunks = pd.read_csv(csv_path, chunksize=10000)\n</code></pre></p> Query muy lenta <p>Causa: Falta indice en columnas consultadas.</p> <p>Solucion: Crea indices: <pre><code>cursor.execute(\"CREATE INDEX idx_nombre ON tabla(columna)\")\n</code></pre></p> Error: database is locked <p>Causa: Otro proceso tiene la BD abierta.</p> <p>Solucion: - Cierra todas las conexiones: <code>conn.close()</code> - Asegurate de no tener la BD abierta en otro programa</p>"},{"location":"ejercicios/01-introduccion-sqlite/#el-panorama-completo-ecosistemas-de-datos","title":"El Panorama Completo: Ecosistemas de Datos","text":"<p>SQLite es solo el primer paso. En el curso exploraras ecosistemas completos de bases de datos:</p>"},{"location":"ejercicios/01-introduccion-sqlite/#sql-vs-nosql","title":"SQL vs NoSQL","text":""},{"location":"ejercicios/01-introduccion-sqlite/#modelos-clave-valor-y-columnar","title":"Modelos Clave-Valor y Columnar","text":""},{"location":"ejercicios/01-introduccion-sqlite/#el-ecosistema-mongodb","title":"El Ecosistema MongoDB","text":""},{"location":"ejercicios/01-introduccion-sqlite/#proximos-pasos","title":"Proximos Pasos","text":"<p>Una vez completado este ejercicio:</p> <ul> <li>Ejercicio 02: Limpieza de Datos - Siguiente ejercicio</li> <li>Guia de Entregas - Como entregar tu trabajo</li> <li>Roadmap - Ver todos los ejercicios</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/","title":"Ejercicio 02: Pipeline ETL Profesional - Quality of Government","text":"<p>Nivel: Avanzado | Duraci\u00f3n: 15-20 horas | Modalidad: Grupal o Individual</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#descripcion-general","title":"Descripcion General","text":"<p>Construiras un pipeline ETL profesional trabajando con el dataset Quality of Government, una base de datos longitudinal con mas de 1000 variables sobre calidad institucional, democracia y desarrollo economico.</p> <p>Objetivo: Aplicar tecnicas de ingenieria de software y ciencia de datos para limpiar, transformar y analizar datos reales de investigacion academica.</p> <p></p>"},{"location":"ejercicios/02-pipeline-etl-qog/#el-ciclo-de-vida-del-dato","title":"El Ciclo de Vida del Dato","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#dataset-quality-of-government-qog","title":"Dataset: Quality of Government (QoG)","text":"<p>\u00bfQu\u00e9 es?</p> <p>Base de datos mantenida por la Universidad de Gotemburgo que agrega variables de m\u00faltiples fuentes internacionales.</p> <p>Caracter\u00edsticas: - 1289 variables de calidad institucional, econom\u00eda, sociedad - 194+ pa\u00edses con datos desde 1946 - Fuentes: World Bank, V-Dem, Transparency International, Freedom House, UNDP</p> <p>Fuente: https://www.qog.pol.gu.se/</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>\u2705 Dise\u00f1ar arquitectura ETL modular</li> <li>\u2705 Trabajar con PostgreSQL para an\u00e1lisis longitudinal</li> <li>\u2705 Limpiar datasets complejos (&gt;1000 variables)</li> <li>\u2705 Preparar datos de panel para econometr\u00eda</li> <li>\u2705 Aplicar buenas pr\u00e1cticas de software engineering</li> <li>\u2705 Escribir c\u00f3digo production-ready</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#temas-de-analisis","title":"Temas de An\u00e1lisis","text":"<p>Elige UNO:</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#tema-1-evolucion-institucional-post-autoritaria","title":"Tema 1: Evoluci\u00f3n Institucional Post-Autoritaria","text":"<p>Pregunta: \u00bfC\u00f3mo evoluciona la calidad institucional en transiciones democr\u00e1ticas?</p> <p>Variables clave: - \u00cdndices de democracia (V-Dem, Polity) - Calidad institucional (Transparency International) - Desarrollo econ\u00f3mico (PIB, HDI)</p> <p>Casos: Europa del Este, Am\u00e9rica Latina, Asia Central</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#tema-2-recursos-naturales-y-desarrollo","title":"Tema 2: Recursos Naturales y Desarrollo","text":"<p>Pregunta: \u00bfLa dependencia de recursos naturales afecta el desarrollo?</p> <p>Variables clave: - Producci\u00f3n petr\u00f3leo/gas (Ross dataset) - Rentas recursos naturales (World Bank) - Acceso servicios b\u00e1sicos (agua, saneamiento) - Calidad institucional</p> <p>Casos: Pa\u00edses petroleros, resource curse, seguridad h\u00eddrica</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#arquitectura-del-proyecto","title":"Arquitectura del Proyecto","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#estructura-esperada","title":"Estructura Esperada","text":"<pre><code>tu_apellido_nombre/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 database/          # Conexi\u00f3n PostgreSQL\n\u2502   \u251c\u2500\u2500 etl/               # Extract, Transform, Load\n\u2502   \u251c\u2500\u2500 analysis/          # An\u00e1lisis de datos\n\u2502   \u2514\u2500\u2500 utils/             # Logging, helpers\n\u251c\u2500\u2500 scripts/               # CLI ejecutables\n\u251c\u2500\u2500 sql/                   # Queries complejas\n\u251c\u2500\u2500 tests/                 # Tests (opcional)\n\u2514\u2500\u2500 docs/                  # Documentaci\u00f3n\n</code></pre> <p>Ver detalles: Arquitectura completa</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#tecnologias","title":"Tecnolog\u00edas","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#obligatorias","title":"Obligatorias","text":"<ul> <li>Python 3.11+</li> <li>PostgreSQL 14+</li> <li>pandas - Manipulaci\u00f3n de datos</li> <li>psycopg2 - Conexi\u00f3n PostgreSQL</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#recomendadas","title":"Recomendadas","text":"<ul> <li>SQLAlchemy - ORM</li> <li>pytest - Testing</li> <li>Black - Code formatting</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#arquitectura-de-ingesta","title":"Arquitectura de Ingesta","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#limpieza-de-datos-enfoque-practico","title":"Limpieza de Datos: Enfoque Practico","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#fases-del-proyecto","title":"Fases del Proyecto","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#1-extract-e","title":"1. Extract (E)","text":"<ul> <li>Descargar dataset QoG</li> <li>Filtrar por tema y periodo</li> <li>Validar integridad</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#2-transform-t","title":"2. Transform (T)","text":"<ul> <li>Renombrar columnas</li> <li>Crear variables derivadas</li> <li>Manejar valores faltantes</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#3-load-l","title":"3. Load (L)","text":"<ul> <li>Cargar a PostgreSQL</li> <li>Validar integridad referencial</li> <li>Optimizar con \u00edndices</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#4-analysis","title":"4. Analysis","text":"<ul> <li>Estad\u00edsticas descriptivas</li> <li>Preparar panel balanceado</li> <li>Exportar para econometr\u00eda (.dta, .csv)</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#datos-de-panel","title":"Datos de Panel","text":"<p>Este ejercicio te prepara para an\u00e1lisis econom\u00e9trico.</p> <p>Panel data = Cross-section \u00d7 Time-series</p> <pre><code>| country | year | democracy | gdp_pc |\n|---------|------|-----------|--------|\n| ESP     | 2000 | 0.85      | 24000  |\n| ESP     | 2001 | 0.86      | 24500  |\n</code></pre> <p>Permite: - Fixed Effects (controlar heterogeneidad) - Random Effects - Difference-in-Differences - Modelos din\u00e1micos</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#especificaciones-tecnicas","title":"Especificaciones T\u00e9cnicas","text":"<p>Toda la documentaci\u00f3n t\u00e9cnica detallada est\u00e1 en:</p> <pre><code>ejercicios/02_limpieza_datos/especificaciones/\n\u251c\u2500\u2500 ARQUITECTURA.md           # Estructura de proyecto\n\u251c\u2500\u2500 ESQUEMA_DB.sql            # Schema PostgreSQL completo\n\u251c\u2500\u2500 FUNCIONES_REQUERIDAS.md   # Firmas de funciones\n\u251c\u2500\u2500 VARIABLES_TEMA1.md        # Variables + prompts AI\n\u251c\u2500\u2500 VARIABLES_TEMA2.md        # Variables + prompts AI\n\u2514\u2500\u2500 VALIDACIONES.md           # Checks obligatorios\n</code></pre> <p>Especialmente \u00fatil: VARIABLES_TEMA*.md incluyen prompts para Claude/ChatGPT para investigar variables adicionales.</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#criterios-de-evaluacion","title":"Criterios de Evaluaci\u00f3n","text":"Criterio Peso Qu\u00e9 evaluamos Funcionalidad 40% Pipeline ejecuta sin errores, datos correctos Arquitectura 25% C\u00f3digo modular, separaci\u00f3n responsabilidades Calidad C\u00f3digo 20% PEP 8, type hints, docstrings Documentaci\u00f3n 10% README, metodolog\u00eda, comentarios Innovaci\u00f3n 5% Tests, visualizaciones, an\u00e1lisis extra"},{"location":"ejercicios/02-pipeline-etl-qog/#entregables","title":"Entregables","text":"<p>Ubicaci\u00f3n: <code>entregas/02_limpieza_datos/tu_apellido_nombre/</code></p> <p>M\u00ednimo requerido: - C\u00f3digo fuente modular (src/) - Scripts ejecutables (scripts/) - Queries SQL (sql/) - README completo - METODOLOGIA.md (decisiones de dise\u00f1o) - requirements.txt</p> <p>NO incluir: Datos, logs, venv/, .env</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/02-pipeline-etl-qog/#dataset","title":"Dataset","text":"<ul> <li>QoG Website</li> <li>Codebook PDF</li> <li>Download CSV</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#guias-del-proyecto","title":"Gu\u00edas del Proyecto","text":"<ul> <li>Setup PostgreSQL</li> <li>Instrucciones de Entrega</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#documentacion-tecnica","title":"Documentaci\u00f3n T\u00e9cnica","text":"<ul> <li>PostgreSQL Docs</li> <li>pandas Docs</li> <li>psycopg2 Docs</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#preparacion-para-docker","title":"Preparaci\u00f3n para Docker","text":"<p>Este proyecto est\u00e1 dise\u00f1ado para ser dockerizado en ejercicios futuros.</p> <p>Tu arquitectura modular facilitar\u00e1: - Contenedor PostgreSQL - Contenedor aplicaci\u00f3n Python - docker-compose orchestration</p> <p>Por ahora: PostgreSQL instalaci\u00f3n local.</p>"},{"location":"ejercicios/02-pipeline-etl-qog/#como-empezar","title":"C\u00f3mo Empezar","text":"<ol> <li>Lee toda la documentaci\u00f3n en <code>ejercicios/02_limpieza_datos/</code></li> <li>Instala PostgreSQL (ver POSTGRESQL_SETUP.md)</li> <li>Elige un tema (1 o 2)</li> <li>Investiga variables usando prompts en VARIABLES_TEMA*.md</li> <li>Implementa paso a paso: Extract \u2192 Transform \u2192 Load \u2192 Analysis</li> <li>Testea frecuentemente</li> <li>Documenta mientras codeas</li> </ol>"},{"location":"ejercicios/02-pipeline-etl-qog/#consejos","title":"Consejos","text":"<ul> <li>Empieza con pipeline b\u00e1sico, optimiza despu\u00e9s</li> <li>Logging en todo (tu mejor amigo para debugging)</li> <li>Git commits frecuentes</li> <li>Testea con subset peque\u00f1o primero (no 1M filas de golpe)</li> <li>Lee el codebook QoG - es tu biblia</li> <li>Pregunta temprano si algo no est\u00e1 claro</li> </ul>"},{"location":"ejercicios/02-pipeline-etl-qog/#faq","title":"FAQ","text":"<p>P: \u00bfIndividual o grupal? R: Eliges. Grupos 2-5 personas.</p> <p>P: \u00bfTengo que usar TODAS las variables sugeridas? R: No, son sugerencias. Investiga y elige las relevantes.</p> <p>P: \u00bfPuedo usar Docker? R: No para este ejercicio. PostgreSQL local. Docker ser\u00e1 futuro.</p> <p>P: \u00bfQu\u00e9 hago si no encuentro una variable? R: Usa los prompts AI en VARIABLES_TEMA*.md para buscar alternativas.</p> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/02-postgresql-hr/","title":"Ejercicio 2.1: PostgreSQL con Base de Datos HR","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/02-postgresql-hr/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Aprender\u00e1s a trabajar con PostgreSQL usando la base de datos HR (Human Resources) de Oracle adaptada.</p> <p>Duraci\u00f3n estimada: 4-6 horas Nivel: Intermedio Prerequisitos: SQL b\u00e1sico, haber completado Ejercicio 1.1</p>"},{"location":"ejercicios/02-postgresql-hr/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Instalar y configurar PostgreSQL en tu sistema</li> <li>\u2705 Crear bases de datos y usuarios en PostgreSQL</li> <li>\u2705 Cargar esquemas y datos desde scripts SQL</li> <li>\u2705 Realizar consultas complejas con m\u00faltiples JOINs</li> <li>\u2705 Usar funciones espec\u00edficas de PostgreSQL</li> <li>\u2705 Comparar sintaxis SQL: Oracle vs PostgreSQL</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#base-de-datos-hr","title":"Base de Datos HR","text":"<p>La BD HR es una base de datos de ejemplo oficial de Oracle que modela un sistema de gesti\u00f3n de recursos humanos.</p>"},{"location":"ejercicios/02-postgresql-hr/#entidades-principales","title":"Entidades Principales","text":"<ul> <li>Employees - Informaci\u00f3n de empleados</li> <li>Departments - Departamentos de la empresa</li> <li>Jobs - Puestos de trabajo</li> <li>Locations - Ubicaciones geogr\u00e1ficas</li> <li>Countries - Pa\u00edses</li> <li>Regions - Regiones</li> <li>Job_History - Historial laboral de empleados</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#diagrama-er-simplificado","title":"Diagrama ER Simplificado","text":"<pre><code>erDiagram\n    REGIONS ||--o{ COUNTRIES : contains\n    COUNTRIES ||--o{ LOCATIONS : has\n    LOCATIONS ||--o{ DEPARTMENTS : located_in\n    DEPARTMENTS ||--o{ EMPLOYEES : employs\n    EMPLOYEES ||--o| EMPLOYEES : manages\n    JOBS ||--o{ EMPLOYEES : has_role</code></pre>"},{"location":"ejercicios/02-postgresql-hr/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/02-postgresql-hr/#software-necesario","title":"Software Necesario","text":"<ol> <li>PostgreSQL 14+</li> <li>Descargar para Windows</li> <li>Descargar para Mac</li> <li> <p>Descargar para Linux</p> </li> <li> <p>Cliente SQL (elige uno):</p> </li> <li>pgAdmin (incluido con PostgreSQL)</li> <li>DBeaver (recomendado para principiantes)</li> <li> <p>VS Code con extensi\u00f3n PostgreSQL</p> </li> <li> <p>Python (opcional):</p> </li> <li>psycopg2 para conectar desde Python    <pre><code>pip install psycopg2-binary\n</code></pre></li> </ol>"},{"location":"ejercicios/02-postgresql-hr/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/2.1_postgresql_hr/\n</code></pre>"},{"location":"ejercicios/02-postgresql-hr/#estructura","title":"Estructura","text":"<ul> <li><code>README.md</code> - Instrucciones detalladas</li> <li><code>scripts/</code> - Scripts SQL (a completar por alumnos)</li> <li><code>soluciones/</code> - Soluciones de referencia</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#datos","title":"Datos","text":"<p>Scripts de la BD HR est\u00e1n en: <code>datos/oracle_hr/</code></p>"},{"location":"ejercicios/02-postgresql-hr/#temas-cubiertos","title":"Temas Cubiertos","text":""},{"location":"ejercicios/02-postgresql-hr/#1-instalacion-y-configuracion","title":"1. Instalaci\u00f3n y Configuraci\u00f3n","text":"<ul> <li>Instalar PostgreSQL</li> <li>Crear usuario y base de datos</li> <li>Configurar conexi\u00f3n</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#2-carga-de-datos","title":"2. Carga de Datos","text":"<ul> <li>Ejecutar scripts DDL (estructura)</li> <li>Ejecutar scripts DML (datos)</li> <li>Verificar integridad</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#3-consultas-basicas","title":"3. Consultas B\u00e1sicas","text":"<ul> <li>SELECT con filtros</li> <li>Ordenamiento y l\u00edmites</li> <li>Funciones de agregaci\u00f3n</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#4-consultas-avanzadas","title":"4. Consultas Avanzadas","text":"<ul> <li>JOINs m\u00faltiples</li> <li>Subconsultas</li> <li>CTEs (Common Table Expressions)</li> <li>Window Functions</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#5-analisis-de-negocio","title":"5. An\u00e1lisis de Negocio","text":"<ul> <li>Salarios por departamento</li> <li>Jerarqu\u00edas de empleados</li> <li>Historial laboral</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.1_postgresql_hr/\n</code></pre></p>"},{"location":"ejercicios/02-postgresql-hr/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/02-postgresql-hr/#documentacion-oficial","title":"Documentaci\u00f3n Oficial","text":"<ul> <li>PostgreSQL Documentation</li> <li>PostgreSQL Tutorial</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#tutoriales","title":"Tutoriales","text":"<ul> <li>Gu\u00eda de Instalaci\u00f3n PostgreSQL</li> <li>SQL Avanzado en PostgreSQL</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#comparativas","title":"Comparativas","text":"<ul> <li>Oracle vs PostgreSQL - Diferencias de Sintaxis</li> </ul>"},{"location":"ejercicios/02-postgresql-hr/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 2.2 - PostgreSQL Jardiner\u00eda (m\u00e1s consultas complejas)</li> <li>Ejercicio 2.3 - Migraci\u00f3n de SQLite a PostgreSQL</li> <li>Ejercicio 3.1 - Oracle con BD HR (comparar con PostgreSQL)</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/03-postgresql-jardineria/","title":"Ejercicio 2.2: PostgreSQL con Base de Datos Jardiner\u00eda","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/03-postgresql-jardineria/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Practicar\u00e1s consultas SQL avanzadas con una base de datos de gesti\u00f3n de ventas de jardiner\u00eda.</p> <p>Duraci\u00f3n estimada: 4-6 horas Nivel: Intermedio Prerequisitos: SQL b\u00e1sico, Ejercicio 2.1 (PostgreSQL HR)</p>"},{"location":"ejercicios/03-postgresql-jardineria/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Analizar datos de ventas con SQL</li> <li>\u2705 Generar reportes de negocio</li> <li>\u2705 Usar agregaciones complejas (GROUP BY, HAVING)</li> <li>\u2705 Aplicar Window Functions para rankings y an\u00e1lisis temporal</li> <li>\u2705 Optimizar consultas con \u00edndices</li> <li>\u2705 Crear vistas materializadas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#base-de-datos-jardineria","title":"Base de Datos Jardiner\u00eda","text":"<p>Base de datos de una empresa que vende productos de jardiner\u00eda y plantas.</p>"},{"location":"ejercicios/03-postgresql-jardineria/#entidades-principales","title":"Entidades Principales","text":"<ul> <li>Clientes - Clientes de la empresa</li> <li>Empleados - Organizaci\u00f3n y ventas</li> <li>Oficinas - Ubicaciones de la empresa</li> <li>Pedidos - \u00d3rdenes de compra</li> <li>Detalle_Pedidos - L\u00edneas de cada pedido</li> <li>Productos - Cat\u00e1logo de productos</li> <li>Gamas_Producto - Categor\u00edas</li> <li>Pagos - Transacciones</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    OFICINAS ||--o{ EMPLEADOS : trabaja_en\n    EMPLEADOS ||--o{ CLIENTES : atiende\n    CLIENTES ||--o{ PEDIDOS : realiza\n    CLIENTES ||--o{ PAGOS : hace\n    GAMAS_PRODUCTO ||--o{ PRODUCTOS : tiene\n    PEDIDOS ||--o{ DETALLE_PEDIDOS : contiene\n    PRODUCTOS ||--o{ DETALLE_PEDIDOS : incluido_en</code></pre>"},{"location":"ejercicios/03-postgresql-jardineria/#casos-de-uso-reales","title":"Casos de Uso Reales","text":"<p>Este ejercicio simula an\u00e1lisis que har\u00edas en una empresa real:</p>"},{"location":"ejercicios/03-postgresql-jardineria/#1-analisis-de-ventas","title":"1. An\u00e1lisis de Ventas","text":"<ul> <li>Total de ventas por cliente</li> <li>Productos m\u00e1s vendidos</li> <li>Evoluci\u00f3n de ventas por mes/a\u00f1o</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#2-gestion-de-clientes","title":"2. Gesti\u00f3n de Clientes","text":"<ul> <li>Clientes con mayor volumen de compra</li> <li>Clientes inactivos (sin pedidos recientes)</li> <li>Distribuci\u00f3n geogr\u00e1fica</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#3-rendimiento-de-empleados","title":"3. Rendimiento de Empleados","text":"<ul> <li>Ventas por empleado</li> <li>Clientes asignados por empleado</li> <li>Performance por oficina</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#4-inventario","title":"4. Inventario","text":"<ul> <li>Productos con bajo stock</li> <li>An\u00e1lisis de rotaci\u00f3n</li> <li>Productos sin ventas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":"<ul> <li>PostgreSQL 14+ instalado (del Ejercicio 2.1)</li> <li>Cliente SQL (pgAdmin, DBeaver)</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/2.2_postgresql_jardineria/\n</code></pre>"},{"location":"ejercicios/03-postgresql-jardineria/#datos","title":"Datos","text":"<p>Scripts SQL est\u00e1n en: <code>datos/jardineria/</code></p>"},{"location":"ejercicios/03-postgresql-jardineria/#temas-cubiertos","title":"Temas Cubiertos","text":""},{"location":"ejercicios/03-postgresql-jardineria/#1-consultas-de-analisis","title":"1. Consultas de An\u00e1lisis","text":"<ul> <li>Agregaciones con GROUP BY</li> <li>Filtrado con HAVING</li> <li>M\u00faltiples JOINs</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#2-window-functions","title":"2. Window Functions","text":"<ul> <li>ROW_NUMBER() para rankings</li> <li>LAG/LEAD para comparaciones temporales</li> <li>PARTITION BY para an\u00e1lisis por grupo</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#3-subconsultas","title":"3. Subconsultas","text":"<ul> <li>Subconsultas correlacionadas</li> <li>EXISTS / NOT EXISTS</li> <li>IN / NOT IN</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#4-optimizacion","title":"4. Optimizaci\u00f3n","text":"<ul> <li>EXPLAIN para analizar planes de ejecuci\u00f3n</li> <li>Creaci\u00f3n de \u00edndices</li> <li>An\u00e1lisis de rendimiento</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#5-vistas","title":"5. Vistas","text":"<ul> <li>Vistas simples</li> <li>Vistas materializadas</li> <li>Actualizaci\u00f3n de vistas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.2_postgresql_jardineria/\n</code></pre></p>"},{"location":"ejercicios/03-postgresql-jardineria/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/03-postgresql-jardineria/#documentacion","title":"Documentaci\u00f3n","text":"<ul> <li>PostgreSQL Window Functions</li> <li>Query Optimization</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#tutoriales","title":"Tutoriales","text":"<ul> <li>An\u00e1lisis de Ventas con SQL</li> <li>Window Functions Explicadas</li> </ul>"},{"location":"ejercicios/03-postgresql-jardineria/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 2.3 - Migraci\u00f3n SQLite \u2192 PostgreSQL</li> <li>Ejercicio 3.2 - Oracle Jardiner\u00eda (comparar implementaci\u00f3n)</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/03-procesamiento-distribuido/","title":"Ejercicio 03: Procesamiento Distribuido con Dask","text":"<p>En este m\u00f3dulo aprender\u00e1s a escalar tu capacidad de c\u00f3mputo m\u00e1s all\u00e1 de la memoria RAM de tu m\u00e1quina, utilizando clusters locales.</p> <p></p>"},{"location":"ejercicios/03-procesamiento-distribuido/#objetivos","title":"Objetivos","text":"<ol> <li>Configurar un Cluster Local con Dask.</li> <li>Leer archivos Parquet de forma particionada.</li> <li>Ejecutar agregaciones complejas (GroupBy) en paralelo.</li> </ol>"},{"location":"ejercicios/03-procesamiento-distribuido/#hadoop-la-democratizacion-del-procesamiento","title":"Hadoop: La Democratizacion del Procesamiento","text":""},{"location":"ejercicios/03-procesamiento-distribuido/#instrucciones","title":"Instrucciones","text":"<p>El script principal se encuentra en <code>ejercicios/03_procesamiento_distribuido/esqueleto.py</code>. Tu tarea es completar las funciones marcadas con <code>TODO</code> para construir un pipeline funcional.</p>"},{"location":"ejercicios/03-procesamiento-distribuido/#tarea-de-programacion","title":"Tarea de Programaci\u00f3n","text":"<p>Debes implementar la funci\u00f3n <code>procesamiento_dask()</code> para que: 1. Inicie un cliente local (<code>LocalCluster</code>). 2. Lea el dataset de QoG procesado en el ejercicio anterior. 3. Calcule el promedio anual del \u00cdndice de Democracia. 4. Compare el tiempo de ejecuci\u00f3n vs Pandas tradicional.</p>"},{"location":"ejercicios/04-machine-learning/","title":"M\u00f3dulo 04: Machine Learning","text":"<p>T\u00e9cnicas de aprendizaje autom\u00e1tico aplicadas a Big Data: desde clustering tradicional hasta Computer Vision con Deep Learning.</p>"},{"location":"ejercicios/04-machine-learning/#ejercicio-41-pca-y-clustering","title":"Ejercicio 4.1: PCA y Clustering","text":"<p>Aplicamos t\u00e9cnicas de aprendizaje no supervisado para detectar patrones en datos complejos.</p>"},{"location":"ejercicios/04-machine-learning/#alcance","title":"Alcance","text":"<ul> <li>Reducci\u00f3n de Dimensionalidad: Principal Component Analysis (PCA)</li> <li>Clustering: K-Means y Hierarchical Clustering (HCA)</li> </ul>"},{"location":"ejercicios/04-machine-learning/#tareas","title":"Tareas","text":"<ol> <li>PCA: Reducir variables a 2 componentes principales para visualizaci\u00f3n</li> <li>Clustering: Implementar K-Means y determinar K \u00f3ptimo (m\u00e9todo del codo, Silhouette)</li> <li>Interpretaci\u00f3n: Generar perfil de cada cluster</li> </ol>"},{"location":"ejercicios/04-machine-learning/#recursos","title":"Recursos","text":"<ul> <li>Dashboard PCA + K-Means Iris</li> <li>Dashboard PCA estilo FactoMineR</li> </ul>"},{"location":"ejercicios/04-machine-learning/#ejercicio-42-transfer-learning-clasificacion-de-flores","title":"Ejercicio 4.2: Transfer Learning - Clasificaci\u00f3n de Flores","text":"<p>Pipeline de Computer Vision que clasifica im\u00e1genes de flores usando Transfer Learning con MobileNetV2.</p>"},{"location":"ejercicios/04-machine-learning/#que-es-transfer-learning","title":"\u00bfQu\u00e9 es Transfer Learning?","text":"<p>En lugar de entrenar una red neuronal desde cero (necesitar\u00edamos millones de im\u00e1genes), usamos una red ya entrenada en ImageNet y la adaptamos:</p> <pre><code>ImageNet (14M imgs) \u2192 MobileNetV2 \u2192 Embeddings (1280D) \u2192 Clasificador ML\n</code></pre> <p>Las primeras capas de la CNN ya aprendieron patrones universales (bordes, texturas, formas) que sirven para cualquier imagen.</p>"},{"location":"ejercicios/04-machine-learning/#pipeline","title":"Pipeline","text":"<pre><code>1. DESCARGA          2. EMBEDDINGS         3. CLASIFICACI\u00d3N      4. VISUALIZACI\u00d3N\n   3,670 flores         MobileNetV2           ML tradicional        Dashboard\n   5 clases             1280 features         KNN/SVM/RF            Plotly\n</code></pre>"},{"location":"ejercicios/04-machine-learning/#resultados","title":"Resultados","text":"Modelo Accuracy SVM 89.9% Random Forest 86.5% KNN 86.2%"},{"location":"ejercicios/04-machine-learning/#ejecutar","title":"Ejecutar","text":"<pre><code>cd ejercicios/04_machine_learning/flores_transfer_learning/\npip install -r requirements.txt\npython 01_flores_transfer_learning.py\n</code></pre> <p>Requisitos: TensorFlow (GPU recomendado pero funciona en CPU)</p>"},{"location":"ejercicios/04-machine-learning/#recursos_1","title":"Recursos","text":"<ul> <li>Dashboard Transfer Learning Flores - Galer\u00eda, t-SNE, Comparativa, Confusion Matrix</li> <li>Dashboard Interactivo</li> </ul>"},{"location":"ejercicios/04-machine-learning/#ejercicio-43-series-temporales-arimasarima","title":"Ejercicio 4.3: Series Temporales (ARIMA/SARIMA)","text":"<p>Metodolog\u00eda Box-Jenkins para an\u00e1lisis y pron\u00f3stico de series temporales.</p>"},{"location":"ejercicios/04-machine-learning/#recursos_2","title":"Recursos","text":"<ul> <li>ARIMA/SARIMA - Metodolog\u00eda Box-Jenkins</li> <li>Dashboard ARIMA Interactivo</li> </ul> <p>Curso: Big Data con Python - De Cero a Producci\u00f3n Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Referencias acad\u00e9micas:</p> <ul> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12.</li> <li>Sandler, M., et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR.</li> <li>van der Maaten, L. &amp; Hinton, G. (2008). Visualizing Data using t-SNE. JMLR.</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/","title":"Ejercicio 2.3: Migraci\u00f3n SQLite \u2192 PostgreSQL","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Aprender\u00e1s a migrar bases de datos desde SQLite a PostgreSQL, comprendiendo las diferencias entre ambos motores.</p> <p>Duraci\u00f3n estimada: 3-4 horas Nivel: Intermedio-Avanzado Prerequisitos: Ejercicio 1.1 (SQLite), Ejercicio 2.1 (PostgreSQL)</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Identificar diferencias entre SQLite y PostgreSQL</li> <li>\u2705 Adaptar tipos de datos entre motores</li> <li>\u2705 Migrar esquemas (DDL)</li> <li>\u2705 Migrar datos (DML)</li> <li>\u2705 Escribir scripts de migraci\u00f3n en Python</li> <li>\u2705 Validar integridad despu\u00e9s de la migraci\u00f3n</li> <li>\u2705 Comparar rendimiento entre motores</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#contexto-del-ejercicio","title":"Contexto del Ejercicio","text":"<p>Usar\u00e1s las bases de datos que creaste en el Ejercicio 1.1 (tienda inform\u00e1tica):</p> <ul> <li><code>tienda_modelo_a.db</code> - 26 tablas independientes</li> <li><code>tienda_modelo_b.db</code> - Modelo normalizado</li> <li><code>tienda_modelo_c.db</code> - E-commerce completo</li> </ul> <p>Objetivo: Migrarlas a PostgreSQL y comparar.</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#diferencias-principales","title":"Diferencias Principales","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#tipos-de-datos","title":"Tipos de Datos","text":"SQLite PostgreSQL <code>INTEGER</code> <code>INTEGER</code> o <code>BIGINT</code> <code>REAL</code> <code>NUMERIC(p,s)</code> o <code>DOUBLE PRECISION</code> <code>TEXT</code> <code>VARCHAR(n)</code> o <code>TEXT</code> <code>BLOB</code> <code>BYTEA</code> Sin tipo PostgreSQL es estricto"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#auto-increment","title":"Auto-increment","text":"SQLite PostgreSQL <code>AUTOINCREMENT</code> <code>SERIAL</code> o <code>IDENTITY</code>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#constraints","title":"Constraints","text":"<p>PostgreSQL tiene constraints m\u00e1s estrictos y soporta m\u00e1s tipos.</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#performance","title":"Performance","text":"<p>PostgreSQL es mucho m\u00e1s r\u00e1pido con grandes vol\u00famenes y consultas complejas.</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/2.3_postgresql_tienda/\n</code></pre>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#archivos","title":"Archivos","text":"<ul> <li><code>migracion_desde_sqlite.py</code> - Script de migraci\u00f3n (plantilla)</li> <li><code>comparativa_sqlite_vs_postgres.md</code> - An\u00e1lisis comparativo</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#proceso-de-migracion","title":"Proceso de Migraci\u00f3n","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#1-analisis-del-esquema-origen","title":"1. An\u00e1lisis del Esquema Origen","text":"<pre><code># Conectar a SQLite\nimport sqlite3\nconn = sqlite3.connect('tienda_modelo_b.db')\n\n# Obtener lista de tablas\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n</code></pre>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#2-adaptacion-de-tipos-de-datos","title":"2. Adaptaci\u00f3n de Tipos de Datos","text":"<p>Convertir tipos SQLite \u2192 PostgreSQL: - Detectar tipos din\u00e1micamente - Mapear a tipos PostgreSQL equivalentes - Ajustar precisi\u00f3n de NUMERIC</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#3-recreacion-del-esquema","title":"3. Recreaci\u00f3n del Esquema","text":"<p>Generar DDL para PostgreSQL: - CREATE TABLE con tipos adaptados - PRIMARY KEY - FOREIGN KEY - CONSTRAINTS</p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#4-migracion-de-datos","title":"4. Migraci\u00f3n de Datos","text":"<pre><code># Leer datos de SQLite\ndf = pd.read_sql(\"SELECT * FROM productos\", sqlite_conn)\n\n# Insertar en PostgreSQL\ndf.to_sql('productos', postgres_engine, if_exists='append')\n</code></pre>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#5-validacion","title":"5. Validaci\u00f3n","text":"<ul> <li>Verificar conteos: SQLite vs PostgreSQL</li> <li>Validar integridad referencial</li> <li>Probar consultas complejas</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#tareas-a-realizar","title":"Tareas a Realizar","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#parte-1-script-de-migracion","title":"Parte 1: Script de Migraci\u00f3n","text":"<p>Crear <code>migracion_desde_sqlite.py</code> que:</p> <ol> <li>Lee esquema de SQLite</li> <li>Genera DDL para PostgreSQL</li> <li>Migra datos tabla por tabla</li> <li>Valida la migraci\u00f3n</li> <li>Genera reporte de \u00e9xito/errores</li> </ol>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#parte-2-analisis-comparativo","title":"Parte 2: An\u00e1lisis Comparativo","text":"<p>Crear <code>comparativa_sqlite_vs_postgres.md</code> con:</p> <ol> <li>Diferencias de Esquema</li> <li>Tipos de datos modificados</li> <li>Constraints a\u00f1adidos</li> <li> <p>\u00cdndices creados</p> </li> <li> <p>Pruebas de Rendimiento</p> </li> <li>Misma consulta en ambos motores</li> <li>Tiempos de ejecuci\u00f3n</li> <li> <p>Uso de memoria</p> </li> <li> <p>Conclusiones</p> </li> <li>\u00bfCu\u00e1ndo usar SQLite?</li> <li>\u00bfCu\u00e1ndo usar PostgreSQL?</li> <li>Recomendaciones</li> </ol>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#software","title":"Software","text":"<ul> <li>SQLite (ya instalado)</li> <li>PostgreSQL 14+</li> <li>Python con:   <pre><code>pip install pandas psycopg2-binary sqlite3\n</code></pre></li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#datos-origen","title":"Datos Origen","text":"<p>Tus bases de datos del Ejercicio 1.1: - <code>tienda_modelo_a.db</code> - <code>tienda_modelo_b.db</code> - <code>tienda_modelo_c.db</code></p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.3_postgresql_tienda/\n\u251c\u2500\u2500 migracion_desde_sqlite.py\n\u251c\u2500\u2500 comparativa_sqlite_vs_postgres.md\n\u2514\u2500\u2500 capturas/\n    \u251c\u2500\u2500 sqlite_query.png\n    \u2514\u2500\u2500 postgres_query.png\n</code></pre></p>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/04-migracion-sqlite-postgresql/#herramientas-de-migracion","title":"Herramientas de Migraci\u00f3n","text":"<ul> <li>pgloader - Migraci\u00f3n autom\u00e1tica</li> <li>SQLite to PostgreSQL Converter</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#documentacion","title":"Documentaci\u00f3n","text":"<ul> <li>PostgreSQL Data Types</li> <li>Migraci\u00f3n de Bases de Datos</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#tutoriales","title":"Tutoriales","text":"<ul> <li>Comparativa SQLite vs PostgreSQL</li> <li>Psycopg2 Tutorial</li> </ul>"},{"location":"ejercicios/04-migracion-sqlite-postgresql/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 3.1 - Oracle con BD HR</li> <li>Ejercicio 4.1 - SQL Server con Tienda</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/05-nlp-mining/","title":"Modulo 05: NLP y Text Mining","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/05-nlp-mining/#descripcion-general","title":"Descripcion General","text":"<p>Introduccion al procesamiento de datos no estructurados mediante tecnicas de Natural Language Processing. Desde tokenizacion basica hasta analisis de sentimiento y similitud de documentos.</p> <p>Nivel: Intermedio-Avanzado Tecnologias: Python, NLTK, TF-IDF, Scikit-learn Prerequisitos: Python intermedio, estadistica basica</p> <p></p>"},{"location":"ejercicios/05-nlp-mining/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>Comprender la tokenizacion y normalizacion de texto</li> <li>Eliminar stopwords y aplicar filtros de ruido</li> <li>Calcular similitud entre documentos con Distancia de Jaccard</li> <li>Vectorizar texto con TF-IDF</li> <li>Aplicar clustering sobre documentos vectorizados</li> <li>Realizar analisis de sentimiento basico</li> </ul>"},{"location":"ejercicios/05-nlp-mining/#material-visual-del-curso","title":"Material Visual del Curso","text":""},{"location":"ejercicios/05-nlp-mining/#mas-alla-del-bag-of-words","title":"Mas alla del Bag-of-Words","text":""},{"location":"ejercicios/05-nlp-mining/#el-desafio-de-la-tokenizacion-multilingue","title":"El Desafio de la Tokenizacion Multilingue","text":""},{"location":"ejercicios/05-nlp-mining/#dependency-parsing-la-jerarquia-del-pensamiento","title":"Dependency Parsing: La Jerarquia del Pensamiento","text":""},{"location":"ejercicios/05-nlp-mining/#algoritmos-de-parsing","title":"Algoritmos de Parsing","text":""},{"location":"ejercicios/05-nlp-mining/#conceptos-clave","title":"Conceptos Clave","text":"Concepto Descripcion Tokenizacion Dividir texto en unidades procesables (tokens) Stopwords Eliminacion de ruido (articulos, preposiciones) Similitud Jaccard Medicion matematica de que tan parecidos son dos documentos TF-IDF Vectorizacion que pondera importancia de terminos en un corpus Sentiment Analysis Clasificacion polar (Positivo/Negativo) basada en lexicos Clustering Agrupacion de documentos por similitud semantica"},{"location":"ejercicios/05-nlp-mining/#contenido-del-modulo","title":"Contenido del Modulo","text":"<p>El modulo completo esta en:</p> <pre><code>ejercicios/05_nlp_text_mining/\n\u251c\u2500\u2500 01_conteo_palabras.py          # Tokenizacion y conteo\n\u251c\u2500\u2500 02_limpieza_texto.py           # Stopwords y filtrado\n\u251c\u2500\u2500 03_sentimiento.py              # Analisis de sentimiento\n\u251c\u2500\u2500 04_similitud_jaccard.py        # Similitud entre documentos\n\u251c\u2500\u2500 05_vectorizacion_clustering.py # TF-IDF + K-Means\n\u2514\u2500\u2500 requirements.txt               # nltk, scikit-learn\n</code></pre>"},{"location":"ejercicios/05-nlp-mining/#ejercicios-practicos","title":"Ejercicios Practicos","text":""},{"location":"ejercicios/05-nlp-mining/#01-conteo-de-palabras","title":"01 - Conteo de Palabras","text":"<p>Tokenizacion basica, normalizacion y frecuencia de terminos.</p>"},{"location":"ejercicios/05-nlp-mining/#02-filtro-de-stopwords","title":"02 - Filtro de Stopwords","text":"<p>Eliminacion de ruido para revelar las palabras con peso real.</p>"},{"location":"ejercicios/05-nlp-mining/#03-analisis-de-sentimiento","title":"03 - Analisis de Sentimiento","text":"<p>Clasificacion polar usando lexicos predefinidos.</p>"},{"location":"ejercicios/05-nlp-mining/#04-similitud-de-jaccard","title":"04 - Similitud de Jaccard","text":"<p>Comparar documentos mediante la Distancia de Jaccard.</p> <p>Reto Principal: Implementar un sistema que compare descripciones de politicas publicas de diferentes paises y detecte cuales son semanticamente similares.</p>"},{"location":"ejercicios/05-nlp-mining/#05-vectorizacion-y-clustering","title":"05 - Vectorizacion y Clustering","text":"<p>TF-IDF para representacion numerica + K-Means para agrupacion.</p>"},{"location":"ejercicios/05-nlp-mining/#dashboards","title":"Dashboards","text":"<p>Explora los resultados visualmente:</p> <ul> <li>Conteo de Palabras</li> <li>Filtro de Stopwords</li> <li>Similitud de Jaccard</li> <li>Vectorizacion y Clustering</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Jurafsky, D., &amp; Martin, J. H. (2024). Speech and Language Processing (3<sup>rd</sup> ed.). Prentice Hall.</li> <li>Manning, C. D., Raghavan, P., &amp; Schutze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.</li> <li>Bird, S., Klein, E., &amp; Loper, E. (2009). Natural Language Processing with Python. O'Reilly Media.</li> </ul>"},{"location":"ejercicios/05-oracle-hr/","title":"Ejercicio 3.1: Oracle con Base de Datos HR","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/05-oracle-hr/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Trabajar\u00e1s con Oracle Database usando la base de datos HR en su entorno nativo original.</p> <p>Duraci\u00f3n estimada: 5-7 horas Nivel: Avanzado Prerequisitos: Ejercicio 2.1 (PostgreSQL HR)</p>"},{"location":"ejercicios/05-oracle-hr/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Instalar y configurar Oracle Database Express Edition (XE)</li> <li>\u2705 Usar SQL Developer o SQL*Plus</li> <li>\u2705 Trabajar con sintaxis espec\u00edfica de Oracle</li> <li>\u2705 Crear secuencias y triggers</li> <li>\u2705 Escribir procedimientos almacenados en PL/SQL</li> <li>\u2705 Comparar Oracle con PostgreSQL</li> <li>\u2705 Entender caracter\u00edsticas enterprise de Oracle</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#oracle-database","title":"Oracle Database","text":"<p>Oracle es el motor de bases de datos relacional l\u00edder en el mercado enterprise.</p> <p>Caracter\u00edsticas: - PL/SQL (lenguaje procedural) - Particionamiento avanzado - Replicaci\u00f3n y alta disponibilidad - Seguridad enterprise - Optimizador de consultas muy potente</p>"},{"location":"ejercicios/05-oracle-hr/#diferencias-oracle-vs-postgresql","title":"Diferencias Oracle vs PostgreSQL","text":""},{"location":"ejercicios/05-oracle-hr/#sintaxis","title":"Sintaxis","text":"Aspecto Oracle PostgreSQL Auto-increment SEQUENCE SERIAL String concat <code>\\|\\|</code> o <code>CONCAT()</code> <code>\\|\\|</code> Tipos VARCHAR <code>VARCHAR2</code> <code>VARCHAR</code> LIMIT <code>ROWNUM</code> o <code>FETCH FIRST</code> <code>LIMIT</code> Outer Join <code>(+)</code> (legacy) <code>LEFT/RIGHT JOIN</code>"},{"location":"ejercicios/05-oracle-hr/#funcionalidad","title":"Funcionalidad","text":"<ul> <li>PL/SQL vs PL/pgSQL: Oracle tiene PL/SQL m\u00e1s maduro</li> <li>Packages: Oracle soporta packages (agrupaci\u00f3n de procedimientos)</li> <li>Triggers: Sintaxis diferente</li> <li>Performance: Oracle optimizado para cargas enterprise</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/3.1_oracle_hr/\n</code></pre>"},{"location":"ejercicios/05-oracle-hr/#datos","title":"Datos","text":"<p>Scripts SQL originales de Oracle est\u00e1n en: <code>datos/oracle_hr/</code></p>"},{"location":"ejercicios/05-oracle-hr/#temas-cubiertos","title":"Temas Cubiertos","text":""},{"location":"ejercicios/05-oracle-hr/#1-instalacion-y-configuracion","title":"1. Instalaci\u00f3n y Configuraci\u00f3n","text":"<ul> <li>Instalar Oracle XE 21c</li> <li>Configurar listener</li> <li>Crear usuarios y permisos</li> <li>Conectar con SQL Developer</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#2-sintaxis-oracle","title":"2. Sintaxis Oracle","text":"<ul> <li>Tipos de datos espec\u00edficos</li> <li>Funciones built-in de Oracle</li> <li>ROWNUM y paginaci\u00f3n</li> <li>Hints del optimizador</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#3-plsql-basico","title":"3. PL/SQL B\u00e1sico","text":"<ul> <li>Bloques an\u00f3nimos</li> <li>Variables y tipos</li> <li>Estructuras de control</li> <li>Manejo de excepciones</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#4-objetos-de-base-de-datos","title":"4. Objetos de Base de Datos","text":"<ul> <li>Secuencias</li> <li>Triggers</li> <li>Vistas</li> <li>Sin\u00f3nimos</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#5-procedimientos-y-funciones","title":"5. Procedimientos y Funciones","text":"<ul> <li>Crear procedimientos almacenados</li> <li>Par\u00e1metros IN/OUT/IN OUT</li> <li>Funciones que retornan valores</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/05-oracle-hr/#software-necesario","title":"Software Necesario","text":"<ol> <li>Oracle Database 21c Express Edition (XE) - Gratuito</li> <li>Descargar Oracle XE</li> <li>Requiere cuenta Oracle (gratuita)</li> <li> <p>Limitaciones XE: 12GB RAM, 2 CPUs, 12GB datos de usuario</p> </li> <li> <p>SQL Developer - Cliente gr\u00e1fico oficial de Oracle</p> </li> <li>Descargar SQL Developer</li> <li> <p>Alternativa: DBeaver con driver Oracle</p> </li> <li> <p>Oracle Instant Client (opcional para conexiones remotas)</p> </li> </ol>"},{"location":"ejercicios/05-oracle-hr/#sistema-operativo","title":"Sistema Operativo","text":"<ul> <li>Windows: Instalaci\u00f3n directa</li> <li>Mac/Linux: Usar Docker   <pre><code>docker pull container-registry.oracle.com/database/express:21.3.0-xe\n</code></pre></li> </ul>"},{"location":"ejercicios/05-oracle-hr/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/3.1_oracle_hr/\n</code></pre></p>"},{"location":"ejercicios/05-oracle-hr/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/05-oracle-hr/#documentacion-oficial","title":"Documentaci\u00f3n Oficial","text":"<ul> <li>Oracle Database Documentation</li> <li>PL/SQL Language Reference</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#tutoriales","title":"Tutoriales","text":"<ul> <li>Oracle Live SQL - Pr\u00e1ctica online gratuita</li> <li>PL/SQL Tutorial</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#comparativas","title":"Comparativas","text":"<ul> <li>Oracle vs PostgreSQL</li> <li>Migraci\u00f3n Oracle \u2192 PostgreSQL</li> </ul>"},{"location":"ejercicios/05-oracle-hr/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio:</p> <ol> <li>Ejercicio 3.2 - Oracle Jardiner\u00eda (m\u00e1s pr\u00e1ctica con Oracle)</li> <li>Ejercicio 4.1 - SQL Server (otro motor enterprise)</li> </ol> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/06-analisis-excel-python/","title":"Ejercicio 5.1: An\u00e1lisis de Datos con Excel y Python","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/06-analisis-excel-python/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Aprender\u00e1s a analizar datos de Excel usando Python, comparando el an\u00e1lisis manual vs automatizado.</p> <p>Duraci\u00f3n estimada: 3-4 horas Nivel: B\u00e1sico-Intermedio Prerequisitos: Python b\u00e1sico, pandas</p>"},{"location":"ejercicios/06-analisis-excel-python/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<p>Al completar este ejercicio ser\u00e1s capaz de:</p> <ul> <li>\u2705 Leer archivos Excel con pandas y openpyxl</li> <li>\u2705 Realizar an\u00e1lisis exploratorio de datos (EDA)</li> <li>\u2705 Generar estad\u00edsticas descriptivas</li> <li>\u2705 Crear visualizaciones (gr\u00e1ficos)</li> <li>\u2705 Automatizar an\u00e1lisis que har\u00edas manualmente en Excel</li> <li>\u2705 Exportar resultados a Excel formateado</li> <li>\u2705 Comparar an\u00e1lisis manual vs program\u00e1tico</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#archivo-de-datos","title":"Archivo de Datos","text":"<p>Trabajar\u00e1s con: <code>datos/Ejercicio-de-Excel-resuelto-nivel-medio.xlsx</code></p> <p>Este archivo contiene datos reales que normalmente analizar\u00edas en Excel.</p>"},{"location":"ejercicios/06-analisis-excel-python/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo est\u00e1 en:</p> <pre><code>ejercicios/01_bases_de_datos/5.1_analisis_excel/\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#archivos","title":"Archivos","text":"<ul> <li><code>analisis_exploratorio.py</code> - Plantilla de script</li> <li><code>INSTRUCCIONES.md</code> - Gu\u00eda paso a paso</li> <li><code>informe_analisis.md</code> - Plantilla para tu informe</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#proceso-de-analisis","title":"Proceso de An\u00e1lisis","text":""},{"location":"ejercicios/06-analisis-excel-python/#1-exploracion-inicial","title":"1. Exploraci\u00f3n Inicial","text":"<pre><code>import pandas as pd\n\n# Leer Excel\ndf = pd.read_excel('datos/Ejercicio-de-Excel-resuelto-nivel-medio.xlsx')\n\n# Ver estructura\nprint(df.info())\nprint(df.describe())\nprint(df.head())\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#2-limpieza-de-datos","title":"2. Limpieza de Datos","text":"<ul> <li>Detectar valores nulos</li> <li>Corregir tipos de datos</li> <li>Eliminar duplicados</li> <li>Normalizar texto</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#3-estadisticas-descriptivas","title":"3. Estad\u00edsticas Descriptivas","text":"<ul> <li>Medidas de tendencia central (media, mediana)</li> <li>Dispersi\u00f3n (desviaci\u00f3n est\u00e1ndar, cuartiles)</li> <li>Correlaciones</li> <li>Distribuciones</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#4-visualizaciones","title":"4. Visualizaciones","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Gr\u00e1fico de barras\ndf.groupby('categoria')['ventas'].sum().plot(kind='bar')\n\n# Histograma\ndf['precio'].hist(bins=20)\n\n# Heatmap de correlaci\u00f3n\nsns.heatmap(df.corr(), annot=True)\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#5-exportar-resultados","title":"5. Exportar Resultados","text":"<pre><code># Crear Excel con formato\nwith pd.ExcelWriter('analisis_resultados.xlsx', engine='openpyxl') as writer:\n    df_resumen.to_excel(writer, sheet_name='Resumen')\n    df_detalle.to_excel(writer, sheet_name='Detalle')\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#tareas-a-realizar","title":"Tareas a Realizar","text":""},{"location":"ejercicios/06-analisis-excel-python/#parte-1-analisis-exploratorio","title":"Parte 1: An\u00e1lisis Exploratorio","text":"<p>Crear <code>analisis_exploratorio.py</code> que:</p> <ol> <li>Lee el archivo Excel</li> <li>Realiza EDA completo</li> <li>Genera estad\u00edsticas descriptivas</li> <li>Crea visualizaciones</li> <li>Exporta resultados a Excel formateado</li> </ol>"},{"location":"ejercicios/06-analisis-excel-python/#parte-2-informe-de-analisis","title":"Parte 2: Informe de An\u00e1lisis","text":"<p>Crear <code>informe_analisis.md</code> con:</p> <ol> <li>Resumen Ejecutivo</li> <li>Hallazgos principales</li> <li> <p>Datos clave</p> </li> <li> <p>An\u00e1lisis Detallado</p> </li> <li>Estructura de los datos</li> <li>Calidad de datos</li> <li> <p>Patrones encontrados</p> </li> <li> <p>Visualizaciones</p> </li> <li>Incluir gr\u00e1ficos generados</li> <li> <p>Interpretar resultados</p> </li> <li> <p>Comparaci\u00f3n Manual vs Automatizado</p> </li> <li>\u00bfQu\u00e9 es m\u00e1s r\u00e1pido?</li> <li>\u00bfQu\u00e9 es m\u00e1s preciso?</li> <li> <p>\u00bfCu\u00e1ndo usar cada uno?</p> </li> <li> <p>Conclusiones</p> </li> </ol>"},{"location":"ejercicios/06-analisis-excel-python/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":""},{"location":"ejercicios/06-analisis-excel-python/#librerias-python","title":"Librer\u00edas Python","text":"<pre><code>pip install pandas openpyxl matplotlib seaborn jupyter\n</code></pre>"},{"location":"ejercicios/06-analisis-excel-python/#software-opcional","title":"Software Opcional","text":"<ul> <li>Excel o LibreOffice Calc (para comparar an\u00e1lisis manual)</li> <li>Jupyter Notebook (para an\u00e1lisis interactivo)</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#comparacion-excel-vs-python","title":"Comparaci\u00f3n: Excel vs Python","text":""},{"location":"ejercicios/06-analisis-excel-python/#ventajas-de-excel","title":"Ventajas de Excel","text":"<ul> <li>\u2705 Interfaz visual intuitiva</li> <li>\u2705 R\u00e1pido para an\u00e1lisis ad-hoc peque\u00f1os</li> <li>\u2705 No requiere programaci\u00f3n</li> <li>\u2705 Gr\u00e1ficos interactivos f\u00e1ciles</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#ventajas-de-python","title":"Ventajas de Python","text":"<ul> <li>\u2705 Escalable a millones de filas</li> <li>\u2705 Reproducible (script = documentaci\u00f3n)</li> <li>\u2705 Automatizable</li> <li>\u2705 M\u00e1s an\u00e1lisis estad\u00edsticos avanzados</li> <li>\u2705 Integraci\u00f3n con bases de datos</li> <li>\u2705 Control de versiones (Git)</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#cuando-usar-cada-uno","title":"\u00bfCu\u00e1ndo usar cada uno?","text":"<p>Usa Excel cuando: - Dataset peque\u00f1o (&lt; 100k filas) - An\u00e1lisis r\u00e1pido one-time - Audiencia no t\u00e9cnica</p> <p>Usa Python cuando: - Dataset grande (&gt; 100k filas) - An\u00e1lisis repetitivo - Necesitas automatizaci\u00f3n - An\u00e1lisis complejo</p>"},{"location":"ejercicios/06-analisis-excel-python/#entregas","title":"Entregas","text":"<p>Consulta las instrucciones de entrega para saber qu\u00e9 archivos debes subir.</p> <p>Carpeta de entrega: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/5.1_analisis_excel/\n\u251c\u2500\u2500 analisis_exploratorio.py\n\u251c\u2500\u2500 informe_analisis.md\n\u251c\u2500\u2500 graficos/\n\u2502   \u251c\u2500\u2500 distribucion.png\n\u2502   \u251c\u2500\u2500 correlacion.png\n\u2502   \u2514\u2500\u2500 tendencias.png\n\u2514\u2500\u2500 analisis_resultados.xlsx\n</code></pre></p>"},{"location":"ejercicios/06-analisis-excel-python/#recursos-de-apoyo","title":"Recursos de Apoyo","text":""},{"location":"ejercicios/06-analisis-excel-python/#documentacion","title":"Documentaci\u00f3n","text":"<ul> <li>Pandas Documentation</li> <li>Openpyxl Documentation</li> <li>Matplotlib Gallery</li> <li>Seaborn Examples</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#tutoriales","title":"Tutoriales","text":"<ul> <li>Pandas Tutorial</li> <li>Excel con Python</li> <li>An\u00e1lisis Exploratorio con Pandas</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#videos","title":"Videos","text":"<ul> <li>An\u00e1lisis de Datos con Pandas</li> </ul>"},{"location":"ejercicios/06-analisis-excel-python/#proximos-pasos","title":"Pr\u00f3ximos Pasos","text":"<p>Despu\u00e9s de completar este ejercicio, habr\u00e1s cubierto:</p> <ul> <li>Bases de datos (SQLite, PostgreSQL, Oracle, SQL Server)</li> <li>An\u00e1lisis de datos (Python + Excel)</li> </ul> <p>Siguiente nivel: Big Data con PySpark, Dask, etc.</p> <p>Fecha de publicaci\u00f3n: Por definir \u00daltima actualizaci\u00f3n: 2025-12-17</p>"},{"location":"ejercicios/06-trabajo-final-capstone/","title":"Trabajo Final: Pipeline de Big Data con Infraestructura Docker","text":"<p>Curso: Big Data con Python - Prof. Juan Marcelo Gutierrez Miranda (@TodoEconometria)</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#objetivo","title":"Objetivo","text":"<p>Construir desde cero una infraestructura de procesamiento de datos usando Docker, Apache Spark y PostgreSQL. A partir del dataset Quality of Government (QoG), disenar y ejecutar un pipeline ETL + analisis que responda una pregunta de investigacion formulada por ti.</p> <p>Lo que se evalua: No solo el codigo, sino tu proceso de aprendizaje. Puedes usar herramientas de IA (ChatGPT, Copilot, Claude, etc.) pero debes documentar como las usaste y que aprendiste.</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#dataset","title":"Dataset","text":"<p>Quality of Government Standard Dataset (QoG) - Enero 2024</p> <ul> <li>~15,500 filas (paises x anios) x ~1,990 columnas</li> <li>Variables: democracia, corrupcion, PIB, salud, educacion, estabilidad politica...</li> <li>Documentacion: QoG Data</li> </ul>"},{"location":"ejercicios/06-trabajo-final-capstone/#estructura-4-bloques","title":"Estructura: 4 Bloques","text":""},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-a-infraestructura-docker-30","title":"Bloque A: Infraestructura Docker (30%)","text":"<p>Escribir un <code>docker-compose.yml</code> que levante un mini-cluster:</p> Servicio Requisito minimo PostgreSQL Base de datos para almacenar resultados Spark Master Coordinador del cluster Spark Worker Al menos 1 nodo de procesamiento <p>Pasos:</p> <ol> <li>Investiga que es Docker Compose y como se estructura un archivo YAML</li> <li>Escribe tu <code>docker-compose.yml</code> con los 3 servicios minimos</li> <li>Agrega <code>healthcheck</code> al menos para PostgreSQL</li> <li>Ejecuta <code>docker compose up -d</code> y verifica que todo arranca</li> <li>Abre el Spark UI y toma una captura de pantalla mostrando el worker conectado</li> <li>Escribe <code>02_INFRAESTRUCTURA.md</code> explicando cada seccion de tu YAML con tus palabras</li> </ol> <p>Pistas:</p> <ul> <li>Imagen Spark: <code>apache/spark:3.5.4-python3</code> (o <code>bitnami/spark:3.5</code>)</li> <li>Imagen PostgreSQL: <code>postgres:15-alpine</code></li> <li>El Master de Spark usa el puerto 7077 para comunicacion y 8080 para la UI web</li> </ul> <p>Entregables: <code>docker-compose.yml</code> + <code>02_INFRAESTRUCTURA.md</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-b-pipeline-etl-con-spark-25","title":"Bloque B: Pipeline ETL con Spark (25%)","text":"<p>Escribir un script Python que procese QoG usando Apache Spark.</p> <p>Pasos:</p> <ol> <li>Elige 5 paises que te interesen (no pueden ser los del ejemplo del profesor: KAZ, UZB, TKM, KGZ, TJK)</li> <li>Elige 5 variables numericas del dataset QoG</li> <li>Formula una pregunta de investigacion</li> <li>Escribe <code>pipeline.py</code> que:<ul> <li>Cree una SparkSession</li> <li>Lea el CSV con <code>spark.read.csv()</code></li> <li>Seleccione tus paises y variables</li> <li>Filtre un rango de anios (ej: 2000-2023)</li> <li>Cree al menos 1 variable derivada</li> <li>Guarde el resultado como Parquet</li> </ul> </li> </ol> <p>Importante: Tu seleccion de paises y variables debe ser UNICA. Si dos alumnos entregan los mismos 5 paises, se considerara copia.</p> <p>Entregable: <code>pipeline.py</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-c-analisis-y-visualizacion-25","title":"Bloque C: Analisis y Visualizacion (25%)","text":"<p>Analizar tus datos procesados y responder tu pregunta de investigacion.</p> <p>Elige UNA opcion:</p> Opcion Que hacer Ejemplo Clustering K-Means sobre tus paises \"Que paises se parecen segun democracia + PIB?\" Serie temporal Grafico de evolucion por pais \"Como cambio la corrupcion entre 2000-2023?\" Comparacion Antes/despues de un evento \"Cambio el PIB tras la crisis de 2008?\" <p>Requisitos minimos:</p> <ul> <li>2 graficos (matplotlib, plotly, o seaborn)</li> <li>Cada grafico con titulo, ejes etiquetados, y leyenda</li> <li>Parrafo de interpretacion por cada grafico</li> </ul> <p>Entregable: Graficos e interpretacion en <code>03_RESULTADOS.md</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#bloque-d-reflexion-ia-3-momentos-clave-20","title":"Bloque D: Reflexion IA - \"3 Momentos Clave\" (20%)","text":"<p>Documentar tu proceso de aprendizaje y compartir tus prompts.</p> <p>Para cada bloque (A, B, C), responde:</p> Momento Pregunta Arranque Que fue lo primero que le pediste a la IA (o buscaste)? Error Que fallo y como lo resolviste? Aprendizaje Que aprendiste que NO sabias antes? <p>Ademas, pega el texto exacto del prompt de IA que mas te ayudo en cada bloque.</p> <p>Se evalua:</p> <ul> <li>Que tus prompts sean reales (pegados tal cual, no inventados despues)</li> <li>Que tus respuestas sean especificas</li> <li>Que los errores sean reales (documentarlos no baja nota)</li> <li>Que el proceso sea coherente con tu codigo</li> </ul> <p>Entregable: <code>04_REFLEXION_IA.md</code></p>"},{"location":"ejercicios/06-trabajo-final-capstone/#preguntas-de-comprension-obligatorias","title":"Preguntas de Comprension (obligatorias)","text":"<p>Responde en <code>05_RESPUESTAS.md</code>:</p> <ol> <li>Infraestructura: Si tu worker tiene 2 GB de RAM y el CSV pesa 3 GB, que pasa? Como lo solucionarias?</li> <li>ETL: Por que <code>spark.read.csv()</code> no ejecuta nada hasta que llamas <code>.count()</code> o <code>.show()</code>?</li> <li>Analisis: Interpreta tu grafico principal: que patron ves y por que crees que ocurre?</li> <li>Escalabilidad: Si tuvieras que repetir con un dataset de 50 GB, que cambiarias en tu infraestructura?</li> </ol>"},{"location":"ejercicios/06-trabajo-final-capstone/#formato-de-entrega","title":"Formato de Entrega","text":"<pre><code>entregas/trabajo_final/apellido_nombre/\n    PROMPTS.md                 &lt;- LO MAS IMPORTANTE (tus prompts de IA)\n    01_README.md               &lt;- Tus datos + pregunta de investigacion\n    02_INFRAESTRUCTURA.md      &lt;- Explicacion YAML\n    03_RESULTADOS.md           &lt;- Graficos + interpretacion\n    04_REFLEXION_IA.md         &lt;- 3 Momentos Clave x 3 bloques\n    05_RESPUESTAS.md           &lt;- 4 preguntas de comprension\n    docker-compose.yml         &lt;- Tu YAML funcional\n    pipeline.py                &lt;- ETL + Analisis\n    requirements.txt           &lt;- Dependencias (pip freeze)\n    .gitignore                 &lt;- Excluir datos, venv, __pycache__\n</code></pre> <p>Copia la plantilla desde <code>trabajo_final/plantilla/</code> a tu carpeta de entrega.</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#proceso-sin-pull-request","title":"Proceso (SIN Pull Request)","text":"<ol> <li>Sincroniza tu fork: <code>git fetch upstream &amp;&amp; git merge upstream/main</code></li> <li>Copia la plantilla: <code>cp -r trabajo_final/plantilla/ entregas/trabajo_final/apellido_nombre/</code></li> <li>Completa PROMPTS.md mientras trabajas - Este archivo es lo que se evalua</li> <li>Completa los archivos (01 al 05) + <code>docker-compose.yml</code> + <code>pipeline.py</code></li> <li>Sube a tu fork: <code>git add . &amp;&amp; git commit -m \"Trabajo Final\" &amp;&amp; git push</code></li> <li>Listo! El sistema evalua tu PROMPTS.md automaticamente</li> </ol> <p>No necesitas crear Pull Request</p> <p>El sistema automatico evalua tu archivo PROMPTS.md directamente en tu fork. Solo asegurate de subir tu trabajo con <code>git push</code>.</p>"},{"location":"ejercicios/06-trabajo-final-capstone/#prohibido-incluir","title":"Prohibido incluir","text":"<ul> <li>Archivos de datos (.csv, .parquet, .db)</li> <li>Entornos virtuales (venv/, .venv/)</li> <li>Archivos .env con credenciales reales</li> <li>Carpetas pycache/</li> </ul>"},{"location":"ejercicios/06-trabajo-final-capstone/#evaluacion","title":"Evaluacion","text":"Bloque Peso Que se evalua A. Infraestructura 30% YAML funcional + explicacion con tus palabras B. Pipeline ETL 25% Spark API + paises/variables propios + pregunta C. Analisis 25% Graficos + interpretacion que responda tu pregunta D. Reflexion IA 20% Proceso de aprendizaje real y especifico <p>Penalizaciones:</p> <ul> <li>Copiar los mismos paises/variables que otro alumno: -50%</li> <li>Copiar los paises del ejemplo del profesor (Asia Central): -30%</li> <li>YAML que no funciona sin explicacion de por que: -15%</li> <li>Reflexion IA ausente o generica: -20%</li> </ul>"},{"location":"ejercicios/06-trabajo-final-capstone/#recursos","title":"Recursos","text":"<ul> <li>Spark Documentation: spark.apache.org</li> <li>Docker Compose: docs.docker.com/compose</li> <li>QoG Codebook: qog.pol.gu.se (descargar codebook para ver variables)</li> <li>Guia de Inicio Rapido: <code>trabajo_final/GUIA_INICIO_RAPIDO.md</code></li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.</li> <li>Teorell, J., et al. (2024). The Quality of Government Standard Dataset. University of Gothenburg.</li> <li>Merkel, D. (2014). Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux Journal, 2014(239), 2.</li> </ul>"},{"location":"ejercicios/07-infraestructura-bigdata/","title":"Modulo 07: Infraestructura Big Data","text":"<p>Nivel: Intermedio-Avanzado | Tipo: Teorico-Conceptual con ejemplos practicos</p>"},{"location":"ejercicios/07-infraestructura-bigdata/#objetivo","title":"Objetivo","text":"<p>Entender como se construye la infraestructura que soporta el procesamiento de grandes volumenes de datos. No solo usar las herramientas, sino comprender por que existen, como se conectan entre si y como se orquestan.</p> <p>Este modulo sienta las bases para el Trabajo Final, donde cada alumno construye su propia infraestructura Docker + Spark desde cero.</p> <p></p>"},{"location":"ejercicios/07-infraestructura-bigdata/#estructura-del-modulo","title":"Estructura del Modulo","text":"Seccion Tema Pregunta que responde 7.1 Docker Compose Como empaqueto y conecto servicios? 7.2 Cluster Apache Spark Como proceso datos en paralelo? <p>Orden de estudio</p> <p>Estudiar en orden. Docker es la base sobre la que se monta Spark.</p>"},{"location":"ejercicios/07-infraestructura-bigdata/#71-docker-compose-contenedores-y-orquestacion","title":"7.1 Docker Compose: Contenedores y Orquestacion","text":""},{"location":"ejercicios/07-infraestructura-bigdata/#el-problema-dependency-hell","title":"El Problema: Dependency Hell","text":""},{"location":"ejercicios/07-infraestructura-bigdata/#contenedores-vs-maquinas-virtuales","title":"Contenedores vs Maquinas Virtuales","text":""},{"location":"ejercicios/07-infraestructura-bigdata/#imagenes-vs-contenedores","title":"Imagenes vs Contenedores","text":""},{"location":"ejercicios/07-infraestructura-bigdata/#orquestacion-con-docker-compose","title":"Orquestacion con Docker Compose","text":""},{"location":"ejercicios/07-infraestructura-bigdata/#que-aprenderas","title":"Que aprenderas","text":"<ul> <li>Que es Docker y por que resuelve el \"dependency hell\"</li> <li>Diferencia entre contenedores y maquinas virtuales</li> <li>Imagenes, contenedores y Dockerfile (capas, cache, Docker Hub)</li> <li>Docker Compose: orquestar multiples servicios con un solo archivo YAML</li> <li>Directivas clave: <code>services</code>, <code>image</code>, <code>build</code>, <code>ports</code>, <code>environment</code>, <code>volumes</code>, <code>networks</code>, <code>depends_on</code>, <code>healthcheck</code>, <code>restart</code></li> <li>Redes en Docker: tipos (bridge, host, overlay), DNS interno, comunicacion por nombre de servicio</li> <li>Volumenes: named volumes, bind mounts, tmpfs, persistencia de datos</li> <li>Comandos esenciales: <code>docker compose up/down/ps/logs/exec</code></li> <li>Errores comunes: puerto ocupado, connection refused, datos perdidos, depends_on sin healthcheck</li> <li>Principio de orquestacion: Docker Compose vs Docker Swarm vs Kubernetes</li> <li>Patrones utiles: archivos <code>.env</code>, <code>docker-compose.override.yml</code>, multi-stage builds, profiles</li> </ul>"},{"location":"ejercicios/07-infraestructura-bigdata/#ejemplo-stack-completo-docker-compose","title":"Ejemplo: Stack completo Docker Compose","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: bigdata_postgres\n    environment:\n      POSTGRES_USER: alumno\n      POSTGRES_PASSWORD: bigdata2026\n      POSTGRES_DB: curso_bigdata\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    networks:\n      - red_bigdata\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U alumno\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  pgadmin:\n    image: dpage/pgadmin4\n    ports:\n      - \"8080:80\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n    networks:\n      - red_bigdata\n\nvolumes:\n  pgdata:\n\nnetworks:\n  red_bigdata:\n    driver: bridge\n</code></pre>"},{"location":"ejercicios/07-infraestructura-bigdata/#conceptos-clave-de-docker","title":"Conceptos clave de Docker","text":"Concepto Descripcion Imagen Plantilla inmutable con todo lo necesario para ejecutar una app Contenedor Instancia en ejecucion de una imagen Dockerfile Receta para construir una imagen (capas cacheables) Volume Persistencia de datos mas alla del ciclo de vida del contenedor Network Red interna para comunicacion entre contenedores (DNS automatico) Healthcheck Verificacion de que un servicio esta listo para recibir conexiones"},{"location":"ejercicios/07-infraestructura-bigdata/#72-cluster-apache-spark-arquitectura-y-computacion-distribuida","title":"7.2 Cluster Apache Spark: Arquitectura y Computacion Distribuida","text":""},{"location":"ejercicios/07-infraestructura-bigdata/#que-aprenderas_1","title":"Que aprenderas","text":"<ul> <li>Historia de Spark: de MapReduce a procesamiento en memoria (100x mas rapido)</li> <li>Arquitectura Master-Worker: Driver, SparkSession, Cluster Manager, Workers, Executors, Tasks</li> <li>Spark Standalone Mode: master, workers, asignacion de recursos, Web UI</li> <li>Construir un cluster con Docker: contenedores como nodos, volumenes compartidos, imagen <code>apache/spark</code> (oficial)</li> <li>SparkSession: punto de entrada, modos local vs cluster, configuraciones clave</li> <li>Lazy Evaluation y DAG: transformaciones vs acciones, optimizador Catalyst, predicate pushdown</li> <li>Modos de despliegue: Local, Standalone, YARN, Kubernetes</li> <li>Tuning basico: dimensionar executors, particiones, small file problem, data locality</li> <li>Monitorizacion con Spark UI: Jobs, Stages, Storage, Executors, REST API</li> <li>Spark + PostgreSQL: JDBC connector, leer y escribir datos entre Spark y PostgreSQL</li> <li>De Standalone a produccion: Kubernetes, servicios gestionados (EMR, Dataproc, Databricks)</li> </ul>"},{"location":"ejercicios/07-infraestructura-bigdata/#ejemplo-cluster-spark-con-docker-compose","title":"Ejemplo: Cluster Spark con Docker Compose","text":"<pre><code>services:\n  spark-master:\n    image: apache/spark:3.5.4-python3\n    container_name: spark-master\n    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master\n    environment:\n      - SPARK_NO_DAEMONIZE=true\n    ports:\n      - \"7077:7077\"    # Comunicacion del cluster\n      - \"8080:8080\"    # Web UI del Master\n\n  spark-worker-1:\n    image: apache/spark:3.5.4-python3\n    command: &gt;\n      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker\n      spark://spark-master:7077\n    environment:\n      - SPARK_WORKER_MEMORY=4G\n      - SPARK_WORKER_CORES=2\n      - SPARK_NO_DAEMONIZE=true\n    depends_on:\n      - spark-master\n</code></pre> <p>Imagen oficial</p> <p>Usamos <code>apache/spark</code> (imagen oficial de Apache). La imagen <code>bitnami/spark</code> fue descontinuada en septiembre 2025 y ya no recibe actualizaciones.</p>"},{"location":"ejercicios/07-infraestructura-bigdata/#arquitectura-spark","title":"Arquitectura Spark","text":"<pre><code>Driver Program (tu script Python)\n    \u2502\n    \u25bc\nSparkSession \u2192 SparkContext\n    \u2502\n    \u25bc\nCluster Manager (Standalone / YARN / K8s)\n    \u2502\n    \u251c\u2500\u2500 Worker 1 \u2192 Executor \u2192 Tasks (procesan particiones)\n    \u251c\u2500\u2500 Worker 2 \u2192 Executor \u2192 Tasks\n    \u2514\u2500\u2500 Worker N \u2192 Executor \u2192 Tasks\n</code></pre>"},{"location":"ejercicios/07-infraestructura-bigdata/#transformaciones-vs-acciones","title":"Transformaciones vs Acciones","text":"Transformaciones (LAZY) Acciones (disparan ejecucion) <code>select()</code>, <code>filter()</code>, <code>groupBy()</code> <code>show()</code>, <code>count()</code>, <code>collect()</code> <code>join()</code>, <code>orderBy()</code>, <code>withColumn()</code> <code>write</code>, <code>toPandas()</code>, <code>take(n)</code> No ejecutan nada inmediatamente Disparan toda la cadena de transformaciones"},{"location":"ejercicios/07-infraestructura-bigdata/#relacion-con-otros-modulos","title":"Relacion con otros modulos","text":"Modulo Conexion Modulo 2 (ETL) Pipelines ETL sobre la infraestructura Docker Modulo 3 (Procesamiento Distribuido) Ejercicios practicos de PySpark y Dask Modulo 4 (Panel Data) Pipeline Spark completo sobre dataset QoG Modulo 6 (Streaming) Kafka y Spark Streaming sobre Docker Trabajo Final El alumno construye infraestructura Docker + Spark desde cero"},{"location":"ejercicios/07-infraestructura-bigdata/#requisitos","title":"Requisitos","text":"<ul> <li>Docker Desktop instalado</li> <li>Haber completado al menos el Modulo 1 (Bases de Datos)</li> <li>Conocimientos basicos de terminal/PowerShell</li> </ul>"},{"location":"ejercicios/07-infraestructura-bigdata/#material-del-modulo","title":"Material del Modulo","text":"<p>El contenido completo de cada seccion esta en los README de la carpeta del modulo:</p> <ul> <li>7.1 Docker Compose - Contenedores, imagenes, orquestacion, redes, volumenes</li> <li>7.2 Cluster Spark - Arquitectura distribuida, Docker cluster, tuning, JDBC</li> </ul>"},{"location":"ejercicios/07-infraestructura-bigdata/#glosario","title":"Glosario","text":"Termino Definicion Contenedor Instancia aislada que comparte el kernel del host (ligero, rapido) Imagen Plantilla inmutable para crear contenedores (capas cacheables) Docker Compose Herramienta para orquestar multiples contenedores con un YAML Driver Proceso principal de Spark que coordina el cluster Executor Proceso JVM en un Worker que ejecuta tareas Task Unidad minima de trabajo; procesa una particion DAG Grafo dirigido aciclico del plan de ejecucion Lazy Evaluation Transformaciones no se ejecutan hasta que una accion las dispara Catalyst Optimizador de queries de Spark SQL Shuffle Redistribucion de datos entre nodos (costoso en red) <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas: - Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. - Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65. - Merkel, D. (2014). Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux Journal, 2014(239), 2. - Chambers, B., &amp; Zaharia, M. (2018). Spark: The Definitive Guide. O'Reilly Media. - Dean, J. &amp; Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1), 107-113.</p>"},{"location":"ejercicios/07-series-temporales-arima/","title":"Series Temporales: ARIMA/SARIMA con Metodologia Box-Jenkins","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/07-series-temporales-arima/#descripcion-general","title":"Descripcion General","text":"<p>Aprenderemos a modelar series temporales utilizando la Metodologia Box-Jenkins completa: identificacion, estimacion, diagnostico y pronostico. Trabajaremos con modelos ARIMA y SARIMA para capturar tanto tendencias como estacionalidad.</p> <p>Nivel: Avanzado Dataset: AirPassengers (144 observaciones mensuales, 1949-1960) Tecnologias: Python, statsmodels, matplotlib</p>"},{"location":"ejercicios/07-series-temporales-arima/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>Comprender la Metodologia Box-Jenkins (4 fases)</li> <li>Identificar componentes de una serie: tendencia, estacionalidad, ruido</li> <li>Usar ACF y PACF para determinar ordenes p, d, q</li> <li>Estimar modelos ARIMA y SARIMA</li> <li>Diagnosticar residuos (Ljung-Box, normalidad)</li> <li>Generar pronosticos con intervalos de confianza</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#contenido-del-ejercicio","title":"Contenido del Ejercicio","text":"<p>El ejercicio completo esta en:</p> <pre><code>ejercicios/04_machine_learning/07_series_temporales_arima/\n\u251c\u2500\u2500 README.md                    # Teoria Box-Jenkins completa (829 lineas)\n\u251c\u2500\u2500 serie_temporal_completa.py   # Script 10 partes (1,286 lineas)\n\u251c\u2500\u2500 output/                      # Directorio para graficos generados\n\u2514\u2500\u2500 .gitignore                   # Excluye output/*.png y *.csv\n</code></pre>"},{"location":"ejercicios/07-series-temporales-arima/#el-script-serie_temporal_completapy-cubre","title":"El script <code>serie_temporal_completa.py</code> cubre:","text":"<ol> <li>Carga y visualizacion de la serie original</li> <li>Descomposicion (tendencia + estacionalidad + residuo)</li> <li>Tests de estacionariedad (ADF, KPSS)</li> <li>Diferenciacion regular y estacional</li> <li>ACF/PACF para identificacion de ordenes</li> <li>Estimacion ARIMA con seleccion por AIC</li> <li>Estimacion SARIMA con componente estacional</li> <li>Diagnostico de residuos (Ljung-Box, QQ-plot, ACF residuos)</li> <li>Pronostico con intervalos de confianza</li> <li>Comparacion de modelos y metricas (MAPE, RMSE)</li> </ol>"},{"location":"ejercicios/07-series-temporales-arima/#teoria-metodologia-box-jenkins","title":"Teoria: Metodologia Box-Jenkins","text":""},{"location":"ejercicios/07-series-temporales-arima/#anatomia-de-la-senal-el-proceso-generador-de-datos","title":"Anatomia de la Senal: El Proceso Generador de Datos","text":""},{"location":"ejercicios/07-series-temporales-arima/#estacionariedad-y-ruido-blanco","title":"Estacionariedad y Ruido Blanco","text":""},{"location":"ejercicios/07-series-temporales-arima/#descomposicion-de-la-serie","title":"Descomposicion de la Serie","text":""},{"location":"ejercicios/07-series-temporales-arima/#transformacion-y-diferenciacion","title":"Transformacion y Diferenciacion","text":""},{"location":"ejercicios/07-series-temporales-arima/#el-decodificador-clasico-arima","title":"El Decodificador Clasico: ARIMA","text":""},{"location":"ejercicios/07-series-temporales-arima/#desestacionalizacion-y-sarima","title":"Desestacionalizacion y SARIMA","text":""},{"location":"ejercicios/07-series-temporales-arima/#el-flujo-de-trabajo-box-jenkins","title":"El Flujo de Trabajo Box-Jenkins","text":""},{"location":"ejercicios/07-series-temporales-arima/#fase-1-identificacion","title":"Fase 1: Identificacion","text":"<ul> <li>Visualizar la serie y detectar tendencia/estacionalidad</li> <li>Aplicar diferenciacion para lograr estacionariedad</li> <li>Analizar ACF y PACF para determinar ordenes (p, d, q)</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#fase-2-estimacion","title":"Fase 2: Estimacion","text":"<ul> <li>Ajustar modelo ARIMA(p,d,q) o SARIMA(p,d,q)(P,D,Q)[s]</li> <li>Comparar modelos candidatos usando AIC/BIC</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#fase-3-diagnostico","title":"Fase 3: Diagnostico","text":"<ul> <li>Verificar que los residuos sean ruido blanco</li> <li>Test de Ljung-Box (autocorrelacion)</li> <li>Test de normalidad</li> <li>Grafico QQ</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#fase-4-pronostico","title":"Fase 4: Pronostico","text":"<ul> <li>Generar predicciones con intervalos de confianza</li> <li>Evaluar precision con metricas (MAPE, RMSE)</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#resultado-del-ejercicio","title":"Resultado del Ejercicio","text":"<p>Modelo seleccionado: SARIMA(1,1,0)(0,1,0)[12]</p> <ul> <li>AIC: -445.41</li> <li>MAPE: 7.41%</li> <li>Captura correctamente tendencia y estacionalidad mensual</li> </ul>"},{"location":"ejercicios/07-series-temporales-arima/#dashboard-interactivo","title":"Dashboard Interactivo","text":"<p>Puedes explorar los resultados en el dashboard interactivo:</p> <p>Ver Dashboard ARIMA/SARIMA</p> <p>El dashboard incluye 6 pestanas con graficos Plotly interactivos: serie original, descomposicion, estacionariedad, ACF/PACF, diagnostico y pronostico.</p>"},{"location":"ejercicios/07-series-temporales-arima/#recursos","title":"Recursos","text":"<ul> <li>statsmodels ARIMA Documentation</li> <li>statsmodels SARIMAX</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Box, G. E. P., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control (5<sup>th</sup> ed.). Wiley.</li> <li>Hyndman, R. J., &amp; Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3<sup>rd</sup> ed.). OTexts.</li> <li>Hamilton, J. D. (1994). Time Series Analysis. Princeton University Press.</li> </ul>"},{"location":"ejercicios/08-panel-data/","title":"Modulo 06: Analisis de Datos de Panel","text":"<p>Estado: Disponible</p>"},{"location":"ejercicios/08-panel-data/#descripcion-general","title":"Descripcion General","text":"<p>Aprenderemos econometria de datos de panel: la combinacion de datos de corte transversal (paises, individuos) con series temporales (anios). Trabajaremos con modelos de Efectos Fijos, Efectos Aleatorios y Two-Way Fixed Effects aplicados a problemas reales de ciencias sociales.</p> <p>Nivel: Avanzado Tecnologias: Python, linearmodels, pandas, Altair Prerequisitos: Estadistica basica, regresion lineal</p>"},{"location":"ejercicios/08-panel-data/#objetivos-de-aprendizaje","title":"Objetivos de Aprendizaje","text":"<ul> <li>Comprender la estructura de datos de panel (unidad x tiempo)</li> <li>Distinguir entre Pooled OLS, Efectos Fijos y Efectos Aleatorios</li> <li>Aplicar el Test de Hausman para elegir entre FE y RE</li> <li>Implementar Two-Way Fixed Effects (efectos de unidad + tiempo)</li> <li>Interpretar Odds Ratios en modelos logisticos</li> <li>Calcular efectos marginales en modelos no lineales</li> </ul>"},{"location":"ejercicios/08-panel-data/#contenido-del-modulo","title":"Contenido del Modulo","text":"<p>El modulo completo esta en:</p> <pre><code>ejercicios/06_analisis_datos_de_panel/\n\u251c\u2500\u2500 01_analisis_guns.py              # Panel: leyes de armas y criminalidad\n\u251c\u2500\u2500 02_analisis_fatality.py          # TWFE: impuesto cerveza vs mortalidad\n\u251c\u2500\u2500 03_dashboard_educativo.py        # Dashboard interactivo 4 pestanas\n\u251c\u2500\u2500 conceptos_visuales_panel.py      # Visualizaciones conceptuales\n\u251c\u2500\u2500 GUIA_PANEL_DATA.md               # Guia teorica completa\n\u251c\u2500\u2500 grafico_panel_guns.png           # Resultado del analisis\n\u2514\u2500\u2500 requirements.txt                 # linearmodels, altair, etc.\n</code></pre>"},{"location":"ejercicios/08-panel-data/#ejercicios-practicos","title":"Ejercicios Practicos","text":""},{"location":"ejercicios/08-panel-data/#01-analisis-guns-leyes-de-armas-y-criminalidad","title":"01 - Analisis Guns: Leyes de Armas y Criminalidad","text":"<p>Pregunta: Las leyes de portacion de armas reducen la criminalidad violenta?</p> <ul> <li>Dataset: Guns (Stock &amp; Watson) - 50 estados de EE.UU., 1977-1999</li> <li>Variable dependiente: <code>log(violent)</code> - tasa de criminalidad violenta (logaritmo)</li> <li>Variable clave: <code>law</code> - si el estado tiene ley \"shall-carry\" (portacion obligatoria)</li> <li>Controles: ingreso, poblacion, densidad</li> <li>Modelos: Pooled OLS vs Fixed Effects vs Random Effects</li> <li>Metodologia: Comparacion de los 3 modelos + Test de Hausman</li> </ul> <pre><code>python ejercicios/06_analisis_datos_de_panel/01_analisis_guns.py\n</code></pre>"},{"location":"ejercicios/08-panel-data/#02-analisis-fatality-impuesto-a-la-cerveza-y-mortalidad","title":"02 - Analisis Fatality: Impuesto a la Cerveza y Mortalidad","text":"<p>Pregunta: Subir el impuesto a la cerveza reduce las muertes por accidentes de trafico?</p> <ul> <li>Dataset: Fatalities (AER) - 48 estados, 1982-1988</li> <li>Variable dependiente: <code>fatality_rate</code> - muertes por 10,000 habitantes</li> <li>Variable clave: <code>beertax</code> - impuesto a la cerveza</li> <li>Controles: edad minima para beber (<code>drinkage</code>), desempleo, ingreso</li> <li>Modelos: Entity FE vs Two-Way Fixed Effects (estado + anio)</li> <li>Innovacion: TWFE controla tendencias temporales (coches mas seguros cada anio)</li> </ul> <pre><code>python ejercicios/06_analisis_datos_de_panel/02_analisis_fatality.py\n</code></pre>"},{"location":"ejercicios/08-panel-data/#03-dashboard-educativo-interactivo-panel-altair","title":"03 - Dashboard Educativo Interactivo (Panel + Altair)","text":"<p>Dashboard local con 4 pestanas interactivas para explorar los conceptos visualmente:</p> <ol> <li>Pooled OLS: Slider de heterogeneidad que muestra la Paradoja de Simpson en accion</li> <li>FE vs RE: Explicacion y tabla de decision del Test de Hausman</li> <li>Odds Ratios: Sliders para explorar probabilidad vs odds vs odds ratio en tiempo real</li> <li>Efectos Marginales: Comparacion Lin-Lin, Log-Lin, Log-Log con graficos dinamicos</li> </ol> <pre><code>panel serve ejercicios/06_analisis_datos_de_panel/03_dashboard_educativo.py\n# Abrir http://localhost:5006 en el navegador\n</code></pre>"},{"location":"ejercicios/08-panel-data/#archivos-adicionales","title":"Archivos adicionales","text":"<ul> <li><code>conceptos_visuales_panel.py</code> - Genera graficos estaticos explicando los conceptos de panel</li> <li><code>dashboard_educativo_panel.py</code> - Version alternativa del dashboard educativo</li> <li><code>GUIA_PANEL_DATA.md</code> - Guia teorica completa: Pooled OLS, FE, RE, Hausman, TWFE</li> </ul>"},{"location":"ejercicios/08-panel-data/#teoria-conceptos-clave","title":"Teoria: Conceptos Clave","text":""},{"location":"ejercicios/08-panel-data/#que-son-los-datos-de-panel","title":"Que son los datos de panel?","text":"<p>Datos que combinan corte transversal (N unidades) con serie temporal (T periodos):</p> <p></p> <pre><code>| pais | anio | democracia | pib_pc |\n|------|------|------------|--------|\n| ESP  | 2000 | 0.85       | 24000  |\n| ESP  | 2001 | 0.86       | 24500  |\n| FRA  | 2000 | 0.88       | 28000  |\n| FRA  | 2001 | 0.89       | 28500  |\n</code></pre>"},{"location":"ejercicios/08-panel-data/#ingenieria-de-datos-formato-wide-vs-long","title":"Ingenieria de Datos: Formato Wide vs Long","text":""},{"location":"ejercicios/08-panel-data/#pooled-ols-vs-fixed-effects-vs-random-effects","title":"Pooled OLS vs Fixed Effects vs Random Effects","text":"Modelo Supuesto Cuando usarlo Pooled OLS No hay heterogeneidad individual Rara vez apropiado Fixed Effects Heterogeneidad correlacionada con X Ciencias sociales (regla general) Random Effects Heterogeneidad NO correlacionada con X Encuestas aleatorias"},{"location":"ejercicios/08-panel-data/#el-modelo-ingenuo-pooled-ols","title":"El Modelo Ingenuo: Pooled OLS","text":""},{"location":"ejercicios/08-panel-data/#heterogeneidad-no-observada","title":"Heterogeneidad no Observada","text":""},{"location":"ejercicios/08-panel-data/#test-de-hausman","title":"Test de Hausman","text":"<p>Decide entre FE y RE:</p> <ul> <li>H0: RE es consistente y eficiente (preferir RE)</li> <li>H1: Solo FE es consistente (preferir FE)</li> <li>Si p-valor &lt; 0.05: usar Fixed Effects</li> </ul>"},{"location":"ejercicios/08-panel-data/#dashboards","title":"Dashboards","text":""},{"location":"ejercicios/08-panel-data/#dashboard-educativo-local","title":"Dashboard Educativo (local)","text":"<p>El dashboard principal de este modulo se ejecuta localmente con Panel (HoloViz):</p> <pre><code>panel serve ejercicios/06_analisis_datos_de_panel/03_dashboard_educativo.py\n</code></pre> <p>Incluye 4 pestanas interactivas: Pooled OLS, FE vs RE, Odds Ratios, Efectos Marginales.</p>"},{"location":"ejercicios/08-panel-data/#dashboard-qog-analisis-avanzado-github-pages","title":"Dashboard QoG - Analisis Avanzado (GitHub Pages)","text":"<p>Como complemento, puedes explorar un dashboard con analisis de panel aplicado al dataset QoG (4 lineas de investigacion con Spark + PostgreSQL + ML):</p> <p>Ver Dashboard QoG - Panel Data Aplicado</p>"},{"location":"ejercicios/08-panel-data/#recursos","title":"Recursos","text":""},{"location":"ejercicios/08-panel-data/#documentacion","title":"Documentacion","text":"<ul> <li>linearmodels Documentation</li> <li>Altair Documentation</li> </ul>"},{"location":"ejercicios/08-panel-data/#referencias-teoricas","title":"Referencias Teoricas","text":"<ul> <li>Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (2<sup>nd</sup> ed.). MIT Press.</li> <li>Stock, J. H., &amp; Watson, M. W. (2019). Introduction to Econometrics (4<sup>th</sup> ed.). Pearson.</li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias academicas:</p> <ul> <li>Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (2<sup>nd</sup> ed.). MIT Press.</li> <li>Stock, J. H., &amp; Watson, M. W. (2019). Introduction to Econometrics (4<sup>th</sup> ed.). Pearson.</li> <li>Baltagi, B. H. (2021). Econometric Analysis of Panel Data (6<sup>th</sup> ed.). Springer.</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/","title":"Modulo 08: Streaming de Datos con Apache Kafka","text":""},{"location":"ejercicios/08-streaming-kafka/#introduccion","title":"Introduccion","text":"<p>El streaming de datos es el procesamiento continuo de datos en tiempo real, a diferencia del procesamiento por lotes (batch) donde los datos se acumulan y procesan periodicamente. En aplicaciones modernas como deteccion de fraudes, monitoreo de sensores IoT, o alertas sismicas, el tiempo de respuesta es critico.</p> <p>Apache Kafka es una plataforma distribuida de streaming que permite publicar, almacenar y procesar flujos de datos en tiempo real. Desarrollado originalmente por LinkedIn y donado a Apache, Kafka se ha convertido en el estandar de facto para arquitecturas de datos en streaming.</p>"},{"location":"ejercicios/08-streaming-kafka/#formatos-de-datos-para-ingesta","title":"Formatos de Datos para Ingesta","text":""},{"location":"ejercicios/08-streaming-kafka/#arquitectura-de-kafka","title":"Arquitectura de Kafka","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PRODUCTOR  \u2502\u2500\u2500\u2500\u2500&gt;\u2502            KAFKA CLUSTER            \u2502\u2500\u2500\u2500\u2500&gt;\u2502 CONSUMIDOR  \u2502\n\u2502  (Python)   \u2502     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502     \u2502  (Python)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  \u2502  Topic: sismos              \u2502    \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502  \u2502  \u251c\u2500\u2500 Partition 0            \u2502    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502  \u2502  \u251c\u2500\u2500 Partition 1            \u2502    \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PRODUCTOR  \u2502\u2500\u2500\u2500\u2500&gt;\u2502  \u2502  \u2514\u2500\u2500 Partition 2            \u2502    \u2502\u2500\u2500\u2500\u2500&gt;\u2502 CONSUMIDOR  \u2502\n\u2502   (API)     \u2502     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     \u2502  (Spark)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   KRaft Mode    \u2502\n                    \u2502  (sin ZooKeeper)\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ejercicios/08-streaming-kafka/#conceptos-clave","title":"Conceptos Clave","text":"Concepto Descripcion Broker Servidor Kafka que almacena y sirve mensajes Topic Canal logico donde se publican mensajes (como una tabla) Partition Division de un topic para paralelismo y escalabilidad Producer Aplicacion que envia mensajes a un topic Consumer Aplicacion que lee mensajes de un topic Consumer Group Conjunto de consumidores que comparten la carga de un topic Offset Posicion de un mensaje dentro de una partition"},{"location":"ejercicios/08-streaming-kafka/#kraft-mode-vs-zookeeper","title":"KRaft Mode vs ZooKeeper","text":"<p>Desde Kafka 3.x, el modo KRaft (Kafka Raft) reemplaza a ZooKeeper para la gestion de metadatos, simplificando la arquitectura:</p> <pre><code># Antes (con ZooKeeper) - 2 servicios\nkafka + zookeeper\n\n# Ahora (KRaft mode) - 1 servicio autocontenido\nkafka\n</code></pre>"},{"location":"ejercicios/08-streaming-kafka/#garantias-de-entrega","title":"Garantias de Entrega","text":"<p>Kafka ofrece tres semanticas de entrega:</p> Semantica Descripcion Uso tipico At-most-once Mensaje puede perderse, nunca duplicarse Logs no criticos At-least-once Mensaje puede duplicarse, nunca perderse Procesamiento con idempotencia Exactly-once Mensaje se entrega exactamente una vez Transacciones financieras"},{"location":"ejercicios/08-streaming-kafka/#herramientas-necesarias","title":"Herramientas Necesarias","text":"<ul> <li>Docker y Docker Compose: Para levantar la infraestructura</li> <li>Python 3.9+: Lenguaje principal</li> <li>confluent-kafka: Cliente Python oficial para Kafka</li> <li>requests: Para consumir APIs externas</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#instalacion-de-dependencias","title":"Instalacion de Dependencias","text":"<pre><code>pip install confluent-kafka requests\n</code></pre>"},{"location":"ejercicios/08-streaming-kafka/#reto-1-levantar-kafka-con-docker-compose","title":"Reto 1: Levantar Kafka con Docker Compose","text":"<p>Objetivo: Crear un cluster Kafka funcional en tu maquina local usando Docker.</p> <p>Dificultad: Basica</p>"},{"location":"ejercicios/08-streaming-kafka/#instrucciones","title":"Instrucciones","text":"<ol> <li> <p>Crea un directorio para el proyecto:    <pre><code>mkdir kafka-streaming\ncd kafka-streaming\n</code></pre></p> </li> <li> <p>Crea un archivo <code>docker-compose.yml</code> con un broker Kafka en modo KRaft</p> </li> <li> <p>El broker debe cumplir:</p> </li> <li>Imagen: <code>apache/kafka:latest</code></li> <li>Puerto 9092 expuesto</li> <li>Modo KRaft habilitado (sin ZooKeeper)</li> <li> <p>Variables de entorno configuradas correctamente</p> </li> <li> <p>Levanta y verifica:    <pre><code>docker-compose up -d\ndocker-compose logs -f kafka\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-exito","title":"Criterios de Exito","text":"<ul> <li> El contenedor Kafka esta corriendo sin errores</li> <li> Los logs muestran \"Kafka Server started\"</li> <li> El puerto 9092 responde</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#pistas","title":"Pistas","text":"<ul> <li>Consulta la documentacion de la imagen <code>apache/kafka</code> en Docker Hub</li> <li>Variables clave: <code>KAFKA_NODE_ID</code>, <code>KAFKA_PROCESS_ROLES</code>, <code>KAFKA_LISTENERS</code></li> <li>El modo KRaft requiere <code>KAFKA_CONTROLLER_QUORUM_VOTERS</code></li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#recursos","title":"Recursos","text":"<ul> <li>Apache Kafka Docker Image</li> <li>KRaft Mode Documentation</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#reto-2-tu-primer-productor","title":"Reto 2: Tu Primer Productor","text":"<p>Objetivo: Crear un script Python que envie mensajes a Kafka.</p> <p>Dificultad: Basica</p>"},{"location":"ejercicios/08-streaming-kafka/#instrucciones_1","title":"Instrucciones","text":"<ol> <li> <p>Crea <code>productor_simple.py</code></p> </li> <li> <p>El script debe:</p> </li> <li>Conectarse a <code>localhost:9092</code></li> <li>Enviar 10 mensajes al topic <code>mensajes-test</code></li> <li>Cada mensaje debe ser JSON con: <code>id</code>, <code>texto</code>, <code>timestamp</code></li> <li> <p>Confirmar cada envio exitoso</p> </li> <li> <p>Estructura base:    <pre><code>from confluent_kafka import Producer\nimport json\nfrom datetime import datetime\n\ndef delivery_callback(err, msg):\n    \"\"\"Callback para confirmar envio\"\"\"\n    # Implementa: imprime error o confirmacion\n    pass\n\nconfig = {\n    'bootstrap.servers': 'localhost:9092',\n    'client.id': 'productor-simple'\n}\n\nproducer = Producer(config)\n\n# Implementa: enviar 10 mensajes\n# Usa: producer.produce(topic, key, value, callback=delivery_callback)\n# No olvides: producer.flush()\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-exito_1","title":"Criterios de Exito","text":"<ul> <li> Script ejecuta sin errores</li> <li> 10 mensajes enviados correctamente</li> <li> Cada mensaje tiene estructura JSON valida</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#pistas_1","title":"Pistas","text":"<ul> <li><code>json.dumps()</code> para serializar a string</li> <li>El <code>key</code> puede ser el ID del mensaje</li> <li><code>flush()</code> asegura que todos los mensajes se envien antes de terminar</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#reto-3-tu-primer-consumidor","title":"Reto 3: Tu Primer Consumidor","text":"<p>Objetivo: Crear un script que lea mensajes de Kafka en tiempo real.</p> <p>Dificultad: Basica</p>"},{"location":"ejercicios/08-streaming-kafka/#instrucciones_2","title":"Instrucciones","text":"<ol> <li> <p>Crea <code>consumidor_simple.py</code></p> </li> <li> <p>El script debe:</p> </li> <li>Suscribirse al topic <code>mensajes-test</code></li> <li>Leer mensajes en loop infinito</li> <li>Imprimir cada mensaje con su offset y partition</li> <li> <p>Salir limpiamente con Ctrl+C</p> </li> <li> <p>Estructura base:    <pre><code>from confluent_kafka import Consumer, KafkaError\nimport json\n\nconfig = {\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'mi-grupo-consumidor',\n    'auto.offset.reset': 'earliest'  # Leer desde el inicio\n}\n\nconsumer = Consumer(config)\nconsumer.subscribe(['mensajes-test'])\n\ntry:\n    while True:\n        msg = consumer.poll(timeout=1.0)\n        # Implementa: verificar errores y procesar mensaje\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    consumer.close()\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-exito_2","title":"Criterios de Exito","text":"<ul> <li> Consumidor se conecta correctamente</li> <li> Lee los mensajes del productor</li> <li> Muestra: partition, offset, key, value</li> <li> Sale limpiamente con Ctrl+C</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#pistas_2","title":"Pistas","text":"<ul> <li>Verifica <code>msg is None</code> (timeout sin mensaje)</li> <li>Verifica <code>msg.error()</code> antes de procesar</li> <li>Usa <code>msg.partition()</code>, <code>msg.offset()</code>, <code>msg.key()</code>, <code>msg.value()</code></li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#reto-4-conectar-con-api-real-usgs-earthquakes","title":"Reto 4: Conectar con API Real (USGS Earthquakes)","text":"<p>Objetivo: Crear un productor que consuma datos de sismos en tiempo real.</p> <p>Dificultad: Intermedia</p>"},{"location":"ejercicios/08-streaming-kafka/#instrucciones_3","title":"Instrucciones","text":"<ol> <li> <p>Crea <code>productor_sismos.py</code></p> </li> <li> <p>El productor debe:</p> </li> <li>Consultar la API de USGS cada 30 segundos</li> <li>Parsear el GeoJSON de respuesta</li> <li>Publicar cada sismo nuevo al topic <code>sismos</code></li> <li> <p>Evitar duplicados (mantener set de IDs procesados)</p> </li> <li> <p>API a usar:    <pre><code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson\n</code></pre></p> </li> <li> <p>Estructura del mensaje a publicar:    <pre><code>{\n  \"id\": \"us7000abcd\",\n  \"magnitud\": 4.5,\n  \"lugar\": \"10km SSW of Somewhere\",\n  \"latitud\": -33.45,\n  \"longitud\": -70.66,\n  \"profundidad_km\": 10.0,\n  \"timestamp\": \"2024-01-15T10:30:00.000Z\",\n  \"tsunami\": false\n}\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-exito_3","title":"Criterios de Exito","text":"<ul> <li> Consulta la API cada 30 segundos</li> <li> Publica sismos al topic <code>sismos</code></li> <li> No publica sismos duplicados</li> <li> Maneja errores de red gracefully</li> <li> Funciona continuamente</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#pistas_3","title":"Pistas","text":"<ul> <li>Los sismos estan en <code>response['features']</code></li> <li>El ID esta en <code>feature['id']</code></li> <li>Las propiedades en <code>feature['properties']</code></li> <li>Las coordenadas en <code>feature['geometry']['coordinates']</code> (lon, lat, depth)</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#reto-5-sistema-de-alertas","title":"Reto 5: Sistema de Alertas","text":"<p>Objetivo: Crear un consumidor que detecte sismos significativos.</p> <p>Dificultad: Intermedia</p>"},{"location":"ejercicios/08-streaming-kafka/#instrucciones_4","title":"Instrucciones","text":"<ol> <li> <p>Crea <code>consumidor_alertas.py</code></p> </li> <li> <p>El consumidor debe:</p> </li> <li>Leer del topic <code>sismos</code></li> <li>Filtrar sismos con magnitud &gt;= 4.5</li> <li>Mostrar alerta en consola con formato destacado</li> <li> <p>Guardar alertas en <code>alertas.log</code></p> </li> <li> <p>Formato de alerta:    <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         ALERTA SISMICA                 \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Magnitud: 5.2                          \u2551\n\u2551 Lugar: 10km S of Tokyo, Japan          \u2551\n\u2551 Hora: 2024-01-15 10:30:00              \u2551\n\u2551 Coords: (35.6, 139.7)                  \u2551\n\u2551 Profundidad: 10.5 km                   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-exito_4","title":"Criterios de Exito","text":"<ul> <li> Filtra correctamente por magnitud &gt;= 4.5</li> <li> Alertas visibles en consola</li> <li> Alertas guardadas en archivo log</li> <li> Sistema corre continuamente</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#reto-6-agregaciones-con-spark-structured-streaming","title":"Reto 6: Agregaciones con Spark Structured Streaming","text":"<p>Objetivo: Procesar el stream de sismos con Spark para calcular estadisticas.</p> <p>Dificultad: Avanzada</p>"},{"location":"ejercicios/08-streaming-kafka/#instrucciones_5","title":"Instrucciones","text":"<ol> <li> <p>A\u00f1ade un servicio Spark a tu <code>docker-compose.yml</code></p> </li> <li> <p>Crea <code>spark_streaming_sismos.py</code></p> </li> <li> <p>El job debe:</p> </li> <li>Leer del topic <code>sismos</code> como stream</li> <li>Parsear los mensajes JSON</li> <li>Calcular por ventana de 5 minutos:<ul> <li>Conteo de sismos</li> <li>Magnitud promedio</li> <li>Magnitud maxima</li> </ul> </li> <li> <p>Escribir resultados a consola</p> </li> <li> <p>Estructura base:    <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nspark = SparkSession.builder \\\n    .appName(\"SismosStreaming\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n    .getOrCreate()\n\n# Define schema del mensaje\nschema = StructType([\n    StructField(\"id\", StringType()),\n    StructField(\"magnitud\", DoubleType()),\n    # ... completa el schema\n])\n\n# Lee del topic\ndf = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"sismos\") \\\n    .load()\n\n# Parsea el JSON\nsismos = df.select(\n    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\n# Implementa: agregaciones por ventana de tiempo\n# Usa: window(), avg(), max(), count()\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-exito_5","title":"Criterios de Exito","text":"<ul> <li> Spark lee del topic Kafka</li> <li> Parsea correctamente los mensajes</li> <li> Calcula agregaciones por ventana</li> <li> Muestra resultados en consola</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#reto-final-dashboard-de-visualizacion","title":"Reto FINAL: Dashboard de Visualizacion","text":"<p>Objetivo: Crear una visualizacion web que muestre los sismos en tiempo real.</p> <p>Dificultad: Avanzada</p>"},{"location":"ejercicios/08-streaming-kafka/#criterios-de-evaluacion","title":"Criterios de Evaluacion","text":"Criterio Puntos Mapa interactivo con ubicacion de sismos 20 Actualizacion automatica sin recargar 20 Estadisticas en vivo (total, max, promedio) 15 Filtros por magnitud 15 Diferenciacion visual por magnitud 15 Dise\u00f1o profesional 15 Total 100"},{"location":"ejercicios/08-streaming-kafka/#requisitos-tecnicos","title":"Requisitos Tecnicos","text":"<ul> <li>HTML5 + JavaScript vanilla (sin frameworks)</li> <li>Leaflet.js para mapas</li> <li>Fetch API para obtener datos</li> <li>CSS moderno (flexbox/grid)</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#sugerencias","title":"Sugerencias","text":"<ul> <li>Consulta directamente la API de USGS desde JavaScript</li> <li>Usa <code>setInterval()</code> para actualizacion automatica</li> <li>Implementa colores por magnitud (escala de riesgo)</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#entrega","title":"Entrega","text":"<ul> <li>Archivo HTML autocontenido</li> <li>Captura de pantalla funcionando</li> <li>Documentacion breve de uso</li> </ul> <p>Referencia: Puedes ver un ejemplo de dashboard en Observatorio Sismico, pero el reto es crear tu propia version con tu estilo.</p>"},{"location":"ejercicios/08-streaming-kafka/#recursos-y-referencias","title":"Recursos y Referencias","text":""},{"location":"ejercicios/08-streaming-kafka/#documentacion-oficial","title":"Documentacion Oficial","text":"<ul> <li>Apache Kafka Documentation</li> <li>Confluent Python Client</li> <li>Spark Structured Streaming + Kafka</li> <li>USGS Earthquake API</li> </ul>"},{"location":"ejercicios/08-streaming-kafka/#geojson-de-sismos","title":"GeoJSON de Sismos","text":"<ul> <li>Ultima hora: <code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson</code></li> <li>Ultimo dia: <code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson</code></li> <li>Ultima semana: <code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_week.geojson</code></li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias Academicas:</p> <ul> <li>Kreps, J., Narkhede, N., &amp; Rao, J. (2011). Kafka: A distributed messaging system for log processing. Proceedings of the NetDB Workshop.</li> <li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.</li> <li>Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. ISBN: 978-1449373320.</li> <li>Narkhede, N., Shapira, G., &amp; Palino, T. (2017). Kafka: The Definitive Guide. O'Reilly Media. ISBN: 978-1491936160.</li> <li>USGS (2024). Earthquake Hazards Program - Real-time Feeds. United States Geological Survey.</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/","title":"Modulo 09: Cloud Engineering con LocalStack","text":""},{"location":"ejercicios/09-cloud-localstack/#introduccion","title":"Introduccion","text":"<p>Cloud Computing ha revolucionado la forma en que desplegamos y escalamos aplicaciones. Sin embargo, aprender AWS, Azure o GCP tiene una barrera: el costo. Un error en produccion puede generar facturas inesperadas.</p> <p>LocalStack resuelve este problema simulando los servicios de AWS localmente. Puedes aprender S3, Lambda, DynamoDB, Kinesis y mas sin gastar un centavo. Si funciona en LocalStack, funciona en AWS real.</p> <p>Terraform es la herramienta estandar para \"Infraestructura como Codigo\" (IaC). En lugar de hacer clicks en consolas web, defines tu infraestructura en archivos de texto que puedes versionar, revisar y reutilizar.</p>"},{"location":"ejercicios/09-cloud-localstack/#conceptos-fundamentales","title":"Conceptos Fundamentales","text":""},{"location":"ejercicios/09-cloud-localstack/#cloud-computing-los-3-modelos","title":"Cloud Computing: Los 3 Modelos","text":"Modelo Descripcion Ejemplo IaaS Infraestructura como Servicio EC2, VMs PaaS Plataforma como Servicio Elastic Beanstalk, Heroku SaaS Software como Servicio Gmail, Salesforce"},{"location":"ejercicios/09-cloud-localstack/#servicios-aws-clave-para-big-data","title":"Servicios AWS Clave para Big Data","text":"Servicio Proposito Equivalente Open Source S3 Almacenamiento de objetos (Data Lake) MinIO Lambda Funciones serverless OpenFaaS Kinesis Streaming de datos Kafka DynamoDB Base de datos NoSQL MongoDB EventBridge Orquestacion de eventos Cron + Kafka IAM Control de acceso -"},{"location":"ejercicios/09-cloud-localstack/#arquitectura-data-lake-medallion","title":"Arquitectura Data Lake (Medallion)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     DATA LAKE (S3)                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    BRONZE     \u2502      SILVER       \u2502          GOLD           \u2502\n\u2502   (Raw Data)  \u2502   (Cleaned Data)  \u2502   (Business-Ready)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 - Datos crudos\u2502 - Datos limpios   \u2502 - Agregaciones          \u2502\n\u2502 - JSON/CSV    \u2502 - Parquet         \u2502 - KPIs                  \u2502\n\u2502 - Sin esquema \u2502 - Con esquema     \u2502 - Dashboards            \u2502\n\u2502 - Append-only \u2502 - Deduplicados    \u2502 - ML-ready              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ejercicios/09-cloud-localstack/#herramientas-necesarias","title":"Herramientas Necesarias","text":"<ul> <li>Docker y Docker Compose: Para LocalStack</li> <li>Python 3.9+: Lenguaje principal</li> <li>Terraform: Infraestructura como codigo</li> <li>awscli-local: CLI de AWS para LocalStack</li> <li>boto3: SDK de AWS para Python</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#instalacion-de-dependencias","title":"Instalacion de Dependencias","text":"<pre><code># Python\npip install boto3 requests\n\n# Terraform (Windows con Chocolatey)\nchoco install terraform\n\n# Terraform (Linux/Mac)\nbrew install terraform\n\n# AWS CLI Local\npip install awscli-local\n</code></pre>"},{"location":"ejercicios/09-cloud-localstack/#reto-1-levantar-localstack","title":"Reto 1: Levantar LocalStack","text":"<p>Objetivo: Crear un entorno LocalStack funcional con Docker.</p> <p>Dificultad: Basica</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones","title":"Instrucciones","text":"<ol> <li> <p>Crea un directorio para el proyecto:    <pre><code>mkdir cloud-localstack\ncd cloud-localstack\n</code></pre></p> </li> <li> <p>Crea un archivo <code>docker-compose.yml</code> con LocalStack</p> </li> <li> <p>El servicio debe:</p> </li> <li>Usar imagen <code>localstack/localstack:latest</code></li> <li>Exponer puerto 4566 (gateway unificado)</li> <li>Habilitar servicios: s3, lambda, dynamodb, events</li> <li> <p>Montar volumen para persistencia</p> </li> <li> <p>Levanta y verifica:    <pre><code>docker-compose up -d\ncurl http://localhost:4566/_localstack/health\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito","title":"Criterios de Exito","text":"<ul> <li> Contenedor LocalStack corriendo</li> <li> Endpoint de health responde con servicios activos</li> <li> Puedes listar buckets S3 (vacio): <code>awslocal s3 ls</code></li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#pistas","title":"Pistas","text":"<ul> <li>Variable <code>SERVICES</code> define que servicios activar</li> <li><code>LOCALSTACK_HOST=localhost</code> para conexiones locales</li> <li>El puerto 4566 es el gateway para todos los servicios</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#recursos","title":"Recursos","text":"<ul> <li>LocalStack Documentation</li> <li>Docker Hub - LocalStack</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-2-crear-bucket-s3-con-terraform","title":"Reto 2: Crear Bucket S3 con Terraform","text":"<p>Objetivo: Definir un bucket S3 usando Terraform y desplegarlo en LocalStack.</p> <p>Dificultad: Basica</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones_1","title":"Instrucciones","text":"<ol> <li> <p>Crea un archivo <code>main.tf</code></p> </li> <li> <p>Configura el provider de AWS para LocalStack:    <pre><code>provider \"aws\" {\n  region                      = \"us-east-1\"\n  access_key                  = \"test\"\n  secret_key                  = \"test\"\n  skip_credentials_validation = true\n  skip_metadata_api_check     = true\n  skip_requesting_account_id  = true\n\n  endpoints {\n    s3     = \"http://localhost:4566\"\n    lambda = \"http://localhost:4566\"\n    # ... a\u00f1ade mas endpoints\n  }\n}\n</code></pre></p> </li> <li> <p>Define un bucket S3:    <pre><code>resource \"aws_s3_bucket\" \"data_lake\" {\n  bucket = \"mi-data-lake\"\n  # ... completa la configuracion\n}\n</code></pre></p> </li> <li> <p>Ejecuta Terraform:    <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito_1","title":"Criterios de Exito","text":"<ul> <li> <code>terraform init</code> descarga el provider</li> <li> <code>terraform plan</code> muestra el bucket a crear</li> <li> <code>terraform apply</code> crea el bucket</li> <li> <code>awslocal s3 ls</code> muestra \"mi-data-lake\"</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#pistas_1","title":"Pistas","text":"<ul> <li>Los endpoints deben apuntar a <code>http://localhost:4566</code></li> <li><code>access_key</code> y <code>secret_key</code> pueden ser cualquier valor en LocalStack</li> <li>Usa <code>terraform destroy</code> para limpiar recursos</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-3-tu-primera-lambda-hello-world","title":"Reto 3: Tu Primera Lambda (Hello World)","text":"<p>Objetivo: Crear una funcion Lambda y desplegarla con Terraform.</p> <p>Dificultad: Intermedia</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones_2","title":"Instrucciones","text":"<ol> <li> <p>Crea una carpeta <code>lambdas/</code> con un archivo <code>hello.py</code>:    <pre><code>def handler(event, context):\n    \"\"\"Tu primera Lambda\"\"\"\n    # Implementa: retorna un saludo con datos del evento\n    return {\n        \"statusCode\": 200,\n        \"body\": \"...\"\n    }\n</code></pre></p> </li> <li> <p>Empaqueta la Lambda en un ZIP:    <pre><code>cd lambdas &amp;&amp; zip hello.zip hello.py &amp;&amp; cd ..\n</code></pre></p> </li> <li> <p>A\u00f1ade a <code>main.tf</code>:    <pre><code>resource \"aws_lambda_function\" \"hello\" {\n  filename         = \"lambdas/hello.zip\"\n  function_name    = \"hello-lambda\"\n  role             = \"arn:aws:iam::000000000000:role/lambda-role\"\n  handler          = \"hello.handler\"\n  runtime          = \"python3.9\"\n  # ... completa\n}\n</code></pre></p> </li> <li> <p>Aplica y prueba:    <pre><code>terraform apply\nawslocal lambda invoke --function-name hello-lambda output.json\ncat output.json\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito_2","title":"Criterios de Exito","text":"<ul> <li> Lambda creada en LocalStack</li> <li> Invocacion retorna statusCode 200</li> <li> El body contiene tu mensaje</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#pistas_2","title":"Pistas","text":"<ul> <li>En LocalStack, el role puede ser un ARN ficticio</li> <li>El handler es <code>nombre_archivo.nombre_funcion</code></li> <li>Usa <code>source_code_hash</code> para detectar cambios en el ZIP</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-4-lambda-que-consume-api-externa","title":"Reto 4: Lambda que Consume API Externa","text":"<p>Objetivo: Crear una Lambda que capture datos de la ISS en tiempo real.</p> <p>Dificultad: Intermedia</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones_3","title":"Instrucciones","text":"<ol> <li> <p>Crea <code>lambdas/capturar_iss.py</code>:    <pre><code>import json\nimport urllib.request\n\nISS_API = \"https://api.wheretheiss.at/v1/satellites/25544\"\n\ndef handler(event, context):\n    \"\"\"Captura posicion actual de la ISS\"\"\"\n    # Implementa:\n    # 1. Hacer request a la API\n    # 2. Parsear el JSON\n    # 3. Extraer: latitude, longitude, altitude, velocity\n    # 4. Retornar los datos formateados\n    pass\n</code></pre></p> </li> <li> <p>Nota: Lambda no tiene <code>requests</code>, usa <code>urllib.request</code>:    <pre><code>with urllib.request.urlopen(ISS_API) as response:\n    data = json.loads(response.read().decode())\n</code></pre></p> </li> <li> <p>Despliega y prueba la Lambda</p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito_3","title":"Criterios de Exito","text":"<ul> <li> Lambda se despliega correctamente</li> <li> Retorna posicion actual de la ISS</li> <li> Datos incluyen lat, lon, alt, velocidad</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#pistas_3","title":"Pistas","text":"<ul> <li>Lambda tiene limitaciones de librerias - usa stdlib</li> <li>El timeout default es 3 segundos, puede necesitar mas</li> <li>Maneja excepciones de red</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-5-guardar-datos-en-s3","title":"Reto 5: Guardar Datos en S3","text":"<p>Objetivo: Modificar la Lambda para guardar los datos de la ISS en S3.</p> <p>Dificultad: Intermedia</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones_4","title":"Instrucciones","text":"<ol> <li>Modifica <code>capturar_iss.py</code> para:</li> <li>Conectarse a S3 usando boto3</li> <li>Guardar cada captura como JSON en el bucket</li> <li> <p>Usar ruta: <code>raw/iss/{fecha}/{timestamp}.json</code></p> </li> <li> <p>Estructura del archivo guardado:    <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"latitude\": 45.123,\n  \"longitude\": -93.456,\n  \"altitude\": 420.5,\n  \"velocity\": 27580.3\n}\n</code></pre></p> </li> <li> <p>Configura boto3 para LocalStack:    <pre><code>import boto3\n\ns3 = boto3.client('s3',\n    endpoint_url='http://host.docker.internal:4566',\n    aws_access_key_id='test',\n    aws_secret_access_key='test'\n)\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito_4","title":"Criterios de Exito","text":"<ul> <li> Lambda guarda archivo en S3</li> <li> Ruta incluye fecha y timestamp</li> <li> Archivo contiene datos validos de la ISS</li> <li> Puedes listar archivos: <code>awslocal s3 ls s3://bucket/raw/iss/ --recursive</code></li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#pistas_4","title":"Pistas","text":"<ul> <li>En Docker, usa <code>host.docker.internal</code> para acceder a LocalStack</li> <li><code>s3.put_object(Bucket, Key, Body)</code> para guardar</li> <li><code>Body</code> debe ser string o bytes</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-6-scheduling-con-eventbridge","title":"Reto 6: Scheduling con EventBridge","text":"<p>Objetivo: Programar la Lambda para que se ejecute automaticamente cada minuto.</p> <p>Dificultad: Avanzada</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones_5","title":"Instrucciones","text":"<ol> <li> <p>A\u00f1ade a <code>main.tf</code> una regla de EventBridge:    <pre><code>resource \"aws_cloudwatch_event_rule\" \"cada_minuto\" {\n  name                = \"captura-iss-schedule\"\n  description         = \"Ejecuta Lambda cada minuto\"\n  schedule_expression = \"rate(1 minute)\"\n}\n</code></pre></p> </li> <li> <p>Conecta la regla con la Lambda:    <pre><code>resource \"aws_cloudwatch_event_target\" \"lambda_target\" {\n  rule      = aws_cloudwatch_event_rule.cada_minuto.name\n  target_id = \"captura-iss\"\n  arn       = aws_lambda_function.capturar_iss.arn\n}\n</code></pre></p> </li> <li> <p>A\u00f1ade permisos para que EventBridge invoque Lambda:    <pre><code>resource \"aws_lambda_permission\" \"allow_eventbridge\" {\n  # ... completa\n}\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito_5","title":"Criterios de Exito","text":"<ul> <li> Regla de EventBridge creada</li> <li> Lambda se ejecuta automaticamente</li> <li> Archivos aparecen en S3 cada minuto</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#pistas_5","title":"Pistas","text":"<ul> <li>Usa <code>schedule_expression = \"rate(1 minute)\"</code> o cron</li> <li>El permiso requiere <code>statement_id</code>, <code>action</code>, <code>function_name</code>, <code>principal</code></li> <li>Verifica logs: <code>awslocal logs tail /aws/lambda/capturar-iss</code></li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-7-dynamodb-para-metadatos","title":"Reto 7: DynamoDB para Metadatos","text":"<p>Objetivo: Crear una tabla DynamoDB para almacenar metadatos de las capturas.</p> <p>Dificultad: Avanzada</p>"},{"location":"ejercicios/09-cloud-localstack/#instrucciones_6","title":"Instrucciones","text":"<ol> <li> <p>A\u00f1ade tabla DynamoDB en Terraform:    <pre><code>resource \"aws_dynamodb_table\" \"capturas_metadata\" {\n  name           = \"capturas-iss\"\n  billing_mode   = \"PAY_PER_REQUEST\"\n  hash_key       = \"capture_id\"\n\n  attribute {\n    name = \"capture_id\"\n    type = \"S\"\n  }\n  # ... a\u00f1ade mas atributos si necesitas\n}\n</code></pre></p> </li> <li> <p>Modifica la Lambda para guardar metadatos:    <pre><code>dynamodb = boto3.resource('dynamodb',\n    endpoint_url='http://host.docker.internal:4566',\n    # ...\n)\n\ntable = dynamodb.Table('capturas-iss')\ntable.put_item(Item={\n    'capture_id': timestamp,\n    's3_path': f's3://bucket/raw/iss/{fecha}/{timestamp}.json',\n    'latitude': data['latitude'],\n    'longitude': data['longitude'],\n    # ...\n})\n</code></pre></p> </li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-exito_6","title":"Criterios de Exito","text":"<ul> <li> Tabla DynamoDB creada</li> <li> Lambda guarda metadatos en cada ejecucion</li> <li> Puedes consultar items: <code>awslocal dynamodb scan --table-name capturas-iss</code></li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#reto-final-dashboard-de-tracker","title":"Reto FINAL: Dashboard de Tracker","text":"<p>Objetivo: Crear una visualizacion web que muestre la posicion de la ISS en tiempo real.</p> <p>Dificultad: Avanzada</p>"},{"location":"ejercicios/09-cloud-localstack/#criterios-de-evaluacion","title":"Criterios de Evaluacion","text":"Criterio Puntos Mapa con posicion actual de la ISS 20 Icono personalizado de la ISS 10 Actualizacion automatica (cada 5-10 seg) 20 Trayectoria/rastro de movimiento 15 Datos en vivo (lat, lon, alt, vel) 15 Predictor de pases sobre una ciudad 10 Dise\u00f1o profesional 10 Total 100"},{"location":"ejercicios/09-cloud-localstack/#requisitos-tecnicos","title":"Requisitos Tecnicos","text":"<ul> <li>HTML5 + JavaScript vanilla</li> <li>Leaflet.js para mapas</li> <li>Fetch API para datos en vivo</li> <li>Sin dependencias de Jupyter/Colab</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#sugerencias","title":"Sugerencias","text":"<ul> <li>Usa la API directamente: <code>https://api.wheretheiss.at/v1/satellites/25544</code></li> <li>Nominatim para geocodificar ciudades</li> <li>SVG para el icono de la ISS</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#entrega","title":"Entrega","text":"<ul> <li>Archivo HTML autocontenido</li> <li>Captura de pantalla funcionando</li> <li>Breve documentacion</li> </ul> <p>Referencia: Puedes ver un ejemplo de tracker profesional en ISS Tracker, pero el reto es crear tu propia version.</p>"},{"location":"ejercicios/09-cloud-localstack/#de-localstack-a-aws-real","title":"De LocalStack a AWS Real","text":"<p>Cuando estes listo para produccion:</p>"},{"location":"ejercicios/09-cloud-localstack/#cambios-necesarios","title":"Cambios Necesarios","text":"<ol> <li> <p>Credenciales reales: <pre><code>aws configure\n# Ingresa Access Key ID y Secret Access Key reales\n</code></pre></p> </li> <li> <p>Eliminar endpoints locales: <pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n  # Sin endpoints personalizados\n}\n</code></pre></p> </li> <li> <p>IAM roles reales:</p> </li> <li>Crea roles con permisos minimos necesarios</li> <li> <p>Usa politicas gestionadas cuando sea posible</p> </li> <li> <p>Consideraciones de costo:</p> </li> <li>S3: $0.023/GB/mes</li> <li>Lambda: 1M invocaciones gratis, luego $0.20/1M</li> <li>DynamoDB: Pay-per-request o provisioned</li> </ol>"},{"location":"ejercicios/09-cloud-localstack/#recursos-y-referencias","title":"Recursos y Referencias","text":""},{"location":"ejercicios/09-cloud-localstack/#documentacion-oficial","title":"Documentacion Oficial","text":"<ul> <li>LocalStack Documentation</li> <li>Terraform AWS Provider</li> <li>AWS Lambda Developer Guide</li> <li>boto3 Documentation</li> </ul>"},{"location":"ejercicios/09-cloud-localstack/#apis-de-datos","title":"APIs de Datos","text":"<ul> <li>ISS Position: <code>https://api.wheretheiss.at/v1/satellites/25544</code></li> <li>ISS Astronauts: <code>http://api.open-notify.org/astros.json</code></li> <li>Geocoding: <code>https://nominatim.openstreetmap.org/search</code></li> </ul> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias Academicas:</p> <ul> <li>Wittig, M., &amp; Wittig, A. (2019). Amazon Web Services in Action (2<sup>nd</sup> ed.). Manning Publications. ISBN: 978-1617295119.</li> <li>Brikman, Y. (2019). Terraform: Up &amp; Running (2<sup>nd</sup> ed.). O'Reilly Media. ISBN: 978-1492046905.</li> <li>Chauhan, A. (2020). Infrastructure as Code (IaC) for Beginners. Medium - Towards Data Science.</li> <li>Jonas, E., et al. (2019). Cloud programming simplified: A Berkeley view on serverless computing. arXiv preprint arXiv:1902.03383.</li> <li>LocalStack Team (2024). LocalStack Documentation. https://docs.localstack.cloud/</li> </ul>"},{"location":"entregas/guia-entregas/","title":"Como Entregar tus Trabajos","text":"<p>Esta guia te explica paso a paso como entregar. No necesitas saber Git avanzado.</p>"},{"location":"entregas/guia-entregas/#resumen-en-30-segundos","title":"Resumen en 30 segundos","text":"<pre><code>1. Haces fork del repo (solo una vez)\n2. Trabajas en TU fork\n3. Subes tus cambios a TU fork\n4. El profesor revisa TU fork automaticamente (sin PR)\n</code></pre> <p>NO necesitas crear Pull Request. El sistema automatico evalua tu archivo PROMPTS.md directamente en tu fork.</p>"},{"location":"entregas/guia-entregas/#diagrama-del-flujo","title":"Diagrama del Flujo","text":"<pre><code>flowchart TB\n    subgraph Profesor[\"REPOSITORIO DEL PROFESOR\"]\n        P1[\"github.com/TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;Aqui estan los ejercicios&lt;br/&gt;NO modificas nada aqui\"]\n    end\n\n    subgraph Alumno[\"TU FORK (tu copia)\"]\n        A1[\"github.com/TU_USUARIO/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;Aqui trabajas y subes tu codigo\"]\n    end\n\n    subgraph Local[\"TU PC\"]\n        L1[\"Carpeta ejercicios-bigdata/&lt;br/&gt;&lt;br/&gt;Aqui programas\"]\n    end\n\n    subgraph Sistema[\"SISTEMA AUTOMATICO\"]\n        S1[\"Script del profesor&lt;br/&gt;&lt;br/&gt;Revisa todos los forks&lt;br/&gt;Genera notas automaticas\"]\n    end\n\n    Profesor --&gt;|\"1. Fork (copiar)\"| Alumno\n    Alumno --&gt;|\"2. Clone (descargar)\"| Local\n    Local --&gt;|\"3. Push (subir)\"| Alumno\n    Alumno --&gt;|\"4. Revision automatica\"| Sistema\n\n    style Profesor fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Alumno fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style Local fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style Sistema fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px</code></pre>"},{"location":"entregas/guia-entregas/#paso-1-crear-tu-fork-solo-una-vez","title":"Paso 1: Crear tu Fork (solo una vez)","text":"<p>Un \"fork\" es tu copia personal del repositorio.</p> <ol> <li>Ve a: github.com/TodoEconometria/ejercicios-bigdata</li> <li>Click en el boton \"Fork\" (arriba a la derecha)</li> <li>Click en \"Create fork\"</li> <li>Listo! Ahora tienes <code>github.com/TU_USUARIO/ejercicios-bigdata</code></li> </ol> <p>Solo haces esto UNA VEZ</p> <p>Tu fork es tuyo para siempre. Todos tus trabajos van ahi.</p>"},{"location":"entregas/guia-entregas/#paso-2-descargar-a-tu-pc-solo-una-vez","title":"Paso 2: Descargar a tu PC (solo una vez)","text":"<pre><code># En tu terminal (CMD, PowerShell, o Terminal)\ncd Documentos\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\ncd ejercicios-bigdata\n</code></pre> <p>Cambia <code>TU_USUARIO</code> por tu nombre de usuario de GitHub.</p> <p>Ahora tienes la carpeta</p> <p>Busca en <code>Documentos/ejercicios-bigdata/</code>. Ahi trabajaras siempre.</p>"},{"location":"entregas/guia-entregas/#paso-3-crear-tu-carpeta-de-entrega","title":"Paso 3: Crear tu carpeta de entrega","text":"<p>Dentro de tu carpeta del repositorio, crea tu carpeta personal:</p> <pre><code>Para Trabajo Final:\nentregas/trabajo_final/apellido_nombre/\n\nPara ejercicios de BD:\nentregas/01_bases_de_datos/1.1_sqlite/apellido_nombre/\n</code></pre> <p>Formato del nombre</p> <ul> <li>Todo en minusculas</li> <li>Sin tildes ni espacios</li> <li>Formato: <code>apellido_nombre</code></li> <li>Ejemplo: <code>garcia_maria</code>, <code>lopez_juan</code></li> </ul>"},{"location":"entregas/guia-entregas/#para-el-trabajo-final-copia-la-plantilla","title":"Para el Trabajo Final, copia la plantilla:","text":"<pre><code># Desde la carpeta del repositorio:\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/tu_apellido_nombre/\n</code></pre> <p>Esto te crea todos los archivos que necesitas completar.</p>"},{"location":"entregas/guia-entregas/#paso-4-trabajar-y-documentar-tus-prompts","title":"Paso 4: Trabajar y documentar tus prompts","text":""},{"location":"entregas/guia-entregas/#el-archivo-mas-importante-promptsmd","title":"El archivo mas importante: PROMPTS.md","text":"<p>Dentro de tu carpeta encontraras <code>PROMPTS.md</code>. Este archivo es LO QUE SE EVALUA.</p> <pre><code>entregas/trabajo_final/garcia_maria/\n\u251c\u2500\u2500 PROMPTS.md          \u2190 OBLIGATORIO - Tus prompts de IA\n\u251c\u2500\u2500 docker-compose.yml  \u2190 Tu infraestructura\n\u251c\u2500\u2500 pipeline.py         \u2190 Tu codigo\n\u2514\u2500\u2500 ... otros archivos\n</code></pre>"},{"location":"entregas/guia-entregas/#que-va-en-promptsmd","title":"Que va en PROMPTS.md","text":"Seccion Que poner Prompt A, B, C Tus prompts REALES copiados tal cual (con errores y todo) Blueprint Al final, pedirle a la IA un resumen profesional <p>MUY IMPORTANTE</p> <p>NO corrijas tus prompts. Si escribiste \"como ago q sparck lea el csv\" con errores, pega ESO. El sistema detecta si \"limpiaste\" tus prompts.</p> <p>Los prompts perfectos en la Parte 1 = SOSPECHOSO.</p>"},{"location":"entregas/guia-entregas/#paso-5-subir-tu-trabajo","title":"Paso 5: Subir tu trabajo","text":"<p>Cuando termines (o quieras guardar avances):</p> <pre><code># Desde la carpeta del repositorio\ngit add .\ngit commit -m \"Entrega Trabajo Final - Garcia Maria\"\ngit push\n</code></pre> <p>Que hace cada comando</p> <ul> <li><code>git add .</code> \u2192 Prepara todos tus archivos</li> <li><code>git commit -m \"...\"</code> \u2192 Guarda con un mensaje</li> <li><code>git push</code> \u2192 Sube a tu fork en GitHub</li> </ul>"},{"location":"entregas/guia-entregas/#paso-6-verificar-tu-entrega","title":"Paso 6: Verificar tu entrega","text":"<ol> <li>Ve a tu fork: <code>github.com/TU_USUARIO/ejercicios-bigdata</code></li> <li>Navega a <code>entregas/trabajo_final/tu_apellido_nombre/</code></li> <li>Verifica que estan todos tus archivos</li> </ol> <p>Listo!</p> <p>No necesitas hacer nada mas. El sistema automatico revisa tu archivo PROMPTS.md y genera notas basado en tu proceso de aprendizaje.</p>"},{"location":"entregas/guia-entregas/#mantener-tu-fork-actualizado","title":"Mantener tu Fork Actualizado","text":"<p>El profesor agrega ejercicios nuevos. Tu fork NO se actualiza solo.</p>"},{"location":"entregas/guia-entregas/#metodo-facil-desde-github","title":"Metodo Facil (desde GitHub)","text":"<ol> <li>Ve a tu fork en GitHub</li> <li>Si ves un banner amarillo \"This branch is X commits behind\", haz click</li> <li>Click en \"Sync fork\" \u2192 \"Update branch\"</li> <li>En tu PC: <code>git pull</code></li> </ol>"},{"location":"entregas/guia-entregas/#metodo-terminal","title":"Metodo Terminal","text":"<pre><code># Agregar el repo del profesor como \"upstream\" (solo una vez)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Actualizar\ngit fetch upstream\ngit merge upstream/main\ngit push\n</code></pre> <p>Cuando sincronizar</p> <p>Hazlo cada lunes antes de clase para tener los ejercicios nuevos.</p>"},{"location":"entregas/guia-entregas/#estructura-de-entrega-trabajo-final","title":"Estructura de Entrega - Trabajo Final","text":"<pre><code>entregas/trabajo_final/apellido_nombre/\n\u2502\n\u251c\u2500\u2500 PROMPTS.md              \u2190 LO MAS IMPORTANTE (se evalua esto)\n\u2502\n\u251c\u2500\u2500 01_README.md            \u2190 Tu pregunta de investigacion\n\u251c\u2500\u2500 02_INFRAESTRUCTURA.md   \u2190 Explicacion de tu Docker\n\u251c\u2500\u2500 03_RESULTADOS.md        \u2190 Graficos e interpretacion\n\u251c\u2500\u2500 04_REFLEXION_IA.md      \u2190 3 momentos clave\n\u251c\u2500\u2500 05_RESPUESTAS.md        \u2190 Preguntas de comprension\n\u2502\n\u251c\u2500\u2500 docker-compose.yml      \u2190 Tu YAML funcional\n\u251c\u2500\u2500 pipeline.py             \u2190 Tu codigo ETL + analisis\n\u251c\u2500\u2500 requirements.txt        \u2190 Dependencias\n\u2502\n\u2514\u2500\u2500 .gitignore              \u2190 Excluir datos grandes\n</code></pre>"},{"location":"entregas/guia-entregas/#que-no-subir","title":"Que NO Subir","text":"<p>El <code>.gitignore</code> ya protege esto, pero recuerda:</p> <ul> <li>\u274c Archivos de datos (<code>.csv</code>, <code>.parquet</code>, <code>.db</code>)</li> <li>\u274c Carpeta <code>venv/</code> o <code>.venv/</code></li> <li>\u274c Carpeta <code>__pycache__/</code></li> <li>\u274c Archivos <code>.env</code> con credenciales</li> <li>\u274c Archivos mayores a 10MB</li> </ul>"},{"location":"entregas/guia-entregas/#como-se-evalua-sistema-por-prompts","title":"Como se Evalua (Sistema por PROMPTS)","text":"<p>LO MAS IMPORTANTE: PROMPTS.md</p> <p>El archivo PROMPTS.md es lo que se evalua. No el codigo, no el YAML, sino TUS PROMPTS de IA documentados.</p> <p>El sistema automatico revisa:</p> <pre><code>1. Lee la lista de alumnos (forks registrados)\n2. Para cada fork:\n   - Verifica que existe PROMPTS.md (OBLIGATORIO)\n   - Analiza calidad y autenticidad de los prompts\n   - Detecta si los prompts fueron \"limpiados\" por IA\n   - Revisa coherencia entre prompts y codigo entregado\n   - Calcula nota automatica basada en proceso de aprendizaje\n3. Genera reporte con:\n   - Ranking de todos\n   - Destacados (posible bonus)\n   - Sospechosos (requieren verificacion)\n</code></pre>"},{"location":"entregas/guia-entregas/#alertas-automaticas","title":"Alertas Automaticas","text":"Alerta Significado \u2b50 DESTACADO Trabajo excepcional, revisar para bonus \u2705 NORMAL Cumple requisitos, nota automatica \u26a0\ufe0f REVISAR Algo no cuadra, el profesor verificara \u274c RECHAZADO Copia detectada o requisitos no cumplidos"},{"location":"entregas/guia-entregas/#problemas-comunes","title":"Problemas Comunes","text":""},{"location":"entregas/guia-entregas/#no-tengo-la-plantilla","title":"\"No tengo la plantilla\"","text":"<pre><code># Actualiza tu fork primero\ngit fetch upstream\ngit merge upstream/main\n\n# Ahora copia la plantilla\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/tu_apellido/\n</code></pre>"},{"location":"entregas/guia-entregas/#git-me-pide-usuario-y-contrasena","title":"\"Git me pide usuario y contrasena\"","text":"<p>Usa tu cuenta de GitHub. Si falla, configura:</p> <pre><code>git config --global user.email \"tu@email.com\"\ngit config --global user.name \"Tu Nombre\"\n</code></pre>"},{"location":"entregas/guia-entregas/#mis-cambios-no-aparecen-en-github","title":"\"Mis cambios no aparecen en GitHub\"","text":"<p>Verifica que hiciste los 3 pasos:</p> <pre><code>git add .                    # 1. Preparar\ngit commit -m \"mensaje\"      # 2. Guardar\ngit push                     # 3. Subir  \u2190 Este es el que sube\n</code></pre>"},{"location":"entregas/guia-entregas/#quiero-empezar-de-nuevo","title":"\"Quiero empezar de nuevo\"","text":"<pre><code># Borrar tu carpeta y copiar plantilla de nuevo\nrm -rf entregas/trabajo_final/tu_apellido/\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/tu_apellido/\n</code></pre>"},{"location":"entregas/guia-entregas/#fechas-y-plazos","title":"Fechas y Plazos","text":"Entrega Fecha limite Trabajo Final [Ver calendario del curso] <p>Entregas tardias</p> <p>El sistema revisa en la fecha indicada. Lo que no este en tu fork para esa fecha, no se evalua.</p>"},{"location":"entregas/guia-entregas/#resumen-visual","title":"Resumen Visual","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    TU FLUJO DE TRABAJO                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  1. Fork (una vez)                                              \u2502\n\u2502     \u2514\u2500\u2500 Creas tu copia en GitHub                                \u2502\n\u2502                                                                 \u2502\n\u2502  2. Clone (una vez)                                             \u2502\n\u2502     \u2514\u2500\u2500 Descargas a tu PC                                       \u2502\n\u2502                                                                 \u2502\n\u2502  3. Copias plantilla                                            \u2502\n\u2502     \u2514\u2500\u2500 cp -r trabajo_final/plantilla/ entregas/.../tu_nombre/  \u2502\n\u2502                                                                 \u2502\n\u2502  4. Trabajas con IA                                             \u2502\n\u2502     \u2514\u2500\u2500 Guardas prompts en PROMPTS.md (con errores y todo)      \u2502\n\u2502                                                                 \u2502\n\u2502  5. Subes cambios                                               \u2502\n\u2502     \u2514\u2500\u2500 git add . &amp;&amp; git commit -m \"...\" &amp;&amp; git push            \u2502\n\u2502                                                                 \u2502\n\u2502  6. Verificas en GitHub                                         \u2502\n\u2502     \u2514\u2500\u2500 Confirmas que tus archivos estan ahi                    \u2502\n\u2502                                                                 \u2502\n\u2502  7. Evaluacion automatica                                       \u2502\n\u2502     \u2514\u2500\u2500 El profesor revisa todos los forks sin que hagas nada   \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"entregas/guia-entregas/#ayuda","title":"Ayuda","text":"<p>Si tienes problemas:</p> <ol> <li>Revisa esta guia de nuevo</li> <li>Pregunta a un companero</li> <li>Pregunta al profesor en clase</li> <li>Revisa la guia de sincronizacion</li> </ol> <p>Ultima actualizacion: 2026-02-04</p>"},{"location":"entregas/streaming-cloud/","title":"Entregas: Streaming y Cloud (Modulos 08-09)","text":"<p>Esta guia explica como entregar los ejercicios de Streaming con Kafka y Cloud con LocalStack.</p>"},{"location":"entregas/streaming-cloud/#resumen","title":"Resumen","text":"<p>Estos modulos son opcionales avanzados. Los completas si quieres ir mas alla del Trabajo Final basico.</p> Modulo Tema Entrega 08 Kafka + Spark Streaming <code>entregas/streaming_cloud/kafka/</code> 09 LocalStack + Terraform <code>entregas/streaming_cloud/localstack/</code>"},{"location":"entregas/streaming-cloud/#estructura-de-entrega","title":"Estructura de Entrega","text":"<pre><code>entregas/streaming_cloud/apellido_nombre/\n\u2502\n\u251c\u2500\u2500 PROMPTS.md              \u2190 LO MAS IMPORTANTE\n\u2502\n\u251c\u2500\u2500 kafka/                  \u2190 Modulo 08 (si lo completaste)\n\u2502   \u251c\u2500\u2500 docker-compose.yml  \u2190 Kafka en KRaft mode\n\u2502   \u251c\u2500\u2500 productor.py        \u2190 Tu productor\n\u2502   \u251c\u2500\u2500 consumidor.py       \u2190 Tu consumidor\n\u2502   \u2514\u2500\u2500 capturas/\n\u2502       \u251c\u2500\u2500 kafka_ui.png\n\u2502       \u2514\u2500\u2500 alertas.png\n\u2502\n\u2514\u2500\u2500 localstack/             \u2190 Modulo 09 (si lo completaste)\n    \u251c\u2500\u2500 docker-compose.yml  \u2190 LocalStack\n    \u251c\u2500\u2500 main.tf             \u2190 Tu Terraform\n    \u251c\u2500\u2500 lambdas/\n    \u2502   \u2514\u2500\u2500 capturar.py\n    \u2514\u2500\u2500 capturas/\n        \u251c\u2500\u2500 terraform_apply.png\n        \u2514\u2500\u2500 s3_bucket.png\n</code></pre>"},{"location":"entregas/streaming-cloud/#el-archivo-promptsmd","title":"El Archivo PROMPTS.md","text":"<p>Igual que el Trabajo Final, lo mas importante es documentar tus prompts reales.</p>"},{"location":"entregas/streaming-cloud/#plantilla","title":"Plantilla","text":"<pre><code># PROMPTS - Streaming y Cloud\n\n**Alumno:** [Tu nombre]\n**Fecha:** [Fecha de entrega]\n**Modulos completados:** [08 / 09 / ambos]\n\n---\n\n## Parte 1: Mis Prompts (TAL CUAL los escribi)\n\n### Reto 1: Levantar Kafka\n[Pega tu prompt real, con errores y todo]\n\n**Respuesta de la IA:**\n[Resumen de lo que te respondio]\n\n**Resultado:**\n- [ ] Funciono a la primera\n- [ ] Tuve que ajustar (explica que)\n- [ ] No funciono (explica el error)\n\n### Reto 2: Productor\n[...]\n\n### Reto 3: Consumidor\n[...]\n\n(continua con cada reto que completaste)\n\n---\n\n## Parte 2: Capturas de Pantalla\n\nIncluye capturas de:\n- Kafka UI mostrando mensajes (si hiciste 08)\n- Alertas en consola (si hiciste 08)\n- `terraform apply` exitoso (si hiciste 09)\n- S3 bucket con datos (si hiciste 09)\n\n---\n\n## Parte 3: Reflexion\n\n### Que aprendi sobre streaming/cloud\n[2-3 parrafos]\n\n### Diferencia entre batch y streaming\n[Tu entendimiento]\n\n### Que haria diferente\n[Autocritica]\n\n---\n\n## Parte 4: Blueprint (generado por IA)\n\nPide a tu IA:\n&gt; \"Resume en formato profesional los prompts anteriores,\n&gt; destacando patrones de aprendizaje y progresion\"\n\n[Pega la respuesta aqui]\n</code></pre>"},{"location":"entregas/streaming-cloud/#que-se-evalua","title":"Que se Evalua","text":"Criterio Peso Descripcion Humanidad de prompts 40% Prompts reales, no limpiados Retos completados 30% Cuantos retos terminaste Capturas 15% Evidencia visual de que funciona Reflexion 15% Comprension conceptual"},{"location":"entregas/streaming-cloud/#bonus-por-dashboard","title":"Bonus por Dashboard","text":"<p>Si creaste tu propio dashboard de sismos o ISS tracker:</p> Criterio Bonus Dashboard funcional +10% Actualizacion en vivo +5% Dise\u00f1o profesional +5%"},{"location":"entregas/streaming-cloud/#como-entregar","title":"Como Entregar","text":""},{"location":"entregas/streaming-cloud/#paso-1-crear-tu-carpeta","title":"Paso 1: Crear tu carpeta","text":"<pre><code># Desde la raiz de tu fork\nmkdir -p entregas/streaming_cloud/tu_apellido_nombre\n</code></pre>"},{"location":"entregas/streaming-cloud/#paso-2-copiar-tus-archivos","title":"Paso 2: Copiar tus archivos","text":"<pre><code># Si hiciste Kafka\nmkdir -p entregas/streaming_cloud/tu_apellido_nombre/kafka\ncp docker-compose.yml productor.py consumidor.py entregas/.../kafka/\n\n# Si hiciste LocalStack\nmkdir -p entregas/streaming_cloud/tu_apellido_nombre/localstack\ncp -r *.tf lambdas/ entregas/.../localstack/\n</code></pre>"},{"location":"entregas/streaming-cloud/#paso-3-crear-promptsmd","title":"Paso 3: Crear PROMPTS.md","text":"<p>Usa la plantilla de arriba y documenta todo tu proceso.</p>"},{"location":"entregas/streaming-cloud/#paso-4-subir","title":"Paso 4: Subir","text":"<pre><code>git add .\ngit commit -m \"Entrega Streaming/Cloud - Apellido Nombre\"\ngit push\n</code></pre>"},{"location":"entregas/streaming-cloud/#retos-disponibles","title":"Retos Disponibles","text":""},{"location":"entregas/streaming-cloud/#modulo-08-kafka","title":"Modulo 08: Kafka","text":"Reto Dificultad Descripcion 1 Basica Levantar Kafka con Docker 2 Basica Crear productor Python 3 Basica Crear consumidor Python 4 Intermedia Conectar API USGS 5 Intermedia Sistema de alertas 6 Avanzada Spark Structured Streaming Final Avanzada Dashboard propio <p>Ver detalles: Streaming con Kafka</p>"},{"location":"entregas/streaming-cloud/#modulo-09-localstack","title":"Modulo 09: LocalStack","text":"Reto Dificultad Descripcion 1 Basica Levantar LocalStack 2 Basica Bucket S3 con Terraform 3 Intermedia Lambda Hello World 4 Intermedia Lambda consume API 5 Intermedia Guardar en S3 6 Avanzada EventBridge scheduling 7 Avanzada DynamoDB metadatos Final Avanzada ISS Tracker propio <p>Ver detalles: Cloud con LocalStack</p>"},{"location":"entregas/streaming-cloud/#ejemplos-de-referencia","title":"Ejemplos de Referencia","text":"<p>Los dashboards de referencia estan disponibles para inspiracion:</p> <ul> <li>Observatorio Sismico Global - Ejemplo de visualizacion de sismos</li> <li>ISS Tracker - Ejemplo de tracker espacial</li> </ul> <p>No copies</p> <p>Estos son ejemplos del profesor. Tu dashboard debe tener tu estilo propio. El sistema detecta similitud excesiva.</p>"},{"location":"entregas/streaming-cloud/#preguntas-frecuentes","title":"Preguntas Frecuentes","text":""},{"location":"entregas/streaming-cloud/#es-obligatorio","title":"Es obligatorio?","text":"<p>No. Estos modulos son bonus para quienes quieran profundizar.</p>"},{"location":"entregas/streaming-cloud/#puedo-hacer-solo-uno","title":"Puedo hacer solo uno?","text":"<p>Si. Puedes entregar solo Kafka (08) o solo LocalStack (09).</p>"},{"location":"entregas/streaming-cloud/#como-afecta-mi-nota","title":"Como afecta mi nota?","text":"<p>Completa correctamente = bonus sobre tu nota del Trabajo Final.</p>"},{"location":"entregas/streaming-cloud/#necesito-spark-para-el-modulo-08","title":"Necesito Spark para el modulo 08?","text":"<p>El reto de Spark Streaming es avanzado. Puedes hacer los retos 1-5 sin Spark.</p>"},{"location":"entregas/streaming-cloud/#localstack-es-igual-a-aws-real","title":"LocalStack es igual a AWS real?","text":"<p>El codigo es identico. La diferencia es que LocalStack corre en tu maquina sin costos. Si tu codigo funciona en LocalStack, funciona en AWS.</p> <p>Curso: Big Data con Python - De Cero a Produccion Profesor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Metodologia: Ejercicios progresivos con datos reales y herramientas profesionales</p> <p>Referencias: - Kreps, J., Narkhede, N., &amp; Rao, J. (2011). Kafka: A distributed messaging system for log processing. - Brikman, Y. (2019). Terraform: Up &amp; Running (2<sup>nd</sup> ed.). O'Reilly Media. - LocalStack Team (2024). LocalStack Documentation. https://docs.localstack.cloud/</p>"},{"location":"git-github/","title":"Git y GitHub","text":"<p>Guias para trabajar con Git y GitHub en este curso.</p>"},{"location":"git-github/#que-encontraras-aqui","title":"Que encontraras aqui?","text":""},{"location":"git-github/#fork-y-clone","title":"Fork y Clone","text":"<p>Aprende a crear tu copia del repositorio y clonarlo a tu computadora:</p> <ul> <li>Que es un Fork y por que lo necesitas</li> <li>Como hacer Fork del repositorio</li> <li>Como clonar tu Fork a tu PC</li> <li>Configurar remotes (origin y upstream)</li> </ul>"},{"location":"git-github/#sincronizar-fork","title":"Sincronizar Fork","text":"<p>Mantener tu Fork actualizado con los ejercicios nuevos del profesor:</p> <ul> <li>Por que tu Fork NO se actualiza automaticamente</li> <li>Como sincronizar desde GitHub Web (facil)</li> <li>Como sincronizar desde Terminal (completo)</li> <li>Resolver conflictos de merge</li> <li>Diagramas visuales del flujo completo</li> </ul>"},{"location":"git-github/#comandos-utiles","title":"Comandos Utiles","text":"<p>Cheatsheet de Git para el dia a dia:</p> <ul> <li>Comandos basicos</li> <li>Comandos avanzados</li> <li>Atajos utiles</li> <li>Resolver problemas comunes</li> </ul>"},{"location":"git-github/#flujo-de-trabajo-sin-pull-request","title":"Flujo de Trabajo (Sin Pull Request)","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Clone a PC]\n    B --&gt; C[Trabajar]\n    C --&gt; D[Documentar PROMPTS.md]\n    D --&gt; E[Commit]\n    E --&gt; F[Push a tu Fork]\n    F --&gt; G[Evaluacion Automatica]</code></pre> <p>Sistema simplificado</p> <p>No necesitas crear Pull Request. El sistema evalua tu <code>PROMPTS.md</code> automaticamente. Solo sube tu trabajo con <code>git push</code>.</p>"},{"location":"git-github/#conceptos-basicos","title":"Conceptos Basicos","text":""},{"location":"git-github/#git-vs-github","title":"Git vs GitHub","text":"<p>Git</p> <p>Git es un sistema de control de versiones que funciona en tu computadora. Te permite:</p> <ul> <li>Guardar versiones de tu codigo</li> <li>Volver a versiones anteriores</li> <li>Trabajar en multiples ramas</li> <li>Colaborar con otros</li> </ul> <p>GitHub</p> <p>GitHub es una plataforma en la nube donde guardas tu codigo. Te permite:</p> <ul> <li>Compartir codigo publicamente</li> <li>Colaborar con otros desarrolladores</li> <li>Alojar proyectos</li> <li>Gestionar proyectos y colaboracion</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  GIT vs GITHUB                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  GIT (Programa en tu PC)                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  Tu computadora                       \u2502                 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502  \u2502  \u2502  \ud83d\udcc1 Carpeta con tu codigo       \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 ejercicio1.py              \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 ejercicio2.py              \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500 .git/  \u2190 Historial local  \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                      \u2502                                       \u2502\n\u2502                      \u2502 git push (subir)                     \u2502\n\u2502                      \u2193                                       \u2502\n\u2502  GITHUB (En Internet)                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  \ud83c\udf10 github.com                        \u2502                 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502  \u2502  \u2502  \ud83d\udce6 Tu repositorio online       \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  (Visible en el navegador)      \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"git-github/#primeros-pasos","title":"Primeros Pasos","text":"<p>Nunca usaste Git?</p> <p>Empieza con Fork y Clone donde te explicamos todo desde cero.</p> <p>Ya tienes el repositorio clonado?</p> <p>Aprende a Sincronizar tu Fork para obtener ejercicios nuevos.</p> <p>Completaste un ejercicio?</p> <p>Solo haz <code>git push</code> a tu fork. Lee la Guia de Entregas.</p>"},{"location":"git-github/#ayuda-y-recursos","title":"Ayuda y Recursos","text":""},{"location":"git-github/#problemas-comunes","title":"Problemas Comunes","text":"<p>Consulta la seccion de Comandos Utiles donde encontraras soluciones a problemas frecuentes como:</p> <ul> <li>\"fatal: not a git repository\"</li> <li>\"Your branch is behind origin/main\"</li> <li>\"CONFLICT (content): Merge conflict\"</li> <li>\"Permission denied (publickey)\"</li> </ul>"},{"location":"git-github/#recursos-externos","title":"Recursos Externos","text":"<ul> <li>Git Handbook</li> <li>GitHub Guides</li> <li>Atlassian Git Tutorial</li> <li>Oh Shit, Git!?! - Para cuando algo sale mal</li> </ul>"},{"location":"git-github/comandos-utiles/","title":"Comandos Utiles de Git","text":"<p>Cheatsheet de comandos Git para el dia a dia en el curso.</p>"},{"location":"git-github/comandos-utiles/#comandos-basicos","title":"Comandos Basicos","text":""},{"location":"git-github/comandos-utiles/#configuracion-inicial","title":"Configuracion Inicial","text":"<pre><code># Configurar nombre\ngit config --global user.name \"Tu Nombre\"\n\n# Configurar email\ngit config --global user.email \"tu@email.com\"\n\n# Ver configuracion\ngit config --list\n\n# Ver configuracion especifica\ngit config user.name\n</code></pre>"},{"location":"git-github/comandos-utiles/#clonar-y-actualizar","title":"Clonar y Actualizar","text":"<pre><code># Clonar tu fork\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Entrar a la carpeta\ncd ejercicios-bigdata\n\n# Agregar upstream (repo del profesor)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Ver remotes configurados\ngit remote -v\n</code></pre>"},{"location":"git-github/comandos-utiles/#trabajo-diario","title":"Trabajo Diario","text":""},{"location":"git-github/comandos-utiles/#estado-y-cambios","title":"Estado y Cambios","text":"<pre><code># Ver estado actual\ngit status\n\n# Ver cambios no guardados\ngit diff\n\n# Ver cambios en un archivo especifico\ngit diff archivo.py\n\n# Ver historial de commits\ngit log\n\n# Ver historial resumido\ngit log --oneline\n\n# Ver cambios de un commit especifico\ngit show abc123d\n</code></pre>"},{"location":"git-github/comandos-utiles/#guardar-cambios","title":"Guardar Cambios","text":"<pre><code># Agregar archivo especifico\ngit add archivo.py\n\n# Agregar todos los archivos modificados\ngit add .\n\n# Agregar solo archivos Python\ngit add *.py\n\n# Hacer commit\ngit commit -m \"Mensaje descriptivo\"\n\n# Hacer commit de archivos ya trackeados (skip add)\ngit commit -am \"Mensaje descriptivo\"\n\n# Modificar ultimo commit (antes de push)\ngit commit --amend -m \"Nuevo mensaje\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#ramas-branches","title":"Ramas (Branches)","text":""},{"location":"git-github/comandos-utiles/#crear-y-cambiar","title":"Crear y Cambiar","text":"<pre><code># Ver ramas locales\ngit branch\n\n# Ver todas las ramas (incluyendo remotas)\ngit branch -a\n\n# Crear nueva rama\ngit branch garcia-ejercicio-01\n\n# Cambiar a una rama\ngit checkout garcia-ejercicio-01\n\n# Crear y cambiar en un solo comando\ngit checkout -b garcia-ejercicio-01\n\n# Cambiar a main\ngit checkout main\n</code></pre>"},{"location":"git-github/comandos-utiles/#fusionar-y-borrar","title":"Fusionar y Borrar","text":"<pre><code># Fusionar una rama en la actual\ngit merge nombre-rama\n\n# Borrar rama local\ngit branch -d garcia-ejercicio-01\n\n# Forzar borrado (si tiene cambios sin fusionar)\ngit branch -D garcia-ejercicio-01\n\n# Borrar rama remota\ngit push origin --delete garcia-ejercicio-01\n</code></pre>"},{"location":"git-github/comandos-utiles/#sincronizacion","title":"Sincronizacion","text":""},{"location":"git-github/comandos-utiles/#descargar-cambios","title":"Descargar Cambios","text":"<pre><code># Descargar cambios del profesor (upstream)\ngit fetch upstream\n\n# Descargar y fusionar de tu fork (origin)\ngit pull origin main\n\n# Ver diferencias con upstream\ngit log HEAD..upstream/main\n\n# Ver commits que tu no tienes\ngit log HEAD..upstream/main --oneline\n</code></pre>"},{"location":"git-github/comandos-utiles/#sincronizar-fork-completo","title":"Sincronizar Fork Completo","text":"<pre><code># Workflow completo para sincronizar\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\ngit push origin main\n\n# O en una sola linea\ngit checkout main &amp;&amp; git fetch upstream &amp;&amp; git merge upstream/main &amp;&amp; git push origin main\n</code></pre>"},{"location":"git-github/comandos-utiles/#subir-cambios","title":"Subir Cambios","text":"<pre><code># Subir rama actual a origin\ngit push origin nombre-rama\n\n# Subir main\ngit push origin main\n\n# Subir y establecer upstream (primera vez)\ngit push -u origin nombre-rama\n\n# Despues solo necesitas\ngit push\n</code></pre>"},{"location":"git-github/comandos-utiles/#resolver-problemas","title":"Resolver Problemas","text":""},{"location":"git-github/comandos-utiles/#deshacer-cambios","title":"Deshacer Cambios","text":"<pre><code># Descartar cambios en un archivo (antes de add)\ngit checkout -- archivo.py\n\n# Descartar todos los cambios no guardados\ngit checkout -- .\n\n# Quitar archivo del staging (despues de add, antes de commit)\ngit reset HEAD archivo.py\n\n# Deshacer ultimo commit (mantiene cambios)\ngit reset --soft HEAD~1\n\n# Deshacer ultimo commit (descarta cambios)\ngit reset --hard HEAD~1\n\n# Volver a un commit especifico\ngit reset --hard abc123d\n</code></pre> <p>Cuidado con --hard</p> <p><code>git reset --hard</code> elimina cambios permanentemente. Usalo solo si estas seguro.</p>"},{"location":"git-github/comandos-utiles/#stash-guardar-temporalmente","title":"Stash (Guardar Temporalmente)","text":"<pre><code># Guardar cambios temporalmente\ngit stash\n\n# Guardar con mensaje\ngit stash save \"WIP: trabajando en ejercicio 03\"\n\n# Ver lista de stashes\ngit stash list\n\n# Aplicar ultimo stash\ngit stash apply\n\n# Aplicar y eliminar ultimo stash\ngit stash pop\n\n# Aplicar stash especifico\ngit stash apply stash@{1}\n\n# Eliminar stash\ngit stash drop stash@{0}\n\n# Eliminar todos los stashes\ngit stash clear\n</code></pre>"},{"location":"git-github/comandos-utiles/#conflictos","title":"Conflictos","text":"<pre><code># Ver archivos con conflictos\ngit status\n\n# Despues de resolver manualmente\ngit add archivo-resuelto.py\ngit commit -m \"Resolver conflicto en archivo\"\n\n# Abortar merge con conflictos\ngit merge --abort\n\n# Ver herramienta de merge\ngit mergetool\n</code></pre>"},{"location":"git-github/comandos-utiles/#informacion-y-busqueda","title":"Informacion y Busqueda","text":""},{"location":"git-github/comandos-utiles/#inspeccionar-historial","title":"Inspeccionar Historial","text":"<pre><code># Ver historial detallado\ngit log --graph --decorate --all\n\n# Ver quien modifico cada linea de un archivo\ngit blame archivo.py\n\n# Buscar en el historial\ngit log --grep=\"palabra clave\"\n\n# Ver archivos modificados en cada commit\ngit log --stat\n\n# Ver cambios de un autor especifico\ngit log --author=\"Tu Nombre\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#buscar-codigo","title":"Buscar Codigo","text":"<pre><code># Buscar en archivos trackeados\ngit grep \"palabra clave\"\n\n# Buscar en archivos Python\ngit grep \"palabra clave\" -- \"*.py\"\n\n# Buscar mostrando numero de linea\ngit grep -n \"palabra clave\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#atajos-y-aliases","title":"Atajos y Aliases","text":""},{"location":"git-github/comandos-utiles/#configurar-aliases","title":"Configurar Aliases","text":"<pre><code># Crear alias para status\ngit config --global alias.st status\n\n# Crear alias para checkout\ngit config --global alias.co checkout\n\n# Crear alias para commit\ngit config --global alias.ci commit\n\n# Crear alias para branch\ngit config --global alias.br branch\n\n# Alias para log bonito\ngit config --global alias.lg \"log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#usar-aliases","title":"Usar Aliases","text":"<pre><code># En lugar de: git status\ngit st\n\n# En lugar de: git checkout main\ngit co main\n\n# En lugar de: git commit -m \"mensaje\"\ngit ci -m \"mensaje\"\n\n# Ver log bonito\ngit lg\n</code></pre>"},{"location":"git-github/comandos-utiles/#workflow-del-curso","title":"Workflow del Curso","text":""},{"location":"git-github/comandos-utiles/#empezar-nuevo-ejercicio","title":"Empezar Nuevo Ejercicio","text":"<pre><code># 1. Actualizar main\ngit checkout main\ngit pull origin main\ngit fetch upstream\ngit merge upstream/main\n\n# 2. Crear rama para ejercicio\ngit checkout -b garcia-ejercicio-01\n\n# 3. Trabajar...\n# ... editar archivos ...\n\n# 4. Guardar trabajo\ngit add .\ngit commit -m \"Implementar carga de datos SQLite\"\n\n# 5. Subir a GitHub\ngit push -u origin garcia-ejercicio-01\n</code></pre>"},{"location":"git-github/comandos-utiles/#actualizar-rama-de-ejercicio","title":"Actualizar Rama de Ejercicio","text":"<pre><code># Si el profesor agrego cambios mientras trabajabas\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\ngit checkout garcia-ejercicio-01\ngit merge main\ngit push origin garcia-ejercicio-01\n</code></pre>"},{"location":"git-github/comandos-utiles/#aplicar-feedback-del-profesor","title":"Aplicar Feedback del Profesor","text":"<pre><code># 1. Asegurate de estar en tu rama\ngit checkout garcia-ejercicio-01\n\n# 2. Hacer correcciones\n# ... editar archivos ...\n\n# 3. Guardar y subir\ngit add .\ngit commit -m \"Aplicar feedback: optimizar queries\"\ngit push origin garcia-ejercicio-01\n\n# El PR se actualiza automaticamente\n</code></pre>"},{"location":"git-github/comandos-utiles/#comandos-avanzados","title":"Comandos Avanzados","text":""},{"location":"git-github/comandos-utiles/#cherry-pick","title":"Cherry Pick","text":"<pre><code># Aplicar un commit especifico a la rama actual\ngit cherry-pick abc123d\n\n# Aplicar sin hacer commit automatico\ngit cherry-pick -n abc123d\n</code></pre>"},{"location":"git-github/comandos-utiles/#rebase","title":"Rebase","text":"<pre><code># Rebase rama actual con main\ngit rebase main\n\n# Rebase interactivo (ultimos 3 commits)\ngit rebase -i HEAD~3\n\n# Continuar rebase despues de resolver conflictos\ngit rebase --continue\n\n# Abortar rebase\ngit rebase --abort\n</code></pre> <p>Cuidado con Rebase</p> <p>No hagas rebase de commits que ya subiste a GitHub (despues de push).</p>"},{"location":"git-github/comandos-utiles/#reflog","title":"Reflog","text":"<pre><code># Ver historial de todas las acciones\ngit reflog\n\n# Recuperar commit \"perdido\"\ngit reflog\ngit checkout abc123d\ngit checkout -b rama-recuperada\n</code></pre>"},{"location":"git-github/comandos-utiles/#trucos-y-tips","title":"Trucos y Tips","text":""},{"location":"git-github/comandos-utiles/#configuracion-util","title":"Configuracion Util","text":"<pre><code># Colorear output\ngit config --global color.ui auto\n\n# Editor por defecto (VSCode)\ngit config --global core.editor \"code --wait\"\n\n# Guardar credenciales temporalmente\ngit config --global credential.helper cache\n\n# Guardar credenciales permanentemente (Windows)\ngit config --global credential.helper wincred\n\n# Ignorar cambios en permisos de archivos\ngit config core.fileMode false\n</code></pre>"},{"location":"git-github/comandos-utiles/#gitignore","title":".gitignore","text":"<p>Crear archivo <code>.gitignore</code> en la raiz del proyecto:</p> <pre><code># Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\n.venv/\n\n# PyCharm\n.idea/\n\n# VSCode\n.vscode/\n\n# Jupyter\n.ipynb_checkpoints/\n\n# Datos grandes\n*.csv\n*.db\n*.parquet\ndatos/grandes/\n\n# Sistema\n.DS_Store\nThumbs.db\n</code></pre>"},{"location":"git-github/comandos-utiles/#errores-comunes-y-soluciones","title":"Errores Comunes y Soluciones","text":""},{"location":"git-github/comandos-utiles/#fatal-not-a-git-repository","title":"\"fatal: not a git repository\"","text":"<pre><code># Solucion: Navega a la carpeta del proyecto\ncd path/to/ejercicios-bigdata\n\n# Verifica que estas en la carpeta correcta\ngit status\n</code></pre>"},{"location":"git-github/comandos-utiles/#your-branch-is-behind-originmain","title":"\"Your branch is behind 'origin/main'\"","text":"<pre><code># Solucion: Actualiza tu rama local\ngit pull origin main\n</code></pre>"},{"location":"git-github/comandos-utiles/#conflict-content-merge-conflict","title":"\"CONFLICT (content): Merge conflict\"","text":"<pre><code># Solucion:\n# 1. Abre el archivo con conflicto\n# 2. Busca las marcas &lt;&lt;&lt;&lt;&lt;&lt;&lt; ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n# 3. Edita manualmente, elige que codigo mantener\n# 4. Elimina las marcas\n# 5. Guarda y haz commit\ngit add archivo-resuelto.py\ngit commit -m \"Resolver conflicto\"\n</code></pre>"},{"location":"git-github/comandos-utiles/#permission-denied-publickey","title":"\"Permission denied (publickey)\"","text":"<pre><code># Solucion: Usa HTTPS en lugar de SSH\ngit remote set-url origin https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre>"},{"location":"git-github/comandos-utiles/#error-failed-to-push-some-refs","title":"\"error: failed to push some refs\"","text":"<pre><code># Causa: Tu rama local esta detras de la remota\n# Solucion: Pull primero\ngit pull origin tu-rama\n# Luego push\ngit push origin tu-rama\n</code></pre>"},{"location":"git-github/comandos-utiles/#comandos-de-emergencia","title":"Comandos de Emergencia","text":""},{"location":"git-github/comandos-utiles/#recuperar-trabajo-perdido","title":"Recuperar Trabajo Perdido","text":"<pre><code># Ver todos los cambios\ngit reflog\n\n# Volver a un estado anterior\ngit reset --hard abc123d\n\n# Recuperar archivo borrado\ngit checkout HEAD -- archivo.py\n</code></pre>"},{"location":"git-github/comandos-utiles/#limpiar-repositorio","title":"Limpiar Repositorio","text":"<pre><code># Eliminar archivos no trackeados (dry run)\ngit clean -n\n\n# Eliminar archivos no trackeados (ejecutar)\ngit clean -f\n\n# Eliminar archivos y carpetas no trackeadas\ngit clean -fd\n\n# Incluir archivos ignorados en .gitignore\ngit clean -fdx\n</code></pre>"},{"location":"git-github/comandos-utiles/#recursos-adicionales","title":"Recursos Adicionales","text":""},{"location":"git-github/comandos-utiles/#ayuda-integrada","title":"Ayuda Integrada","text":"<pre><code># Ayuda general\ngit help\n\n# Ayuda de un comando especifico\ngit help commit\ngit commit --help\n\n# Version corta de ayuda\ngit commit -h\n</code></pre>"},{"location":"git-github/comandos-utiles/#links-utiles","title":"Links Utiles","text":"<ul> <li>Git Cheat Sheet (GitHub)</li> <li>Visualizar Git</li> <li>Learn Git Branching</li> <li>Oh Shit, Git!?!</li> </ul>"},{"location":"git-github/comandos-utiles/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que conoces los comandos esenciales:</p> <ul> <li>Fork y Clone - Setup inicial del proyecto</li> <li>Sincronizar Fork - Mantener tu fork actualizado</li> <li>Guia de Entregas - Como entregar ejercicios</li> </ul>"},{"location":"git-github/fork-clone/","title":"Fork y Clone","text":"<p>Guia completa para crear tu copia del repositorio y trabajar con ella.</p>"},{"location":"git-github/fork-clone/#que-es-git-que-es-github","title":"Que es Git? Que es GitHub?","text":"<p>Git</p> <p>Git = Sistema de control de versiones (como \"guardar versiones\" de tu codigo)</p> <p>GitHub</p> <p>GitHub = Nube donde guardas tu codigo (como Dropbox, pero para codigo)</p> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph Local[\"\ud83d\udcbb GIT - Tu Computadora\"]\n        direction TB\n        PC[\"\ud83d\udcc1 Carpeta con tu c\u00f3digo&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 ejercicio1.py&lt;br/&gt;\u251c\u2500\u2500 ejercicio2.py&lt;br/&gt;\u2514\u2500\u2500 .git/ \u2190 Historial local\"]\n    end\n\n    subgraph Cloud[\"\ud83c\udf10 GITHUB - Internet (github.com)\"]\n        direction TB\n        Repo[\"\ud83d\udce6 Tu repositorio online&lt;br/&gt;&lt;br/&gt;Visible en el navegador&lt;br/&gt;Respaldo en la nube\"]\n    end\n\n    PC --&gt;|git push&lt;br/&gt;Subir cambios| Cloud\n    Cloud --&gt;|git pull&lt;br/&gt;Descargar cambios| PC\n\n    style Local fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style Cloud fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style PC fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n    style Repo fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px</code></pre>"},{"location":"git-github/fork-clone/#que-es-un-fork","title":"Que es un FORK?","text":"<p>Un fork es hacer TU PROPIA COPIA del repositorio del profesor en GitHub.</p> <p>Piensalo asi:</p> <ul> <li> El profesor tiene un libro (repositorio)</li> <li> Haces una fotocopia del libro completo (fork)</li> <li> Ahora puedes escribir en TU copia sin afectar el original</li> <li> Cuando termines, subes tu trabajo con <code>git push</code> (evaluacion automatica)</li> </ul> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TD\n    subgraph Original[\"\ud83d\udc68\u200d\ud83c\udfeb REPOSITORIO ORIGINAL (Profesor)\"]\n        direction TB\n        RepoProf[\"TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\ud83d\udcc1 ejercicio_01/&lt;br/&gt;\ud83d\udcc1 ejercicio_02/&lt;br/&gt;\ud83d\udcc1 datos/&lt;br/&gt;&lt;br/&gt;\ud83d\udd12 NO puedes modificar directamente\"]\n    end\n\n    ForkAction{{\"\ud83c\udf74 HACER FORK&lt;br/&gt;(Click en bot\u00f3n 'Fork')\"}}\n\n    subgraph TuCopia[\"\ud83d\udc64 TU FORK (Tu Copia Personal)\"]\n        direction TB\n        RepoTuyo[\"TU_USUARIO/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\ud83d\udcc1 ejercicio_01/&lt;br/&gt;\ud83d\udcc1 ejercicio_02/&lt;br/&gt;\ud83d\udcc1 datos/&lt;br/&gt;&lt;br/&gt;\u2705 Esta copia S\u00cd puedes modificarla\"]\n    end\n\n    Original --&gt; ForkAction\n    ForkAction --&gt;|Crea una copia&lt;br/&gt;completa e independiente| TuCopia\n\n    style Original fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style TuCopia fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style ForkAction fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n    style RepoProf fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style RepoTuyo fill:#e8f5e9,stroke:#388e3c,stroke-width:2px</code></pre>"},{"location":"git-github/fork-clone/#paso-1-hacer-fork-del-repositorio","title":"PASO 1: Hacer Fork del Repositorio","text":""},{"location":"git-github/fork-clone/#instrucciones-paso-a-paso","title":"Instrucciones Paso a Paso","text":"<p>1. Ir al repositorio del profesor:</p> <p>Abre tu navegador y ve a:</p> <pre><code>https://github.com/TodoEconometria/ejercicios-bigdata\n</code></pre> <p>2. Hacer Fork (copiar a tu cuenta):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GitHub - Pagina del Repositorio       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                          \u2502\n\u2502  [\u2b50 Star]  [\ud83c\udf74 Fork]  [\u2b07 Code]        \u2502\n\u2502              \u2191                           \u2502\n\u2502              \u2514\u2500\u2500 HAZ CLICK AQUI         \u2502\n\u2502                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Click en el boton \"Fork\" (arriba a la derecha)</li> <li>Selecciona tu cuenta de GitHub como destino</li> <li>Espera unos segundos mientras GitHub copia todo</li> </ul> <p>3. Verificar tu fork:</p> <p>Ahora deberias estar en TU copia:</p> <pre><code>https://github.com/TU_USUARIO/ejercicios-bigdata\n        \u2191\n        \u2514\u2500\u2500 Aqui debe aparecer TU nombre de usuario\n</code></pre> <p> Listo! Ya tienes tu copia personal del repositorio.</p>"},{"location":"git-github/fork-clone/#paso-2-clonar-tu-fork-a-tu-computadora","title":"PASO 2: Clonar TU Fork a Tu Computadora","text":""},{"location":"git-github/fork-clone/#que-significa-clonar","title":"Que significa \"clonar\"?","text":"<p>Clonar = Descargar todo el codigo de GitHub a tu computadora</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \ud83c\udf10 GitHub (Tu Fork)                    \u2502\n\u2502  https://github.com/TU_USUARIO/...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 git clone (descargar)\n                  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \ud83d\udcbb Tu PC                                \u2502\n\u2502  \ud83d\udcc1 Carpeta: ejercicios-bigdata/        \u2502\n\u2502     \u251c\u2500\u2500 ejercicio_01/                   \u2502\n\u2502     \u251c\u2500\u2500 ejercicio_02/                   \u2502\n\u2502     \u2514\u2500\u2500 datos/                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"git-github/fork-clone/#instrucciones-paso-a-paso_1","title":"Instrucciones Paso a Paso","text":"<p>1. Abrir la terminal/cmd:</p> WindowsmacOSLinux <p>Presiona <code>Win + R</code>, escribe <code>cmd</code>, Enter</p> <p>Busca \"Terminal\" en Spotlight (<code>Cmd + Space</code>)</p> <p>Presiona <code>Ctrl + Alt + T</code></p> <p>2. Ir a la carpeta donde quieres guardar el proyecto:</p> <pre><code># Ejemplo: Ir a Documentos\ncd Documents\n\n# O crear una carpeta nueva para tus proyectos\nmkdir mis-proyectos\ncd mis-proyectos\n</code></pre> <p>3. Clonar TU fork (reemplaza TU_USUARIO):</p> <pre><code>git clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre> <p>IMPORTANTE</p> <p>Asegurate de poner TU nombre de usuario, no \"TodoEconometria\"</p> <p>4. Entrar a la carpeta:</p> <pre><code>cd ejercicios-bigdata\n</code></pre> <p>5. Conectar con el repo original del profesor:</p> <p>Esto te permite recibir actualizaciones cuando el profesor agregue ejercicios nuevos:</p> <pre><code>git remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n</code></pre> <p>6. Verificar que todo esta bien:</p> <pre><code>git remote -v\n</code></pre> <p>Deberias ver algo asi:</p> <pre><code>origin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (fetch)\norigin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (push)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (fetch)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (push)\n</code></pre> <p> Listo! Ya tienes todo el codigo en tu computadora.</p>"},{"location":"git-github/fork-clone/#entendiendo-origin-y-upstream","title":"Entendiendo origin y upstream","text":"<p>origin</p> <p>origin = Tu fork en GitHub (donde subes tus cambios)</p> <p>upstream</p> <p>upstream = Repositorio original del profesor (de donde descargas actualizaciones)</p> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph Upstream[\"\u2b06\ufe0f UPSTREAM (Profesor)\"]\n        direction TB\n        UP[\"TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\u2713 Repo original&lt;br/&gt;\u2713 Solo lectura para ti&lt;br/&gt;\u2713 Descargas actualizaciones de aqu\u00ed\"]\n    end\n\n    subgraph Origin[\"\ud83c\udf10 ORIGIN (Tu Fork en GitHub)\"]\n        direction TB\n        OR[\"TU_USUARIO/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\u2713 Tu copia en GitHub&lt;br/&gt;\u2713 Lectura y escritura&lt;br/&gt;\u2713 Subes tus cambios aqu\u00ed\"]\n    end\n\n    subgraph Local[\"\ud83d\udcbb LOCAL (Tu PC)\"]\n        direction TB\n        LOC[\"ejercicios-bigdata/&lt;br/&gt;&lt;br/&gt;\u2713 Carpeta en tu computadora&lt;br/&gt;\u2713 Trabajas aqu\u00ed&lt;br/&gt;\u2713 Haces commits locales\"]\n    end\n\n    Upstream --&gt;|\"\ud83c\udf74 Fork\"| Origin\n    Origin --&gt;|\"\ud83d\udce5 Clone&lt;br/&gt;(git clone)\"| Local\n    Local --&gt;|\"\ud83d\udce4 Push&lt;br/&gt;(git push origin)\"| Origin\n    Upstream --&gt;|\"\ud83d\udd04 Fetch&lt;br/&gt;(git fetch upstream)\"| Local\n\n    style Upstream fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style Origin fill:#fff9c4,stroke:#f57f17,stroke-width:3px\n    style Local fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style UP fill:#bbdefb,stroke:#1976d2,stroke-width:2px\n    style OR fill:#fff59d,stroke:#f9a825,stroke-width:2px\n    style LOC fill:#c8e6c9,stroke:#43a047,stroke-width:2px</code></pre>"},{"location":"git-github/fork-clone/#flujo-completo-de-trabajo","title":"Flujo Completo de Trabajo","text":"<pre><code>graph TD\n    A[Repo Profesor&lt;br/&gt;upstream] --&gt;|1. Fork| B[Tu Fork&lt;br/&gt;origin]\n    B --&gt;|2. Clone| C[Tu PC&lt;br/&gt;local]\n    C --&gt;|3. Trabajas| D[Editar codigo]\n    D --&gt;|4. Commit| E[Guardar cambios]\n    E --&gt;|5. Push| B\n    B --&gt;|6. Evaluacion| F[Sistema evalua&lt;br/&gt;PROMPTS.md]\n    A --&gt;|7. Nuevos ejercicios| C\n\n    style A fill:#e1f5ff,stroke:#0277bd\n    style B fill:#fff9c4,stroke:#f57f17\n    style C fill:#e8f5e9,stroke:#388e3c\n    style F fill:#f3e5f5,stroke:#7b1fa2</code></pre>"},{"location":"git-github/fork-clone/#comandos-basicos","title":"Comandos Basicos","text":""},{"location":"git-github/fork-clone/#descargar-cambios-del-profesor","title":"Descargar cambios del profesor","text":"<pre><code># Descargar cambios\ngit fetch upstream\n\n# Aplicar cambios a tu rama main\ngit checkout main\ngit merge upstream/main\n\n# Subir a tu fork\ngit push origin main\n</code></pre>"},{"location":"git-github/fork-clone/#subir-tus-cambios","title":"Subir tus cambios","text":"<pre><code># Ver que cambiaste\ngit status\n\n# Agregar archivos\ngit add archivo.py\n\n# Guardar con mensaje\ngit commit -m \"Descripcion del cambio\"\n\n# Subir a tu fork\ngit push origin nombre-de-tu-rama\n</code></pre>"},{"location":"git-github/fork-clone/#problemas-comunes","title":"Problemas Comunes","text":"Error: Permission denied (publickey) <p>Causa: No tienes configuradas las SSH keys.</p> <p>Solucion: Usa HTTPS en lugar de SSH:</p> <pre><code># Usa esta URL (HTTPS):\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# NO uses esta (SSH):\ngit clone git@github.com:TU_USUARIO/ejercicios-bigdata.git\n</code></pre> Error: fatal: not a git repository <p>Causa: No estas en la carpeta del proyecto.</p> <p>Solucion:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica que estas en la carpeta correcta\nls -la  # Deberas ver una carpeta .git/\n</code></pre> Clono el repo del profesor en lugar de mi fork <p>Causa: Usaste la URL del profesor.</p> <p>Solucion:</p> <ol> <li>Borra la carpeta clonada</li> <li>Haz fork primero en GitHub</li> <li>Clona TU fork, no el del profesor</li> </ol> <pre><code># \u274c MAL\ngit clone https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# \u2705 BIEN\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre>"},{"location":"git-github/fork-clone/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que tienes el repositorio clonado:</p> <ul> <li>Tu Primer Ejercicio - Empezar a trabajar</li> <li>Sincronizar Fork - Mantener tu fork actualizado</li> <li>Guia de Entregas - Como entregar ejercicios</li> </ul>"},{"location":"git-github/sincronizar-fork/","title":"Sincronizar tu Fork","text":"<p>IMPORTANTE</p> <p>Tu fork NO se actualiza automaticamente. Debes sincronizarlo manualmente para obtener los ejercicios nuevos que el profesor agregue.</p> <p>Sistema de Evaluacion</p> <p>Ya NO se usan Pull Requests. El sistema evalua tu <code>PROMPTS.md</code> directamente en tu fork. Ver la Guia de Entregas para mas detalles.</p>"},{"location":"git-github/sincronizar-fork/#el-problema","title":"El Problema","text":"<p>Cuando haces fork, obtienes una copia en ese momento. Durante el curso agregare ejercicios nuevos, pero tu fork NO se actualiza solo.</p> <p><pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph S1[\"SEMANA 1 - Hiciste Fork\"]\n        direction LR\n        Prof1[\"Repo Profesor&lt;br/&gt;[01] [02]\"]\n        Fork1[\"Tu Fork&lt;br/&gt;[01] [02]\"]\n        Prof1 -.-&gt;|Fork| Fork1\n    end\n\n    subgraph S3[\"SEMANA 3 - Profesor agrego ejercicios\"]\n        direction LR\n        Prof3[\"Repo Profesor&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        Fork3[\"Tu Fork&lt;br/&gt;[01] [02]&lt;br/&gt;Te faltan [03] [04] [05]\"]\n    end\n\n    S1 --&gt; S3\n\n    style S1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style S3 fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style Prof1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style Fork1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n    style Prof3 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style Fork3 fill:#ffcdd2,stroke:#d32f2f,stroke-width:3px</code></pre> https://github.com/TU_USUARIO/ejercicios-bigdata <pre><code>**Paso 2:** Buscar el banner de sincronizacion\n\nCuando hay cambios nuevos, veras un banner as\u00ed:\n\n!!! example \"Banner en GitHub\"\n    ```\n    \u26a0\ufe0f This branch is 15 commits behind TodoEconometria:main\n\n    [Sync fork \u25bc]  \u2190 CLICK AQUI\n    ```\n\n**Paso 3:** Click en \"Sync fork\" \u2192 \"Update branch\"\n\n!!! example \"Opciones de sincronizaci\u00f3n\"\n    **Sync fork**\n\n    This will update your branch with the latest changes from TodoEconometria:main\n\n    **[Update branch]** \u2190 CLICK AQUI\n    [Discard commits]\n\n**Paso 4:** Actualizar tu copia local\n\nAhora tu fork en GitHub esta actualizado, pero tu PC no. Ejecuta:\n\n```bash\ngit checkout main\ngit pull origin main\n</code></pre></p> <p>Paso 5: Traer cambios a tu rama de trabajo</p> <pre><code># Ve a tu rama de ejercicio\ngit checkout tu-apellido-ejercicio\n\n# Trae los cambios de main\ngit merge main\n\n# Sube a GitHub\ngit push origin tu-apellido-ejercicio\n</code></pre> <p> Listo! Tienes los ejercicios nuevos sin perder tu trabajo.</p>"},{"location":"git-github/sincronizar-fork/#diagrama-visual-del-flujo","title":"Diagrama Visual del Flujo","text":""},{"location":"git-github/sincronizar-fork/#como-funciona-la-sincronizacion","title":"Como funciona la sincronizacion","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\nsequenceDiagram\n    actor T\u00fa\n    participant Local as \ud83d\udcbb Tu PC&lt;br/&gt;(main: 01, 02)\n    participant TuBranch as \ud83d\udcbb Tu PC&lt;br/&gt;(tu-rama: 01, 02 + TU C\u00d3DIGO)\n    participant Origin as \ud83c\udf10 Tu Fork GitHub&lt;br/&gt;(01, 02)\n    participant Upstream as \ud83d\udc68\u200d\ud83c\udfeb Repo Profesor&lt;br/&gt;(01, 02, 03, 04, 05)\n\n    Note over T\u00fa,Upstream: ESTADO INICIAL - Tu fork desactualizado\n\n    rect rgb(255, 243, 224)\n    Note over T\u00fa,Upstream: PASO 1: Cambiar a rama main\n    T\u00fa-&gt;&gt;Local: git checkout main\n    activate Local\n    Note over Local: Ahora est\u00e1s en main\n    end\n\n    rect rgb(232, 245, 233)\n    Note over T\u00fa,Upstream: PASO 2: Descargar y fusionar cambios del profesor\n    T\u00fa-&gt;&gt;Upstream: git fetch upstream\n    Upstream--&gt;&gt;Local: Descarga [03, 04, 05]\n    T\u00fa-&gt;&gt;Local: git merge upstream/main\n    Note over Local: main: 01, 02, 03, 04, 05 \u2705\n    deactivate Local\n    end\n\n    rect rgb(237, 231, 246)\n    Note over T\u00fa,Upstream: PASO 3: Cambiar a tu rama de trabajo\n    T\u00fa-&gt;&gt;TuBranch: git checkout tu-rama\n    activate TuBranch\n    Note over TuBranch: Ahora est\u00e1s en tu-rama\n    end\n\n    rect rgb(255, 249, 196)\n    Note over T\u00fa,Upstream: PASO 4: Traer cambios a tu rama\n    T\u00fa-&gt;&gt;TuBranch: git merge main\n    Note over TuBranch: tu-rama: 01-05 + TU C\u00d3DIGO \u2705\n    deactivate TuBranch\n    end\n\n    rect rgb(225, 245, 254)\n    Note over T\u00fa,Upstream: PASO 5: Subir todo a GitHub\n    T\u00fa-&gt;&gt;Origin: git push origin tu-rama\n    Note over Origin: tu-rama: 01-05 + TU C\u00d3DIGO \u2705\n    end\n\n    rect rgb(200, 230, 201)\n    Note over T\u00fa,Upstream: \u2705 RESULTADO - Tienes todo sin perder tu trabajo\n    end</code></pre>"},{"location":"git-github/sincronizar-fork/#vista-simplificada-del-proceso","title":"Vista Simplificada del Proceso","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#e1f5ff','primaryTextColor':'#000','primaryBorderColor':'#0277bd','secondaryColor':'#fff9c4','tertiaryColor':'#e8f5e9','noteBkgColor':'#fff3e0','noteTextColor':'#000'}}}%%\nflowchart TB\n    subgraph Antes[\"\u274c ANTES - Desactualizado\"]\n        direction LR\n        A1[\"\ud83d\udc68\u200d\ud83c\udfeb Repo Profesor&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Ejercicios:&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        A2[\"\ud83c\udf10 Tu Fork&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Tus ejercicios:&lt;br/&gt;[01] [02]&lt;br/&gt;&lt;br/&gt;\u26a0\ufe0f Te faltan 3 ejercicios\"]\n        A3[\"\ud83d\udcbb Tu PC&lt;br/&gt;&lt;br/&gt;\ud83d\udcc2 tu-rama:&lt;br/&gt;[01] [02] + TU C\u00d3DIGO\"]\n    end\n\n    subgraph Proceso[\"\ud83d\udd04 PROCESO DE SINCRONIZACI\u00d3N\"]\n        direction TB\n        P1[\"\u2460 git fetch upstream&lt;br/&gt;Descargar cambios del profesor\"]\n        P2[\"\u2461 git merge upstream/main&lt;br/&gt;Aplicar a tu main local\"]\n        P3[\"\u2462 git merge main&lt;br/&gt;Traer a tu rama de trabajo\"]\n        P4[\"\u2463 git push origin tu-rama&lt;br/&gt;Subir todo a GitHub\"]\n\n        P1 --&gt; P2 --&gt; P3 --&gt; P4\n    end\n\n    subgraph Despues[\"\u2705 DESPU\u00c9S - Actualizado\"]\n        direction LR\n        D1[\"\ud83d\udc68\u200d\ud83c\udfeb Repo Profesor&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Ejercicios:&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        D2[\"\ud83c\udf10 Tu Fork&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Tus ejercicios:&lt;br/&gt;[01-05] + TU C\u00d3DIGO&lt;br/&gt;&lt;br/&gt;\u2705 Completamente actualizado\"]\n        D3[\"\ud83d\udcbb Tu PC&lt;br/&gt;&lt;br/&gt;\ud83d\udcc2 tu-rama:&lt;br/&gt;[01-05] + TU C\u00d3DIGO&lt;br/&gt;&lt;br/&gt;\ud83c\udfaf Listo para trabajar\"]\n    end\n\n    Antes --&gt; Proceso --&gt; Despues\n\n    style A1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style A2 fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style A3 fill:#fff3e0,stroke:#ef6c00,stroke-width:2px\n\n    style P1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style P2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P3 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style P4 fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n\n    style D1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style D2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style D3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px</code></pre>"},{"location":"git-github/sincronizar-fork/#metodo-detallado-terminal","title":"Metodo Detallado (Terminal)","text":""},{"location":"git-github/sincronizar-fork/#situacion","title":"Situacion","text":"<p>Trabajas en una rama (ejemplo: <code>garcia-ejercicio-1.1</code>) y el profesor agrego ejercicios nuevos.</p> <p>Objetivo: Traer los ejercicios nuevos SIN perder tu trabajo.</p>"},{"location":"git-github/sincronizar-fork/#paso-1-guarda-tu-trabajo-actual","title":"PASO 1: Guarda tu trabajo actual","text":"<pre><code># Ver que archivos cambiaste\ngit status\n\n# Guardar tus cambios\ngit add entregas/01_bases_de_datos/tu_apellido_nombre/\ngit commit -m \"Guardar mi avance\"\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-2-ve-a-tu-rama-main","title":"PASO 2: Ve a tu rama main","text":"<pre><code>git checkout main\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-3-descarga-los-cambios-del-profesor","title":"PASO 3: Descarga los cambios del profesor","text":"<pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre> <p>Ahora tu <code>main</code> local tiene los ejercicios nuevos </p>"},{"location":"git-github/sincronizar-fork/#paso-4-vuelve-a-tu-rama-de-trabajo","title":"PASO 4: Vuelve a tu rama de trabajo","text":"<pre><code>git checkout garcia-ejercicio-1.1\n</code></pre> <p>(Reemplaza <code>garcia-ejercicio-1.1</code> por el nombre de TU rama)</p>"},{"location":"git-github/sincronizar-fork/#paso-5-trae-los-ejercicios-nuevos-a-tu-rama","title":"PASO 5: Trae los ejercicios nuevos a tu rama","text":"<pre><code>git merge main\n</code></pre> <p>Que hace esto?</p> <p>Combina los ejercicios nuevos del profesor con tu trabajo. NO borra nada tuyo.</p>"},{"location":"git-github/sincronizar-fork/#paso-6-sube-a-github","title":"PASO 6: Sube a GitHub","text":"<pre><code>git push origin garcia-ejercicio-1.1\n</code></pre> <p> Listo! Tienes los ejercicios nuevos Y tu trabajo intacto.</p>"},{"location":"git-github/sincronizar-fork/#que-pasa-cuando-el-profesor-agrega-ejercicios","title":"Que Pasa Cuando el Profesor Agrega Ejercicios?","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'git0':'#e1f5ff','git1':'#fff9c4','git2':'#ffebee'}}}%%\ngitGraph\n    commit id: \"01: Intro SQLite\" tag: \"Semana 1\"\n    commit id: \"02: Limpieza Datos\"\n    branch tu-fork\n    checkout tu-fork\n    commit id: \"\u2705 Hiciste Fork\" type: HIGHLIGHT\n\n    checkout main\n    commit id: \"03: Dask &amp; Parquet\" tag: \"Semana 3\"\n    commit id: \"04: PySpark\"\n    commit id: \"05: Dashboard\"\n\n    checkout tu-fork\n    commit id: \"\u274c Desactualizado\" type: REVERSE\n    commit id: \"\u26a0\ufe0f Te faltan 03, 04, 05\" type: REVERSE</code></pre> <p>El fork NO se actualiza autom\u00e1ticamente</p> <p>Cuando el profesor agrega ejercicios nuevos al repositorio original, tu fork en GitHub NO recibe esos cambios autom\u00e1ticamente. Debes sincronizarlo manualmente siguiendo los pasos de esta gu\u00eda.</p>"},{"location":"git-github/sincronizar-fork/#regla-de-oro-para-evitar-problemas","title":"Regla de Oro para Evitar Problemas","text":"<pre><code>%%{init: {'theme':'base'}}%%\nflowchart LR\n    subgraph Bien[\"\u2705 BIEN - Edita solo aqu\u00ed\"]\n        direction TB\n        B1[\"\ud83d\udcc1 entregas/01_bases_de_datos/tu_apellido_nombre/&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 1.1_sqlite/&lt;br/&gt;\u2502   \u251c\u2500\u2500 ANALISIS_DATOS.md&lt;br/&gt;\u2502   \u251c\u2500\u2500 resumen_eda.md&lt;br/&gt;\u2502   \u2514\u2500\u2500 REFLEXION.md&lt;br/&gt;&lt;br/&gt;\u2705 Aqu\u00ed haces tus cambios\"]\n    end\n\n    subgraph Mal[\"\u274c MAL - NO toques esto\"]\n        direction TB\n        M1[\"\ud83d\udcc1 ejercicios/01_bases_de_datos/&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 README.md \u2190 NO TOCAR&lt;br/&gt;\u251c\u2500\u2500 eda_exploratorio.py \u2190 Solo ejecutar&lt;br/&gt;&lt;br/&gt;\ud83d\udd12 Archivos del profesor\"]\n    end\n\n    Bien -.-&gt;|Sin conflictos| OK[\"\ud83c\udf89 Sincronizaci\u00f3n&lt;br/&gt;perfecta\"]\n    Mal -.-&gt;|Causa conflictos| NOK[\"\u26a0\ufe0f Problemas&lt;br/&gt;al sincronizar\"]\n\n    style Bien fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style Mal fill:#ffcdd2,stroke:#c62828,stroke-width:3px\n    style B1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style M1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px\n    style OK fill:#a5d6a7,stroke:#43a047,stroke-width:2px\n    style NOK fill:#ef9a9a,stroke:#e53935,stroke-width:2px</code></pre> <p>Regla de Oro</p> <p>Si solo editas archivos en <code>entregas/TU_CARPETA/</code>, NUNCA tendr\u00e1s conflictos.</p> <p>El profesor actualiza <code>ejercicios/</code>, t\u00fa trabajas en <code>entregas/</code>. Cero problemas.</p>"},{"location":"git-github/sincronizar-fork/#que-hago-si-git-dice-conflict","title":"Que hago si Git dice \"CONFLICT\"?","text":""},{"location":"git-github/sincronizar-fork/#paso-1-git-te-dira-que-archivo-tiene-conflicto","title":"Paso 1: Git te dira que archivo tiene conflicto","text":"<pre><code>Auto-merging ejercicio_01.py\nCONFLICT (content): Merge conflict in ejercicio_01.py\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-2-abre-el-archivo","title":"Paso 2: Abre el archivo","text":"<p>Veras algo asi:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ntu codigo aqui\n=======\ncodigo del profesor\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-3-decide-que-mantener","title":"Paso 3: Decide que mantener","text":"<ul> <li>Si es un archivo del profesor que NO deberias tocar \u2192 Mant\u00e9n la version del profesor</li> <li>Si es TU archivo de entrega \u2192 Mant\u00e9n tu version</li> </ul>"},{"location":"git-github/sincronizar-fork/#paso-4-borra-las-marcas","title":"Paso 4: Borra las marcas","text":"<p>Elimina estas lineas:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n</code></pre>"},{"location":"git-github/sincronizar-fork/#paso-5-termina-el-merge","title":"Paso 5: Termina el merge","text":"<pre><code>git add nombre-del-archivo\ngit commit -m \"Resolver conflicto\"\ngit push origin tu-rama\n</code></pre> <p>Consejo</p> <p>Si trabajas solo en <code>entregas/TU_CARPETA/</code>, esto nunca te pasara.</p>"},{"location":"git-github/sincronizar-fork/#resumen-ultra-rapido","title":"Resumen Ultra-Rapido","text":"<pre><code># 1. Guardar tu trabajo\ngit add .\ngit commit -m \"Guardar avance\"\n\n# 2. Actualizar main\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\n\n# 3. Volver a tu rama y traer cambios\ngit checkout tu-rama\ngit merge main\n\n# 4. Subir\ngit push origin tu-rama\n</code></pre> <p>Frecuencia: Haz esto cada lunes antes de clase.</p>"},{"location":"git-github/sincronizar-fork/#buenas-practicas-de-sincronizacion","title":"Buenas Practicas de Sincronizacion","text":""},{"location":"git-github/sincronizar-fork/#1-sincroniza-antes-de-empezar-un-ejercicio-nuevo","title":"1. Sincroniza ANTES de empezar un ejercicio nuevo","text":"<pre><code># \u2705 BIEN - Sincronizar primero\ngit fetch upstream &amp;&amp; git merge upstream/main\n# Ahora empieza a trabajar\n\n# \u274c MAL - Trabajar con codigo viejo\n# Empiezas sin actualizar, luego tienes conflictos\n</code></pre>"},{"location":"git-github/sincronizar-fork/#2-haz-un-commit-de-tu-trabajo-antes-de-sincronizar","title":"2. Haz un commit de tu trabajo ANTES de sincronizar","text":"<pre><code># \u2705 BIEN - Guarda tu trabajo primero\ngit add .\ngit commit -m \"Avance en ejercicio 03\"\ngit fetch upstream &amp;&amp; git merge upstream/main\n\n# \u274c MAL - Sincronizar con cambios sin guardar\n# Puedes perder tu trabajo\n</code></pre>"},{"location":"git-github/sincronizar-fork/#3-frecuencia-recomendada","title":"3. Frecuencia recomendada","text":"<pre><code>%%{init: {'theme':'base'}}%%\ngantt\n    title \ud83d\udcc5 Calendario de Sincronizaci\u00f3n Semanal\n    dateFormat YYYY-MM-DD\n    section Lunes\n    Sincronizar antes de clase :milestone, m1, 2024-01-01, 0d\n    git fetch upstream :active, 2024-01-01, 1h\n    git merge upstream/main :active, 2024-01-01, 30m\n    section Martes a Jueves\n    Trabajar en ejercicios :2024-01-02, 3d\n    Commits locales :2024-01-02, 3d\n    section Viernes\n    Push de tu avance :milestone, m2, 2024-01-05, 0d\n    git push origin tu-rama :crit, 2024-01-05, 1h\n    section Domingo\n    Verificar actualizaciones (opcional) :done, 2024-01-07, 30m</code></pre> <p>Recomendaci\u00f3n de frecuencia</p> <ul> <li>Lunes: Sincroniza antes de clase para tener los \u00faltimos ejercicios</li> <li>Durante la semana: Trabaja normalmente, haz commits frecuentes</li> <li>Viernes: Sube tu avance a GitHub</li> <li>Domingo (opcional): Verifica si hay actualizaciones nuevas</li> </ul>"},{"location":"git-github/sincronizar-fork/#verificar-estado-de-sincronizacion","title":"Verificar Estado de Sincronizacion","text":""},{"location":"git-github/sincronizar-fork/#comando-util-para-saber-si-estas-desactualizado","title":"Comando util para saber si estas desactualizado","text":"<pre><code># Ver diferencias entre tu fork y el repo del profesor\ngit fetch upstream\ngit log HEAD..upstream/main --oneline\n</code></pre> <p>Si ves commits nuevos:</p> <pre><code>a1b2c3d Agregar ejercicio 06\nd4e5f6g Corregir typo en ejercicio 05\ng7h8i9j Agregar datos para ejercicio 06\n</code></pre> <p>Significa que tienes 3 commits (ejercicios/actualizaciones) que no tienes.</p> <p>Si no ves nada:</p> <pre><code>(vacio)\n</code></pre> <p>Significa que estas actualizado. </p>"},{"location":"git-github/sincronizar-fork/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que sabes como sincronizar tu fork:</p> <ul> <li>Guia de Entregas - Como entregar ejercicios</li> <li>Comandos Utiles - Cheatsheet de Git</li> <li>Fork y Clone - Si necesitas repasar los conceptos basicos</li> </ul>"},{"location":"guia-inicio/","title":"Guia de Inicio","text":"<p>Bienvenido a la guia de inicio del curso de Big Data con Python. Esta seccion te guiara a traves de todo lo necesario para comenzar a trabajar con los ejercicios.</p>"},{"location":"guia-inicio/#que-encontraras-aqui","title":"Que encontraras aqui?","text":""},{"location":"guia-inicio/#instalacion-de-herramientas","title":"Instalacion de Herramientas","text":"<p>Aprende a instalar todas las herramientas necesarias para el curso:</p> <ul> <li>Python 3.11+</li> <li>Git</li> <li>PyCharm (IDE recomendado)</li> <li>Dependencias del proyecto</li> </ul>"},{"location":"guia-inicio/#tu-primer-ejercicio","title":"Tu Primer Ejercicio","text":"<p>Una guia paso a paso para completar tu primer ejercicio:</p> <ul> <li>Como navegar el repositorio</li> <li>Como ejecutar los ejercicios</li> <li>Como entregar tu trabajo</li> <li>Mejores practicas</li> </ul>"},{"location":"guia-inicio/#roadmap-del-curso","title":"Roadmap del Curso","text":"<p>Vision general de todos los ejercicios y tecnologias:</p> <ul> <li>Niveles de aprendizaje</li> <li>Tiempo estimado por ejercicio</li> <li>Tecnologias que dominaras</li> <li>Plan de estudio recomendado</li> </ul>"},{"location":"guia-inicio/#por-donde-empezar","title":"Por donde empezar?","text":"<p>Primera vez con Git y Python?</p> <p>Empieza con la Instalacion de Herramientas donde te explicamos paso a paso como instalar todo lo necesario.</p> <p>Ya tienes todo instalado?</p> <p>Ve directo a Tu Primer Ejercicio para comenzar a trabajar.</p> <p>Desarrollador experimentado?</p> <p>Revisa el Roadmap del Curso para ver todos los ejercicios y elegir por donde empezar.</p>"},{"location":"guia-inicio/#ayuda-y-soporte","title":"Ayuda y Soporte","text":"<p>Tienes dudas?</p> <ul> <li>Alumnos del curso presencial: Consulta en las sesiones presenciales</li> <li>Autodidactas: Crea un Issue en GitHub con tu pregunta</li> <li>Empresas: Contacta directamente via email</li> </ul>"},{"location":"guia-inicio/instalacion/","title":"Instalacion de Herramientas","text":"<p>Esta guia te llevara paso a paso por la instalacion de todas las herramientas necesarias para el curso.</p>"},{"location":"guia-inicio/instalacion/#requisitos-del-sistema","title":"Requisitos del Sistema","text":"<p>Requisitos Minimos</p> <ul> <li>RAM: 8GB</li> <li>Espacio en disco: 20GB</li> <li>Procesador: i5 o equivalente</li> <li>Sistema Operativo: Windows 10+, macOS 10.14+, o Linux (Ubuntu 20.04+)</li> </ul> <p>Requisitos Recomendados</p> <ul> <li>RAM: 16GB</li> <li>Espacio en disco: 50GB SSD</li> <li>Procesador: i7 o equivalente</li> <li>Sistema Operativo: Ultimo sistema operativo disponible</li> </ul>"},{"location":"guia-inicio/instalacion/#paso-1-instalar-git","title":"Paso 1: Instalar Git","text":"<p>Git es el sistema de control de versiones que usaremos para gestionar el codigo.</p> WindowsmacOSLinux"},{"location":"guia-inicio/instalacion/#opcion-a-con-winget-recomendado","title":"Opcion A: Con winget (Recomendado)","text":"<pre><code># Abrir PowerShell o CMD como Administrador\nwinget install Git.Git\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-b-instalador-grafico","title":"Opcion B: Instalador grafico","text":"<ol> <li>Descargar desde git-scm.com</li> <li>Ejecutar el instalador</li> <li>Usar configuracion por defecto (Next, Next, Next...)</li> </ol>"},{"location":"guia-inicio/instalacion/#verificar-instalacion","title":"Verificar instalacion","text":"<pre><code>git --version\n# Debe mostrar: git version 2.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-a-con-homebrew-recomendado","title":"Opcion A: Con Homebrew (Recomendado)","text":"<pre><code># Si no tienes Homebrew, instalalo primero:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Instalar Git\nbrew install git\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-b-con-xcode-command-line-tools","title":"Opcion B: Con Xcode Command Line Tools","text":"<pre><code>xcode-select --install\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_1","title":"Verificar instalacion","text":"<pre><code>git --version\n# Debe mostrar: git version 2.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install git\n</code></pre>"},{"location":"guia-inicio/instalacion/#fedora","title":"Fedora","text":"<pre><code>sudo dnf install git\n</code></pre>"},{"location":"guia-inicio/instalacion/#arch-linux","title":"Arch Linux","text":"<pre><code>sudo pacman -S git\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_2","title":"Verificar instalacion","text":"<pre><code>git --version\n# Debe mostrar: git version 2.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#paso-2-configurar-git","title":"Paso 2: Configurar Git","text":"<p>Una vez instalado Git, debemos configurarlo con tu informacion:</p> <pre><code># Configurar nombre (usa tu nombre real)\ngit config --global user.name \"Tu Nombre Completo\"\n\n# Configurar email (usa el mismo email de GitHub)\ngit config --global user.email \"tu@email.com\"\n\n# Verificar configuracion\ngit config --list\n</code></pre> <p>Consejo</p> <p>El nombre y email que configures aparecera en todos tus commits, asi que usa tu nombre real y el email que usaras en GitHub.</p>"},{"location":"guia-inicio/instalacion/#paso-3-crear-cuenta-en-github","title":"Paso 3: Crear Cuenta en GitHub","text":"<p>GitHub es la plataforma donde alojaremos el codigo.</p> <ol> <li>Ve a github.com</li> <li>Click en \"Sign Up\"</li> <li>Completa el formulario:<ul> <li>Username: Elige un nombre profesional (ej: <code>juan-garcia</code>, no <code>gatito123</code>)</li> <li>Email: Usa el mismo que configuraste en Git</li> <li>Password: Usa un password seguro</li> </ul> </li> <li>Verifica tu email</li> <li>Completa tu perfil (foto, bio opcional)</li> </ol> <p>Importante</p> <p>Usa el mismo email que configuraste en Git. Esto vincula tus commits con tu cuenta de GitHub.</p>"},{"location":"guia-inicio/instalacion/#paso-4-instalar-python","title":"Paso 4: Instalar Python","text":"<p>Necesitamos Python 3.11 o superior.</p> WindowsmacOSLinux <p>Nota para macOS/Linux</p> <p>En macOS y Linux, usa <code>python3</code> y <code>pip3</code> en lugar de <code>python</code> y <code>pip</code>.</p>"},{"location":"guia-inicio/instalacion/#opcion-a-con-winget-recomendado_1","title":"Opcion A: Con winget (Recomendado)","text":"<pre><code>winget install Python.Python.3.11\n</code></pre>"},{"location":"guia-inicio/instalacion/#opcion-b-instalador-grafico_1","title":"Opcion B: Instalador grafico","text":"<ol> <li>Descargar desde python.org</li> <li>IMPORTANTE: Marcar \"Add Python to PATH\"</li> <li>Ejecutar instalador</li> <li>Click en \"Install Now\"</li> </ol>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_3","title":"Verificar instalacion","text":"<pre><code>python --version\n# Debe mostrar: Python 3.11.x\n\npip --version\n# Debe mostrar: pip 23.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#con-homebrew-recomendado","title":"Con Homebrew (Recomendado)","text":"<pre><code>brew install python@3.11\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_4","title":"Verificar instalacion","text":"<pre><code>python3 --version\n# Debe mostrar: Python 3.11.x\n\npip3 --version\n# Debe mostrar: pip 23.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#ubuntudebian_1","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install python3.11 python3-pip\n</code></pre>"},{"location":"guia-inicio/instalacion/#fedora_1","title":"Fedora","text":"<pre><code>sudo dnf install python3.11 python3-pip\n</code></pre>"},{"location":"guia-inicio/instalacion/#verificar-instalacion_5","title":"Verificar instalacion","text":"<pre><code>python3 --version\n# Debe mostrar: Python 3.11.x\n\npip3 --version\n# Debe mostrar: pip 23.x.x\n</code></pre>"},{"location":"guia-inicio/instalacion/#paso-5-instalar-pycharm-opcional-pero-recomendado","title":"Paso 5: Instalar PyCharm (Opcional pero Recomendado)","text":"<p>PyCharm es el IDE que recomendamos para el curso.</p>"},{"location":"guia-inicio/instalacion/#pycharm-community-edition-gratis","title":"PyCharm Community Edition (Gratis)","text":"WindowsmacOSLinux <ol> <li>Descargar desde jetbrains.com/pycharm</li> <li>Elegir \"Community Edition\" (gratis)</li> <li>Ejecutar instalador</li> <li>Seguir pasos del instalador</li> </ol> <pre><code>brew install --cask pycharm-ce\n</code></pre> <p>O descarga desde jetbrains.com/pycharm</p> <p>Descarga desde jetbrains.com/pycharm</p> <p>O usa Snap:</p> <pre><code>sudo snap install pycharm-community --classic\n</code></pre>"},{"location":"guia-inicio/instalacion/#alternativas-a-pycharm","title":"Alternativas a PyCharm","text":"<p>Si prefieres otro editor:</p> <ul> <li>Visual Studio Code: Ligero y extensible (code.visualstudio.com)</li> <li>Jupyter Lab: Para trabajar con notebooks (jupyter.org)</li> <li>Sublime Text: Editor de texto avanzado (sublimetext.com)</li> </ul>"},{"location":"guia-inicio/instalacion/#paso-6-clonar-el-repositorio","title":"Paso 6: Clonar el Repositorio","text":"<p>Ahora que tienes todo instalado, clona tu fork del repositorio:</p> <p>Importante</p> <p>Primero debes hacer Fork del repositorio en GitHub. Ve a la guia de Fork y Clone para mas detalles.</p> <pre><code># Navega a la carpeta donde quieres guardar el proyecto\ncd Documents  # o la carpeta que prefieras\n\n# Clona TU fork (reemplaza TU_USUARIO)\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Entra a la carpeta\ncd ejercicios-bigdata\n\n# Conecta con el repositorio original (upstream)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Verifica que todo este bien\ngit remote -v\n</code></pre> <p>Deberas ver algo asi:</p> <pre><code>origin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (fetch)\norigin    https://github.com/TU_USUARIO/ejercicios-bigdata.git (push)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (fetch)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (push)\n</code></pre>"},{"location":"guia-inicio/instalacion/#paso-7-crear-entorno-virtual","title":"Paso 7: Crear Entorno Virtual","text":"<p>Es una buena practica usar entornos virtuales para cada proyecto:</p> <pre><code># Asegurate de estar en la carpeta del proyecto\ncd ejercicios-bigdata\n\n# Crear entorno virtual\npython -m venv .venv\n\n# Activar entorno virtual\n# Windows:\n.venv\\Scripts\\activate\n\n# macOS/Linux:\nsource .venv/bin/activate\n\n# Deberas ver (.venv) al inicio de tu terminal\n</code></pre> <p>Consejo</p> <p>Siempre activa el entorno virtual antes de trabajar en el proyecto.</p>"},{"location":"guia-inicio/instalacion/#paso-8-instalar-dependencias","title":"Paso 8: Instalar Dependencias","text":"<p>Con el entorno virtual activado, instala las dependencias del proyecto:</p> <pre><code># Actualizar pip\npip install --upgrade pip\n\n# Instalar dependencias del proyecto\npip install -r requirements.txt\n\n# Verificar que todo se instalo correctamente\npython -c \"import pandas, dask, sqlite3; print('Todo OK!')\"\n</code></pre> <p>Si ves \"Todo OK!\", estas listo para empezar.</p>"},{"location":"guia-inicio/instalacion/#verificacion-final","title":"Verificacion Final","text":"<p>Ejecuta estos comandos para verificar que todo esta instalado correctamente:</p> <pre><code># Git\ngit --version\n\n# Python\npython --version\n\n# Pip\npip --version\n\n# Verificar librerias\npython -c \"import pandas; print(f'Pandas {pandas.__version__}')\"\npython -c \"import dask; print(f'Dask {dask.__version__}')\"\n</code></pre> <p>Instalacion Completa</p> <p>Si todos los comandos anteriores funcionaron, estas listo para empezar con Tu Primer Ejercicio!</p>"},{"location":"guia-inicio/instalacion/#problemas-comunes","title":"Problemas Comunes","text":"Error: 'python' no se reconoce como comando <p>Windows: Python no esta en el PATH.</p> <p>Solucion:</p> <ol> <li>Reinstala Python</li> <li>Marca la opcion \"Add Python to PATH\"</li> <li>Reinicia la terminal</li> </ol> <p>macOS/Linux: Usa <code>python3</code> en lugar de <code>python</code></p> Error: Permission denied al instalar con pip <p>Causa: Intentando instalar paquetes globalmente sin permisos.</p> <p>Solucion: Usa un entorno virtual:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\npip install -r requirements.txt\n</code></pre> Git dice 'fatal: not a git repository' <p>Causa: No estas en la carpeta del proyecto.</p> <p>Solucion:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica que estas en la carpeta correcta\nls -la  # Deberas ver una carpeta .git/\n</code></pre> PyCharm no detecta el interprete de Python <p>Solucion:</p> <ol> <li>Abre PyCharm</li> <li>File \u2192 Settings (Windows/Linux) o PyCharm \u2192 Preferences (macOS)</li> <li>Project \u2192 Python Interpreter</li> <li>Click en el icono de engranaje \u2192 Add</li> <li>Selecciona \"Existing environment\"</li> <li>Busca <code>.venv/Scripts/python.exe</code> (Windows) o <code>.venv/bin/python</code> (macOS/Linux)</li> </ol>"},{"location":"guia-inicio/instalacion/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que tienes todo instalado, continua con:</p> <ul> <li>Tu Primer Ejercicio - Aprende el flujo de trabajo basico</li> <li>Fork y Clone - Entiende como trabajar con Git y GitHub</li> <li>Roadmap del Curso - Ve todos los ejercicios disponibles</li> </ul>"},{"location":"guia-inicio/primer-ejercicio/","title":"Tu Primer Ejercicio","text":"<p>Esta guia te llevara paso a paso por el proceso de completar y entregar tu primer ejercicio.</p>"},{"location":"guia-inicio/primer-ejercicio/#flujo-de-trabajo-general","title":"Flujo de Trabajo General","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Clone a tu PC]\n    B --&gt; C[Trabajar en Ejercicio]\n    C --&gt; D[Documentar PROMPTS.md]\n    D --&gt; E[Commit Cambios]\n    E --&gt; F[Push a tu Fork]\n    F --&gt; G[Evaluacion Automatica]</code></pre> <p>Sistema de Evaluacion por PROMPTS</p> <p>NO se usan Pull Requests. El sistema evalua tu archivo <code>PROMPTS.md</code> directamente en tu fork. Solo necesitas hacer <code>git push</code>.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-1-abrir-el-proyecto-en-pycharm","title":"Paso 1: Abrir el Proyecto en PyCharm","text":"<ol> <li>Abre PyCharm</li> <li>File \u2192 Open...</li> <li>Selecciona la carpeta <code>ejercicios-bigdata/</code></li> <li>Click en \"OK\"</li> </ol> <p>Primera vez en PyCharm?</p> <p>PyCharm te preguntara si confias en el proyecto. Click en \"Trust Project\".</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-2-configurar-el-interprete-de-python","title":"Paso 2: Configurar el Interprete de Python","text":"<p>PyCharm debe detectar automaticamente el entorno virtual. Si no lo hace:</p> <ol> <li>File \u2192 Settings (Windows/Linux) o PyCharm \u2192 Preferences (macOS)</li> <li>Project: ejercicios-bigdata \u2192 Python Interpreter</li> <li>Click en el icono de engranaje \u2192 Add</li> <li>Selecciona \"Existing environment\"</li> <li>Busca <code>.venv/Scripts/python.exe</code> (Windows) o <code>.venv/bin/python</code> (macOS/Linux)</li> <li>Click \"OK\"</li> </ol>"},{"location":"guia-inicio/primer-ejercicio/#paso-3-navegar-a-tu-primer-ejercicio","title":"Paso 3: Navegar a Tu Primer Ejercicio","text":"<p>En el explorador de archivos de PyCharm:</p> <pre><code>ejercicios-bigdata/\n\u2514\u2500\u2500 ejercicios/\n    \u2514\u2500\u2500 01_cargar_sqlite.py  \u2190 Abre este archivo\n</code></pre> <p>Estructura del Ejercicio</p> <p>Cada ejercicio tiene:</p> <ul> <li>Codigo base: Archivo <code>.py</code> con instrucciones</li> <li>Datos: Carpeta <code>datos/</code> con los datasets</li> <li>README: Explicacion detallada del ejercicio</li> </ul>"},{"location":"guia-inicio/primer-ejercicio/#paso-4-leer-el-enunciado","title":"Paso 4: Leer el Enunciado","text":"<p>IMPORTANTE: Lee TODO el archivo antes de empezar a codear.</p> <p>El ejercicio tendra secciones como:</p> <pre><code>\"\"\"\nEjercicio 01: Carga de Datos con SQLite\n\nOBJETIVO:\nAprender a cargar datos desde CSV a una base de datos SQLite\n\nDATASET:\n- Archivo: datos/muestra_taxi.csv\n- Tamano: ~10MB\n- Registros: ~100,000\n\nTAREAS:\n1. Cargar CSV en chunks a SQLite\n2. Crear indices para optimizar queries\n3. Ejecutar queries de analisis\n4. Exportar resultados\n\nTIEMPO ESTIMADO: 2-3 horas\n\"\"\"\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#paso-5-crear-una-rama-de-trabajo","title":"Paso 5: Crear una Rama de Trabajo","text":"<p>NUNCA trabajes directamente en <code>main</code>. Siempre crea una rama:</p> <pre><code># Asegurate de estar en main y actualizado\ngit checkout main\ngit pull origin main\n\n# Crea una rama con tu apellido y el numero de ejercicio\ngit checkout -b garcia-ejercicio-01\n\n# Verifica que estas en la rama correcta\ngit branch\n# Debera mostrar: * garcia-ejercicio-01\n</code></pre> <p>Nomenclatura de Ramas</p> <p>Usa el formato: <code>tu-apellido-ejercicio-XX</code></p> <p>Ejemplos: - <code>garcia-ejercicio-01</code> - <code>martinez-ejercicio-02</code></p>"},{"location":"guia-inicio/primer-ejercicio/#paso-6-trabajar-en-el-ejercicio","title":"Paso 6: Trabajar en el Ejercicio","text":""},{"location":"guia-inicio/primer-ejercicio/#editar-el-codigo","title":"Editar el Codigo","text":"<p>Abre <code>ejercicios/01_cargar_sqlite.py</code> y empieza a trabajar.</p> <p>Ejemplo de Codigo</p> <pre><code>import sqlite3\nimport pandas as pd\n\n# Tarea 1: Cargar CSV en chunks\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"\n    Carga un CSV grande a SQLite en chunks para evitar problemas de memoria\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # Leer CSV por partes\n    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n\n    for i, chunk in enumerate(chunks):\n        chunk.to_sql('trips', conn, if_exists='append', index=False)\n        print(f\"Chunk {i+1} cargado ({len(chunk)} registros)\")\n\n    conn.close()\n    print(\"Carga completa!\")\n\n# Ejecutar\nif __name__ == \"__main__\":\n    cargar_datos_sqlite(\n        csv_path='datos/muestra_taxi.csv',\n        db_path='datos/taxi.db'\n    )\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#probar-tu-codigo","title":"Probar tu Codigo","text":"<p>Ejecuta tu codigo frecuentemente para verificar que funciona:</p> Desde PyCharmDesde Terminal <ol> <li>Click derecho en el archivo</li> <li>Run 'ejercicio_01'</li> <li>O presiona <code>Shift + F10</code></li> </ol> <pre><code># Asegurate de tener el entorno virtual activado\npython ejercicios/01_cargar_sqlite.py\n</code></pre> <p>Debug Frecuente</p> <p>No escribas todo el codigo de una vez. Escribe una funcion, pruebalas, y continua.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-7-guardar-tu-trabajo-con-git","title":"Paso 7: Guardar tu Trabajo con Git","text":"<p>Cuando tengas un avance significativo (por ejemplo, completaste una tarea):</p> <pre><code># Ver que archivos cambiaste\ngit status\n\n# Agregar los archivos modificados\ngit add ejercicios/01_cargar_sqlite.py\n\n# Hacer commit con un mensaje descriptivo\ngit commit -m \"Implementar carga de CSV a SQLite en chunks\"\n\n# Continua trabajando...\n</code></pre> <p>Buenos Mensajes de Commit</p> <p>\u2705 BIEN: - \"Implementar carga de CSV a SQLite en chunks\" - \"Agregar indices para optimizar queries\" - \"Completar analisis de ingresos por hora\"</p> <p>\u274c MAL: - \"update\" - \"fix\" - \"asdfasdf\"</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-8-subir-a-github","title":"Paso 8: Subir a GitHub","text":"<p>Cuando hayas completado el ejercicio:</p> <pre><code># Hacer un commit final\ngit add .\ngit commit -m \"Completar ejercicio 01: carga de datos SQLite\"\n\n# Subir tu rama a GitHub\ngit push origin garcia-ejercicio-01\n</code></pre> <p>Primera vez haciendo push?</p> <p>Git te pedira autenticacion. Usa tu usuario y password de GitHub, o configura SSH keys.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-9-verificar-tu-entrega","title":"Paso 9: Verificar tu Entrega","text":"<ol> <li>Ve a tu fork en GitHub: <code>https://github.com/TU_USUARIO/ejercicios-bigdata</code></li> <li>Navega a tu carpeta de entrega</li> <li>Verifica que estan todos tus archivos, especialmente <code>PROMPTS.md</code></li> </ol> <p>Entrega Completada</p> <p>No necesitas hacer nada mas. El sistema evalua tu <code>PROMPTS.md</code> automaticamente.</p>"},{"location":"guia-inicio/primer-ejercicio/#paso-10-el-archivo-promptsmd","title":"Paso 10: El Archivo PROMPTS.md","text":"<p>Este es el archivo mas importante de tu entrega.</p> <p>Documenta tus prompts de IA mientras trabajas:</p> <pre><code># Prompts de IA - Ejercicio 01\n\n## Prompt A: Cargar datos a SQLite\n\n**IA usada:** ChatGPT / Claude / etc.\n\n**Prompt exacto:**\n&gt; como cargo un csv grande a sqlite usando python con chunks\n\n---\n\n## Prompt B: Optimizar queries\n\n[Mismo formato...]\n\n---\n\n## Blueprint Final\n\n[Al terminar, pide a la IA un resumen de lo que construiste]\n</code></pre> <p>NO limpies tus prompts</p> <p>Pega tus prompts TAL CUAL los escribiste, con errores y todo. El sistema detecta si fueron \"limpiados\".</p>"},{"location":"guia-inicio/primer-ejercicio/#mejores-practicas","title":"Mejores Practicas","text":""},{"location":"guia-inicio/primer-ejercicio/#codigo-limpio","title":"Codigo Limpio","text":"<pre><code># \u2705 BIEN - Codigo legible con comentarios\ndef calcular_promedio_tarifas(db_path):\n    \"\"\"\n    Calcula el promedio de tarifas por hora del dia\n\n    Args:\n        db_path: Ruta a la base de datos SQLite\n\n    Returns:\n        DataFrame con promedio de tarifas por hora\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    query = \"\"\"\n        SELECT\n            strftime('%H', pickup_datetime) as hora,\n            AVG(total_amount) as promedio_tarifa\n        FROM trips\n        GROUP BY hora\n        ORDER BY hora\n    \"\"\"\n\n    resultado = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return resultado\n\n# \u274c MAL - Sin documentacion, nombres confusos\ndef calc(p):\n    c = sqlite3.connect(p)\n    r = pd.read_sql_query(\"SELECT strftime('%H', pickup_datetime) as h, AVG(total_amount) as t FROM trips GROUP BY h\", c)\n    c.close()\n    return r\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#commits-atomicos","title":"Commits Atomicos","text":"<p>Haz commits pequenos y especificos:</p> <pre><code># \u2705 BIEN - Commits pequenos y descriptivos\ngit commit -m \"Agregar funcion de carga de datos\"\ngit commit -m \"Implementar creacion de indices\"\ngit commit -m \"Agregar queries de analisis\"\n\n# \u274c MAL - Un solo commit gigante\ngit commit -m \"Todo el ejercicio\"\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#probar-antes-de-subir","title":"Probar Antes de Subir","text":"<pre><code># Siempre verifica que funciona antes de push\npython ejercicios/01_cargar_sqlite.py\n\n# Si funciona, entonces push\ngit push origin garcia-ejercicio-01\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#checklist-del-ejercicio","title":"Checklist del Ejercicio","text":"<p>Antes de subir tu trabajo (git push), verifica:</p> <ul> <li> El codigo ejecuta sin errores</li> <li> Todas las tareas del ejercicio estan completas</li> <li> El codigo esta documentado (comentarios, docstrings)</li> <li> Los commits tienen mensajes descriptivos</li> <li> El codigo sigue las mejores practicas de Python</li> <li> Probaste con el dataset completo</li> </ul>"},{"location":"guia-inicio/primer-ejercicio/#problemas-comunes","title":"Problemas Comunes","text":"Error: ModuleNotFoundError: No module named 'pandas' <p>Causa: Entorno virtual no activado o dependencias no instaladas.</p> <p>Solucion:</p> <pre><code># Activar entorno virtual\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\n\n# Instalar dependencias\npip install -r requirements.txt\n</code></pre> Git dice: 'Your branch is behind origin/main' <p>Causa: Tu rama main local esta desactualizada.</p> <p>Solucion:</p> <pre><code>git checkout main\ngit pull origin main\ngit checkout garcia-ejercicio-01\ngit merge main\n</code></pre> No puedo hacer push: 'Permission denied' <p>Causa: Problemas de autenticacion con GitHub.</p> <p>Solucion: Configura SSH keys o usa Personal Access Token.</p> <p>Ver: GitHub Authentication</p> PyCharm no encuentra los datos <p>Causa: Ruta relativa incorrecta.</p> <p>Solucion: Usa rutas relativas desde la raiz del proyecto:</p> <pre><code># \u2705 BIEN\ncsv_path = 'datos/muestra_taxi.csv'\n\n# \u274c MAL\ncsv_path = '../datos/muestra_taxi.csv'\n</code></pre>"},{"location":"guia-inicio/primer-ejercicio/#proximos-pasos","title":"Proximos Pasos","text":"<p>Una vez completado tu primer ejercicio:</p> <ul> <li>Sincronizar Fork - Mantener tu fork actualizado</li> <li>Roadmap del Curso - Ver todos los ejercicios disponibles</li> <li>Comandos Utiles - Cheatsheet de Git</li> </ul>"},{"location":"guia-inicio/roadmap/","title":"Roadmap del Curso","text":"<p>Vision completa de todos los ejercicios, tecnologias y el plan de aprendizaje recomendado.</p>"},{"location":"guia-inicio/roadmap/#niveles-de-aprendizaje","title":"Niveles de Aprendizaje","text":"<pre><code>graph TD\n    A[NIVEL 1: Fundamentos&lt;br/&gt;2-3 semanas] --&gt; B[NIVEL 2: Escalando&lt;br/&gt;3-4 semanas]\n    B --&gt; C[NIVEL 3: Big Data Real&lt;br/&gt;4-5 semanas]\n    C --&gt; D[NIVEL 4: Visualizacion&lt;br/&gt;3-4 semanas]\n\n    A1[SQLite&lt;br/&gt;Pandas&lt;br/&gt;Git/GitHub] --&gt; A\n    B1[Dask&lt;br/&gt;Parquet&lt;br/&gt;Optimizacion] --&gt; B\n    C1[PySpark&lt;br/&gt;SQL Avanzado&lt;br/&gt;Pipelines] --&gt; C\n    D1[Dashboards&lt;br/&gt;APIs&lt;br/&gt;Deploy] --&gt; D</code></pre>"},{"location":"guia-inicio/roadmap/#nivel-1-fundamentos","title":"NIVEL 1: Fundamentos","text":"<p>Duracion: 2-3 semanas | Dificultad: \ud83d\udfe2 Basico</p>"},{"location":"guia-inicio/roadmap/#objetivos","title":"Objetivos","text":"<ul> <li>Dominar las bases de datos relacionales con SQLite</li> <li>Aprender analisis de datos con Pandas</li> <li>Entender control de versiones con Git/GitHub</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias","title":"Tecnologias","text":"Tecnologia Proposito Recursos SQLite Base de datos embebida Docs oficiales Pandas Analisis de datos en memoria Pandas docs Git Control de versiones Git handbook"},{"location":"guia-inicio/roadmap/#ejercicios","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-01-carga-de-datos-con-sqlite","title":"Ejercicio 01: Carga de Datos con SQLite","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 2-3 horas</li> <li>Dataset: NYC Taxi (10MB muestra)</li> <li>Nivel: \ud83d\udfe2 Basico</li> </ul> <p>Que aprenderas:</p> <ul> <li>Cargar datos desde CSV a base de datos</li> <li>Queries SQL basicas (SELECT, WHERE, GROUP BY)</li> <li>Optimizacion con indices</li> <li>Exportar resultados</li> </ul> <p>Habilidades:</p> <ul> <li> Cargar CSV en chunks</li> <li> Crear base de datos SQLite</li> <li> Ejecutar queries SQL</li> <li> Crear indices</li> <li> Exportar resultados a CSV</li> </ul>"},{"location":"guia-inicio/roadmap/#ejercicio-02-limpieza-y-transformacion","title":"Ejercicio 02: Limpieza y Transformacion","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 3-4 horas</li> <li>Dataset: NYC Taxi (datos sucios)</li> <li>Nivel: \ud83d\udfe2 Basico</li> </ul> <p>Que aprenderas:</p> <ul> <li>Detectar y manejar valores nulos</li> <li>Identificar outliers</li> <li>Transformaciones de datos</li> <li>Validacion de tipos</li> </ul>"},{"location":"guia-inicio/roadmap/#nivel-2-escalando","title":"NIVEL 2: Escalando","text":"<p>Duracion: 3-4 semanas | Dificultad: \ud83d\udfe1 Intermedio</p>"},{"location":"guia-inicio/roadmap/#objetivos_1","title":"Objetivos","text":"<ul> <li>Procesar datos mas grandes que tu RAM</li> <li>Entender procesamiento paralelo</li> <li>Optimizar rendimiento</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias_1","title":"Tecnologias","text":"Tecnologia Proposito Cuando Usarla Dask Procesamiento paralelo Datos &gt; RAM (5-100GB) Parquet Formato columnar Almacenamiento eficiente Optimizacion Performance Siempre"},{"location":"guia-inicio/roadmap/#ejercicios_1","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-03-procesamiento-con-parquet-y-dask","title":"Ejercicio 03: Procesamiento con Parquet y Dask","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 4-5 horas</li> <li>Dataset: NYC Taxi completo (121MB)</li> <li>Nivel: \ud83d\udfe1 Intermedio</li> </ul> <p>Que aprenderas:</p> <ul> <li>Por que Parquet es mejor que CSV</li> <li>Procesamiento paralelo con Dask</li> <li>Lazy evaluation</li> <li>Optimizacion de memoria</li> </ul> <p>Comparativa de Formatos:</p> Metrica CSV Parquet Tamano en disco 121 MB 45 MB Tiempo de lectura 8.5 seg 1.2 seg Compresion No Si Tipos de datos No preserva Si preserva"},{"location":"guia-inicio/roadmap/#nivel-3-big-data-real","title":"NIVEL 3: Big Data Real","text":"<p>Duracion: 4-5 semanas | Dificultad: \ud83d\udd34 Avanzado</p>"},{"location":"guia-inicio/roadmap/#objetivos_2","title":"Objetivos","text":"<ul> <li>Dominar procesamiento distribuido</li> <li>Trabajar con datos masivos (&gt;100GB)</li> <li>Construir pipelines de produccion</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias_2","title":"Tecnologias","text":"Tecnologia Proposito Escala PySpark Procesamiento distribuido &gt; 100GB SQL Avanzado Queries complejas Cualquier tamano Pipelines ETL Automatizacion Produccion"},{"location":"guia-inicio/roadmap/#ejercicios_2","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-04-queries-complejas-con-pyspark","title":"Ejercicio 04: Queries Complejas con PySpark","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 5-6 horas</li> <li>Dataset: NYC Taxi + Weather (multiple fuentes)</li> <li>Nivel: \ud83d\udd34 Avanzado</li> </ul> <p>Que aprenderas:</p> <ul> <li>Introduccion a Spark</li> <li>DataFrames distribuidos</li> <li>SQL en Spark</li> <li>Joins de multiples fuentes</li> <li>Particionamiento de datos</li> </ul>"},{"location":"guia-inicio/roadmap/#ejercicio-06-pipeline-etl-completo","title":"Ejercicio 06: Pipeline ETL Completo","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 10-12 horas</li> <li>Dataset: Multiples fuentes</li> <li>Nivel: \ud83d\udd34 Avanzado</li> </ul> <p>Arquitectura del Pipeline:</p> <pre><code>graph LR\n    A[CSV 100GB] --&gt;|Extract| B[Dask]\n    B --&gt;|Transform| C[PySpark]\n    C --&gt;|Load| D[Parquet 10GB]\n    D --&gt;|Serve| E[API Flask]\n    E --&gt;|Visualize| F[Dashboard]</code></pre>"},{"location":"guia-inicio/roadmap/#nivel-4-visualizacion-y-deploy","title":"NIVEL 4: Visualizacion y Deploy","text":"<p>Duracion: 3-4 semanas | Dificultad: \ud83d\udd34 Avanzado</p>"},{"location":"guia-inicio/roadmap/#objetivos_3","title":"Objetivos","text":"<ul> <li>Crear dashboards profesionales</li> <li>Servir datos via API</li> <li>Deploy a produccion</li> </ul>"},{"location":"guia-inicio/roadmap/#tecnologias_3","title":"Tecnologias","text":"Tecnologia Proposito Uso Flask Backend web APIs y dashboards Chart.js Visualizaciones Graficos interactivos Docker Contenedores Deploy"},{"location":"guia-inicio/roadmap/#ejercicios_3","title":"Ejercicios","text":""},{"location":"guia-inicio/roadmap/#ejercicio-05-dashboard-interactivo","title":"Ejercicio 05: Dashboard Interactivo","text":"<p>Detalles</p> <ul> <li>Tiempo estimado: 8-10 horas</li> <li>Proyecto: Dashboard EDA NYC Taxi</li> <li>Nivel: \ud83d\udd34 Avanzado</li> </ul> <p>Funcionalidades:</p> <ul> <li> Visualizacion de 10M+ registros</li> <li> Filtros dinamicos por fecha/hora</li> <li> Mapas de calor</li> <li> Analisis de tendencias</li> </ul> <p>Tech Stack:</p> <pre><code>Frontend: HTML + Bootstrap + Chart.js\nBackend:  Flask + Pandas/Dask\nData:     SQLite/Parquet\nDeploy:   Docker\n</code></pre>"},{"location":"guia-inicio/roadmap/#plan-de-estudio-recomendado","title":"Plan de Estudio Recomendado","text":""},{"location":"guia-inicio/roadmap/#para-principiantes-10-12-semanas","title":"Para Principiantes (10-12 semanas)","text":"<pre><code>gantt\n    title Plan de Estudio - Principiantes\n    dateFormat YYYY-MM-DD\n    section Fundamentos\n    Ejercicio 01           :2024-01-01, 1w\n    Ejercicio 02           :2024-01-08, 1w\n    Practica Fundamentos   :2024-01-15, 1w\n    section Escalando\n    Ejercicio 03           :2024-01-22, 2w\n    Proyecto Personal      :2024-02-05, 1w\n    section Big Data\n    Ejercicio 04           :2024-02-12, 2w\n    Ejercicio 06           :2024-02-26, 2w\n    section Visualizacion\n    Ejercicio 05           :2024-03-11, 2w</code></pre> <p>Dedicacion: 10-15 horas/semana</p>"},{"location":"guia-inicio/roadmap/#para-intermedios-6-8-semanas","title":"Para Intermedios (6-8 semanas)","text":"<p>Recomendacion</p> <p>Si ya conoces Python y Pandas, puedes empezar directamente en el NIVEL 2.</p> <p>Dedicacion: 8-10 horas/semana</p>"},{"location":"guia-inicio/roadmap/#para-avanzados-4-5-semanas","title":"Para Avanzados (4-5 semanas)","text":"<p>Recomendacion</p> <p>Si ya trabajaste con Big Data, enfocate en los ejercicios de PySpark y el proyecto final.</p> <p>Dedicacion: 5-8 horas/semana</p>"},{"location":"guia-inicio/roadmap/#tecnologias-por-ejercicio","title":"Tecnologias por Ejercicio","text":"Ejercicio SQLite Pandas Dask PySpark Flask Nivel 01 - SQLite \u2705 \u2705 - - - \ud83d\udfe2 02 - Limpieza - \u2705 - - - \ud83d\udfe2 03 - Dask - \u2705 \u2705 - - \ud83d\udfe1 04 - PySpark - - \u2705 \u2705 - \ud83d\udd34 05 - Dashboard \u2705 \u2705 - - \u2705 \ud83d\udd34 06 - Pipeline - - \u2705 \u2705 \u2705 \ud83d\udd34"},{"location":"guia-inicio/roadmap/#comparativa-de-tecnologias","title":"Comparativa de Tecnologias","text":""},{"location":"guia-inicio/roadmap/#cuando-usar-cada-herramienta","title":"Cuando usar cada herramienta?","text":"<pre><code>graph TD\n    A[Tienes datos?] --&gt; B{Cuanto pesa?}\n    B --&gt;|&lt; 5GB| C[Pandas]\n    B --&gt;|5-100GB| D[Dask]\n    B --&gt;|&gt; 100GB| E[PySpark]\n\n    C --&gt; F{Necesitas BD?}\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt;|Si, local| G[SQLite]\n    F --&gt;|Si, produccion| H[PostgreSQL/MySQL]\n    F --&gt;|No| I[Parquet]</code></pre>"},{"location":"guia-inicio/roadmap/#tabla-comparativa","title":"Tabla Comparativa","text":"Tamano de Datos Herramienta Tiempo de Procesamiento RAM Necesaria &lt; 1GB Pandas Segundos 2-4x tamano datos 1-5GB Pandas Minutos 2-4x tamano datos 5-50GB Dask Minutos Cualquier RAM 50-500GB Dask/PySpark Minutos-Horas Cualquier RAM &gt; 500GB PySpark Horas Cluster"},{"location":"guia-inicio/roadmap/#certificacion-y-evaluacion","title":"Certificacion y Evaluacion","text":""},{"location":"guia-inicio/roadmap/#para-alumnos-del-curso-presencial","title":"Para Alumnos del Curso Presencial","text":"<ul> <li> Certificado de 230 horas</li> <li> Evaluacion automatica via PROMPTS.md</li> <li> Proyecto final integrador</li> <li> Soporte directo del instructor</li> </ul>"},{"location":"guia-inicio/roadmap/#para-autodidactas","title":"Para Autodidactas","text":"<ul> <li> Portfolio de proyectos en GitHub</li> <li> Codigo revisable por empleadores</li> <li> Experiencia con datos reales</li> <li> Aprendizaje a tu ritmo</li> </ul> <p>Tu GitHub es tu Certificado</p> <p>Los empleadores valoran mas ver tu codigo y proyectos que un PDF. Asegurate de:</p> <ul> <li>Hacer commits claros y profesionales</li> <li>Documentar tu codigo</li> <li>Completar los ejercicios con calidad</li> <li>Agregar un README personalizado a tu fork</li> </ul>"},{"location":"guia-inicio/roadmap/#recursos-adicionales","title":"Recursos Adicionales","text":""},{"location":"guia-inicio/roadmap/#documentacion-oficial","title":"Documentacion Oficial","text":"<ul> <li>Pandas Documentation</li> <li>Dask Documentation</li> <li>PySpark Documentation</li> <li>SQLite Tutorial</li> </ul>"},{"location":"guia-inicio/roadmap/#cursos-complementarios","title":"Cursos Complementarios","text":"<ul> <li>Python for Data Science</li> <li>Big Data with Spark</li> <li>SQL for Data Science</li> </ul>"},{"location":"guia-inicio/roadmap/#comunidades","title":"Comunidades","text":"<ul> <li>r/datascience</li> <li>Stack Overflow - pandas</li> <li>Dask Discourse</li> </ul>"},{"location":"guia-inicio/roadmap/#proximos-pasos","title":"Proximos Pasos","text":"<p>Ahora que conoces el roadmap completo:</p> <ol> <li>Instalar Herramientas - Si aun no las tienes</li> <li>Tu Primer Ejercicio - Empezar a practicar</li> <li>Fork y Clone - Configurar tu entorno de trabajo</li> </ol>"},{"location":"en/","title":"COMPLETE BIG DATA COURSE","text":"<p> \"Without experience there is no knowledge\" </p>"},{"location":"en/#live-demos","title":"Live Demos","text":"Global Seismic Observatory <p>Real-time earthquakes from USGS API. Interactive map, magnitude filters, tsunami alerts.</p> <p> View Live </p> ISS Tracker <p>Track the International Space Station in real time. Pass predictor over your city.</p> <p> View Live </p> <p> These dashboards update automatically with real data from public APIs </p>"},{"location":"en/#the-course-in-numbers","title":"The Course in Numbers","text":"230 Hours of content 9 Complete modules 25+ Hands-on exercises 12+ Interactive dashboards 30+ Technologies"},{"location":"en/#complete-technology-stack","title":"Complete Technology Stack","text":""},{"location":"en/#databases","title":"Databases","text":"Technology Level What You'll Learn SQLite Basic SQL queries, indexes, optimization PostgreSQL Intermediate Complex joins, Window Functions, CTEs Oracle Advanced PL/SQL, stored procedures DynamoDB Advanced NoSQL, key-value, serverless"},{"location":"en/#data-processing","title":"Data Processing","text":"Technology When to Use It Scale Pandas Exploratory analysis &lt; 5 GB Dask Large datasets, single machine 5-100 GB Apache Spark Clusters, production &gt; 100 GB Spark Streaming Real-time data Unlimited"},{"location":"en/#streaming-and-messaging","title":"Streaming and Messaging","text":"Technology Purpose Apache Kafka Distributed streaming, KRaft mode Spark Structured Streaming Stream processing AWS Kinesis Cloud streaming"},{"location":"en/#cloud-and-infrastructure","title":"Cloud and Infrastructure","text":"Technology What It Does Docker Containers, reproducible environments Docker Compose Multi-container orchestration LocalStack Local AWS simulation (free) Terraform Infrastructure as Code AWS S3 Object storage AWS Lambda Serverless functions EventBridge Task scheduling"},{"location":"en/#machine-learning-and-ai","title":"Machine Learning and AI","text":"Technology Application Scikit-learn Classic ML, clustering, classification PCA Dimensionality reduction K-Means Segmentation, clustering TensorFlow Deep Learning, neural networks MobileNetV2 Transfer Learning, Computer Vision ARIMA/SARIMA Time series, forecasting"},{"location":"en/#nlp-and-text-mining","title":"NLP and Text Mining","text":"Technology Use NLTK Natural language processing TF-IDF Text vectorization Sentiment Analysis Sentiment analysis Jaccard Similarity Document similarity"},{"location":"en/#visualization","title":"Visualization","text":"Technology Type Plotly Interactive dashboards Matplotlib Static charts Seaborn Statistical visualization Leaflet.js Interactive maps Altair Declarative charts"},{"location":"en/#econometrics","title":"Econometrics","text":"Technology Application linearmodels Panel data Panel OLS Fixed and random effects Hausman Test Model selection"},{"location":"en/#course-modules","title":"Course Modules","text":""},{"location":"en/#module-1-databases","title":"Module 1: Databases","text":"<p>SQLite, PostgreSQL, Oracle, migrations</p> <p>From your first SELECT query to Oracle stored procedures. You will learn to design schemas, optimize queries, and migrate between engines.</p> <p>View Exercises</p>"},{"location":"en/#module-2-data-cleaning-and-etl","title":"Module 2: Data Cleaning and ETL","text":"<p>Professional ETL pipeline, QoG Dataset, PostgreSQL</p> <p>Build a modular ETL pipeline that processes the Quality of Government dataset (1,289 variables, 194+ countries). Cleaning, transformation, and loading into PostgreSQL.</p> <p>View Exercises</p>"},{"location":"en/#module-3-distributed-processing","title":"Module 3: Distributed Processing","text":"<p>Dask, Parquet, Local Cluster</p> <p>Process large datasets without needing a cluster. Dask lets you scale pandas to data that doesn't fit in memory, using Parquet and local parallelism.</p> <p>View Exercises</p>"},{"location":"en/#module-4-machine-learning","title":"Module 4: Machine Learning","text":"<p>PCA, K-Means, Transfer Learning, ARIMA/SARIMA</p> <p>Dimensionality reduction, clustering, image classification with TensorFlow, and time series with Box-Jenkins methodology. All with real datasets.</p> <p>View Exercises</p>"},{"location":"en/#module-5-nlp-and-text-mining","title":"Module 5: NLP and Text Mining","text":"<p>NLTK, TF-IDF, Jaccard, Sentiment Analysis</p> <p>Tokenization, text cleaning, document similarity, sentiment analysis, and vectorization with TF-IDF.</p> <p>View Exercises</p>"},{"location":"en/#module-6-panel-data-analysis","title":"Module 6: Panel Data Analysis","text":"<p>Fixed Effects, Random Effects, Hausman Test</p> <p>Analyze longitudinal data (country x year). Replicate real academic studies on gun laws and traffic mortality.</p> <p>View Exercises</p>"},{"location":"en/#module-7-big-data-infrastructure","title":"Module 7: Big Data Infrastructure","text":"<p>Docker, Docker Compose, Apache Spark, Cluster Computing</p> <p>Understand how infrastructure is built. Containers, orchestration with Docker Compose, Spark clusters with Master-Worker architecture. The foundation for the Capstone Project.</p> <p>View Exercises</p>"},{"location":"en/#module-8-streaming-with-kafka","title":"Module 8: Streaming with Kafka","text":"<p>Apache Kafka, KRaft, Spark Structured Streaming</p> <p>Real-time streaming with Kafka (KRaft mode, no ZooKeeper). Producers, consumers, Spark Structured Streaming, and a seismic alert system.</p> <p>View Exercises</p>"},{"location":"en/#module-9-cloud-with-localstack","title":"Module 9: Cloud with LocalStack","text":"<p>LocalStack, Terraform, AWS (S3, Lambda, DynamoDB)</p> <p>Simulate AWS on your machine at no cost. Infrastructure as Code with Terraform, serverless Lambda functions, and Data Lake architecture.</p> <p>View Exercises</p>"},{"location":"en/#capstone-project","title":"Capstone Project","text":"<p>Docker + Spark + PostgreSQL + Full Analysis</p> <p>Integrate everything you have learned into an end-to-end project. Infrastructure with Docker, ETL with Spark, analysis with your own research question.</p> <p>View Assignment</p>"},{"location":"en/#dashboard-gallery","title":"Dashboard Gallery","text":"<p>All of these dashboards were created during the course:</p> ARIMA PRO Bloomberg-style time series View Dashboard PCA + K-Means Clustering and dimensionality reduction View Dashboard Transfer Learning Flower classification with CNN View Dashboard Panel Data QoG Spark + PostgreSQL + ML View Dashboard <p>View All Dashboards</p>"},{"location":"en/#who-is-this-course-for","title":"Who Is This Course For?","text":"Students and Self-LearnersWorking ProfessionalsCompanies <ul> <li>All content is free and open source</li> <li>Learn at your own pace with progressive exercises</li> <li>Build a professional portfolio of projects</li> <li>Real dashboards you can showcase in interviews</li> </ul> <ul> <li>Update your skills to modern technologies</li> <li>From Excel to Spark in weeks, not years</li> <li>Streaming, Cloud, ML - all in a single course</li> <li>Immediately applicable to your job</li> </ul> <ul> <li>In-company training available</li> <li>Material proven in 230+ hours of in-person classes</li> <li>Real industry use cases</li> <li>Consulting for specific projects</li> </ul>"},{"location":"en/#how-to-get-started","title":"How to Get Started","text":"<p>In-Person Course Students</p> <p>Read the Submission Guide first to learn how to submit your assignments.</p>"},{"location":"en/#step-1-fork-and-clone","title":"Step 1: Fork and Clone","text":"<pre><code># Fork on GitHub (button at the top right)\n# Then clone YOUR fork:\ngit clone https://github.com/YOUR_USERNAME/ejercicios-bigdata.git\ncd ejercicios-bigdata\n</code></pre>"},{"location":"en/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"en/#step-3-choose-your-path","title":"Step 3: Choose Your Path","text":"If you are... Start with... Beginner Exercise 1.1: SQLite Intermediate ETL Pipeline QoG Advanced Streaming with Kafka"},{"location":"en/#instructor","title":"Instructor","text":"Juan Marcelo Gutierrez Miranda @TodoEconometria 10+ years in data analysis and Big Data. I have trained hundreds of professionals at companies across Latin America and Spain."},{"location":"en/#professional-services","title":"Professional Services","text":"<ul> <li>In-Company Training: Courses tailored to your team and technologies</li> <li>Big Data Consulting: Pipeline design, data architecture</li> <li>Dashboard Development: Interactive visualizations for your business</li> </ul> <p>Contact:</p> <ul> <li>Email: cursos@todoeconometria.com</li> <li>LinkedIn: Juan Gutierrez</li> <li>Web: www.todoeconometria.com</li> </ul>"},{"location":"en/#contributions","title":"ContributionsYour Big Data Career Starts Here","text":"<p>This repository is open source. If you find errors or want to contribute:</p> <ol> <li>Fork the repository</li> <li>Create a branch for your change</li> <li>Submit a Pull Request</li> </ol> <p>230 hours of content, 30+ technologies, real-time dashboards</p> Start Now <p> Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c </p>"},{"location":"en/faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Answers to the most common questions about the course.</p>"},{"location":"en/faq/#do-i-need-prior-experience-in-big-data","title":"Do I need prior experience in Big Data?","text":"<p>No. The course starts from scratch. You only need:</p> <ul> <li>Basic Python knowledge</li> <li>Ability to use the terminal/console</li> <li>Willingness to learn</li> </ul> <p>Don't have Python?</p> <p>Go to the Installation Guide where we explain how to install everything from scratch.</p>"},{"location":"en/faq/#how-long-does-it-take-to-complete-the-exercises","title":"How long does it take to complete the exercises?","text":"<p>It depends on your level:</p> Level Total Time Hours/Week Duration Beginner 120-140 hours 10-15h 10-12 weeks Intermediate 60-80 hours 8-10h 6-8 weeks Advanced 40-50 hours 5-8h 4-5 weeks <p>No rush. Learn at your own pace. The important thing is to understand each concept well.</p>"},{"location":"en/faq/#is-the-data-real-or-synthetic","title":"Is the data real or synthetic?","text":"<p>Real. We use real public datasets:</p> <ul> <li>NYC Taxi &amp; Limousine Commission (TLC)</li> <li>Weather data from NOAA</li> <li>Other public datasets from Kaggle</li> </ul> <p>This gives you experience with real-world data (dirty, incomplete, large).</p>"},{"location":"en/faq/#can-i-use-this-in-my-portfolio","title":"Can I use this in my portfolio?","text":"<p>Yes! In fact, we recommend it.</p> <p>Many students have landed jobs by showcasing:</p> <ul> <li>Their exercise solutions</li> <li>The dashboard they created</li> <li>Their GitHub fork with professional commits</li> </ul> <p>Tip</p> <p>Make your fork public and add a personalized README explaining your learning journey.</p>"},{"location":"en/faq/#is-there-a-certificate-upon-completion","title":"Is there a certificate upon completion?","text":"<p>For in-person course students: Yes, a 230-hour certificate.</p> <p>For self-learners: There is no official certificate, but your GitHub is your certificate. Employers value seeing your code more than a PDF.</p> <p>Your GitHub is Your Certificate</p> <ul> <li>Professional commits</li> <li>Well-documented code</li> <li>Complete projects</li> <li>Active contributions</li> </ul>"},{"location":"en/faq/#technical","title":"Technical","text":""},{"location":"en/faq/#what-computer-do-i-need","title":"What computer do I need?","text":"<p>Minimum:</p> <ul> <li>8GB RAM</li> <li>20GB disk space</li> <li>i5 processor or equivalent</li> <li>Windows 10+, macOS 10.14+, or Linux (Ubuntu 20.04+)</li> </ul> <p>Recommended:</p> <ul> <li>16GB RAM</li> <li>50GB SSD disk space</li> <li>i7 processor or equivalent</li> </ul> <p>Don't have good hardware?</p> <p>You can use Google Colab or GitHub Codespaces (free) to work in the cloud.</p>"},{"location":"en/faq/#does-it-work-on-windowsmaclinux","title":"Does it work on Windows/Mac/Linux?","text":"<p>Yes. The course is compatible with all three operating systems.</p> <ul> <li>Windows: Preferably Windows 10 or higher</li> <li>macOS: macOS 10.14 (Mojave) or higher</li> <li>Linux: Ubuntu 20.04+, Fedora, Arch, etc.</li> </ul> <p>The Installation Guide has specific instructions for each system.</p>"},{"location":"en/faq/#can-i-use-another-ide-instead-of-pycharm","title":"Can I use another IDE instead of PyCharm?","text":"<p>Yes. PyCharm is recommended but not required.</p> <p>Alternatives:</p> <ul> <li>Visual Studio Code - Lightweight and very popular</li> <li>Jupyter Lab - Excellent for notebooks</li> <li>Sublime Text - Advanced text editor</li> <li>Vim/Emacs - If you are an advanced user</li> </ul> <p>The important thing is that you feel comfortable with your tool.</p>"},{"location":"en/faq/#how-do-i-download-the-data","title":"How do I download the data?","text":"<p>The data is downloaded automatically with a script:</p> <pre><code># Ir a la carpeta de datos\ncd datos\n\n# Ejecutar script de descarga\npython descargar_datos.py\n</code></pre> <p>The script automatically downloads and decompresses all the required datasets.</p> <p>Disk space</p> <p>The complete datasets take up ~5GB. Make sure you have enough space.</p>"},{"location":"en/faq/#git-and-github","title":"Git and GitHub","text":""},{"location":"en/faq/#i-have-never-used-git-is-it-very-difficult","title":"I have never used Git. Is it very difficult?","text":"<p>It is not difficult, but it requires practice.</p> <p>We have complete step-by-step guides:</p> <ol> <li>Fork and Clone - The basics</li> <li>Your First Exercise - Complete workflow</li> <li>Useful Commands - Quick reference</li> </ol> <p>Learn by doing</p> <p>The best way to learn Git is by using it. Your first commits will feel awkward, but you improve quickly.</p>"},{"location":"en/faq/#what-is-a-fork-why-do-i-need-one","title":"What is a Fork? Why do I need one?","text":"<p>Fork = Your personal copy of the repository on GitHub.</p> <p>You need it because:</p> <ul> <li> You cannot modify the instructor's repository directly</li> <li> The fork is YOUR space to work</li> <li> You can sync it with the original</li> <li> The system automatically evaluates your PROMPTS in your fork</li> </ul> <p>See the full guide: Fork and Clone</p>"},{"location":"en/faq/#how-do-i-keep-my-fork-up-to-date","title":"How do I keep my Fork up to date?","text":"<p>When the instructor adds new exercises, you need to sync your fork.</p> <p>Easy method (GitHub Web):</p> <ol> <li>Go to your fork on GitHub</li> <li>Click \"Sync fork\" \u2192 \"Update branch\"</li> <li>On your PC: <code>git pull origin main</code></li> </ol> <p>Full method (Terminal):</p> <pre><code>git checkout main\ngit fetch upstream\ngit merge upstream/main\ngit push origin main\n</code></pre> <p>See the full guide: Sync Fork</p>"},{"location":"en/faq/#i-made-a-bad-commit-how-do-i-undo-it","title":"I made a bad commit. How do I undo it?","text":"<p>Before pushing:</p> <pre><code># Deshacer ultimo commit (mantiene cambios)\ngit reset --soft HEAD~1\n\n# Deshacer ultimo commit (descarta cambios)\ngit reset --hard HEAD~1\n</code></pre> <p>After pushing:</p> <pre><code># Crear nuevo commit que revierte el anterior\ngit revert HEAD\ngit push origin tu-rama\n</code></pre> <p>Avoid force push</p> <p>Never use <code>git push --force</code> on shared branches or open Pull Requests.</p>"},{"location":"en/faq/#exercises","title":"Exercises","text":""},{"location":"en/faq/#i-cannot-complete-an-exercise-what-should-i-do","title":"I cannot complete an exercise. What should I do?","text":"<p>Step 1: Read the error carefully</p> <p>Most of the time the error tells you exactly what is wrong.</p> <p>Step 2: Search on Google</p> <p>Copy the error message and search for it. Someone else has probably had it before.</p> <p>Step 3: Check the documentation</p> <ul> <li>Pandas Docs</li> <li>SQLite Tutorial</li> <li>Python Docs</li> </ul> <p>Step 4: Ask for help</p> <ul> <li>In-person students: Ask during class</li> <li>Self-learners: Create an Issue on GitHub explaining your problem</li> </ul> <p>How to ask for help</p> <p>Include:</p> <ul> <li>What you tried to do</li> <li>What error you got (full message)</li> <li>What you have already tried</li> <li>Your relevant code</li> </ul>"},{"location":"en/faq/#can-i-do-the-exercises-out-of-order","title":"Can I do the exercises out of order?","text":"<p>Not recommended. The exercises are designed to:</p> <ul> <li>Build upon previous knowledge</li> <li>Gradually increase in difficulty</li> <li>Introduce concepts in a logical order</li> </ul> <p>Exception</p> <p>If you already have experience with Python and Pandas, you can start at LEVEL 2 (Exercise 03).</p>"},{"location":"en/faq/#how-many-times-can-i-attempt-an-exercise","title":"How many times can I attempt an exercise?","text":"<p>As many as you need. There is no limit on attempts.</p> <p>The goal is to learn, not to pass quickly.</p> <ul> <li>You can update your fork as many times as you want with <code>git push</code></li> <li>The automatic evaluation system checks your PROMPTS.md file</li> <li>You learn more from mistakes than from successes</li> </ul>"},{"location":"en/faq/#can-i-use-additional-libraries","title":"Can I use additional libraries?","text":"<p>Yes, but:</p> <ol> <li>Justify why you need them</li> <li>Add them to <code>requirements.txt</code></li> <li>Document how to install them</li> <li>Mention in PROMPTS.md which libraries you used</li> </ol> <p>Example</p> <p>If you use <code>seaborn</code> for visualizations:</p> <pre><code># requirements.txt\npandas==2.0.0\nseaborn==0.12.0  # Para visualizaciones avanzadas\n</code></pre> <p>And in your <code>PROMPTS.md</code> file mention: \"I used seaborn to create more professional charts\"</p>"},{"location":"en/faq/#support","title":"Support","text":""},{"location":"en/faq/#do-you-offer-support-if-i-get-stuck","title":"Do you offer support if I get stuck?","text":"<p>For in-person course students:</p> <ul> <li> Full support during sessions</li> <li> Email consultations</li> <li> Automatic submission review</li> </ul> <p>For self-learners:</p> <ul> <li> No direct support</li> <li> You can create Issues on GitHub</li> <li> The community can help you</li> <li> Complete documentation available</li> </ul>"},{"location":"en/faq/#how-do-i-contact-the-instructor","title":"How do I contact the instructor?","text":"<p>For course inquiries:</p> <ul> <li>GitHub Issues: Create Issue</li> <li>Email: cursos@todoeconometria.com</li> </ul> <p>For business consulting:</p> <ul> <li>Email: cursos@todoeconometria.com</li> <li>LinkedIn: Juan Gutierrez</li> <li>Web: TodoEconometria</li> </ul> <p>Response time</p> <ul> <li>In-person students: 24-48 hours</li> <li>Self-learners via Issues: When available</li> <li>Businesses: 24 hours</li> </ul>"},{"location":"en/faq/#technical-issues","title":"Technical Issues","text":""},{"location":"en/faq/#python-is-not-recognized-as-a-command","title":"Python is not recognized as a command","text":"<p>Windows:</p> <ol> <li>Reinstall Python</li> <li>Check \"Add Python to PATH\"</li> <li>Restart the terminal</li> </ol> <p>macOS/Linux:</p> <p>Use <code>python3</code> instead of <code>python</code>:</p> <pre><code>python3 --version\npip3 install pandas\n</code></pre>"},{"location":"en/faq/#error-modulenotfounderror","title":"Error: ModuleNotFoundError","text":"<p>Cause: You did not install the dependencies.</p> <p>Solution:</p> <pre><code># Activa el entorno virtual\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\n\n# Instala dependencias\npip install -r requirements.txt\n</code></pre>"},{"location":"en/faq/#git-says-fatal-not-a-git-repository","title":"Git says \"fatal: not a git repository\"","text":"<p>Cause: You are not in the project folder.</p> <p>Solution:</p> <pre><code># Navega a la carpeta correcta\ncd path/to/ejercicios-bigdata\n\n# Verifica\ngit status  # Deberia funcionar\n</code></pre>"},{"location":"en/faq/#i-cannot-push-permission-denied","title":"I cannot push: \"Permission denied\"","text":"<p>Cause: Authentication issues with GitHub.</p> <p>Quick solution (HTTPS):</p> <pre><code># Cambiar a HTTPS\ngit remote set-url origin https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Intentar push de nuevo\ngit push origin tu-rama\n</code></pre> <p>Permanent solution (SSH):</p> <p>Configure SSH keys: GitHub SSH Guide</p>"},{"location":"en/faq/#the-dashboard-does-not-load-the-data","title":"The dashboard does not load the data","text":"<p>Check:</p> <ol> <li> <p>Does the database exist?    <pre><code>ls datos/taxi.db\n</code></pre></p> </li> <li> <p>Is Flask running?    <pre><code>python app.py\n</code></pre></p> </li> <li> <p>Correct port? (default: 5000)    <pre><code>http://localhost:5000\n</code></pre></p> </li> <li> <p>Check the browser error console (F12)</p> </li> </ol>"},{"location":"en/faq/#career-and-employment","title":"Career and Employment","text":""},{"location":"en/faq/#will-this-course-help-me-get-a-job","title":"Will this course help me get a job?","text":"<p>It can help a lot, especially if:</p> <ul> <li>You complete all exercises with quality</li> <li>You create a professional dashboard</li> <li>You document your code well</li> <li>You maintain an active GitHub</li> </ul> <p>What employers value</p> <ol> <li>Project portfolio (your GitHub)</li> <li>Clean, documented code</li> <li>Experience with real data</li> <li>Problem-solving ability</li> </ol> <p>A well-crafted repository is worth more than 10 certificates.</p>"},{"location":"en/faq/#what-jobs-can-i-get-with-these-skills","title":"What jobs can I get with these skills?","text":"<p>With the skills from this course you can apply for:</p> <ul> <li>Data Analyst - Data analysis with Python/SQL</li> <li>Junior Data Scientist - Modeling and advanced analysis</li> <li>Data Engineer - ETL pipelines, data processing</li> <li>Business Intelligence Developer - Dashboards and reports</li> <li>Python Developer - Backend development with data</li> </ul>"},{"location":"en/faq/#do-i-need-a-university-degree","title":"Do I need a university degree?","text":"<p>It depends on the employer.</p> <ul> <li>Tech companies: Value portfolio over degree</li> <li>Traditional companies: May require a degree</li> <li>Startups: Portfolio &gt; Degree</li> <li>Freelance: Only your work matters</li> </ul> <p>Compensating for lack of a degree</p> <ul> <li>Solid GitHub portfolio</li> <li>Relevant certifications</li> <li>Impressive personal projects</li> <li>Open source contributions</li> </ul>"},{"location":"en/faq/#other","title":"Other","text":""},{"location":"en/faq/#can-i-share-my-solution-publicly","title":"Can I share my solution publicly?","text":"<p>Yes, but consider:</p> <ul> <li>After completing: Share after receiving your grade</li> <li>With credits: Mention that it is from the TodoEconometria course</li> <li>No spoilers: Do not share solutions to help others cheat</li> </ul> <p>Sharing is good</p> <p>Sharing your code helps:</p> <ul> <li>Others to learn</li> <li>Build your personal brand</li> <li>Demonstrate skills to employers</li> </ul>"},{"location":"en/faq/#is-the-course-updated","title":"Is the course updated?","text":"<p>Yes. The repository is updated regularly with:</p> <ul> <li>New exercises</li> <li>Improvements to existing ones</li> <li>Library updates</li> <li>New datasets</li> <li>Bug fixes</li> </ul> <p>Keep your fork synced to get updates: Sync Fork</p>"},{"location":"en/faq/#can-i-contribute-to-the-course","title":"Can I contribute to the course?","text":"<p>Yes! Contributions are welcome.</p> <p>You can contribute by:</p> <ul> <li> Reporting bugs via Issues</li> <li> Improving documentation</li> <li> Suggesting new exercises</li> <li> Sharing your dashboard in the gallery</li> </ul> <p>See the Contributions section</p>"},{"location":"en/faq/#where-can-i-learn-more","title":"Where can I learn more?","text":"<p>Recommended resources:</p> <ul> <li>Python for Data Analysis - Book by Wes McKinney</li> <li>SQL Tutorial - Interactive SQL</li> <li>Dask Tutorial - Official Dask tutorial</li> <li>r/datascience - Reddit community</li> </ul> <p>Complementary courses:</p> <ul> <li>Python for Data Science (Coursera)</li> <li>SQL for Data Science (DataCamp)</li> <li>Apache Spark (Udacity)</li> </ul>"},{"location":"en/faq/#unanswered-questions","title":"Unanswered Questions?","text":"<p>Didn't find your answer?</p> <p>In-person students: Ask in the next session or send an email</p> <p>Self-learners: Create an Issue on GitHub:</p> <p>Create Issue</p> <p>Include: - Descriptive title - Detailed description of your question - Context (which exercise, what you tried, etc.)</p>"},{"location":"en/faq/#additional-resources","title":"Additional Resources","text":"<ul> <li>Getting Started Guide - Start from scratch</li> <li>Git and GitHub - Git guides</li> <li>Exercises - Exercise list</li> <li>Roadmap - Study plan</li> </ul>"},{"location":"en/infraestructura/","title":"Lab Infrastructure","text":"<p>\"Prevent Big Data from crashing your main disk\"</p> <p>To carry out this course, we have designed a hybrid architecture that combines the ease of use of Windows with the power of isolated Linux servers in Docker.</p> <p>This guide explains what is happening \"under the hood\" when you run the installation script.</p>"},{"location":"en/infraestructura/#hybrid-architecture","title":"Hybrid Architecture","text":"<p>Our environment uses three layers:</p> <ol> <li>User Layer (You): You write Python code in PyCharm/VSCode.</li> <li>Compute Layer (Docker): Heavy services (Spark, Postgres) run isolated.</li> <li>Data Layer (Storage): Massive files are stored on a dedicated disk.</li> </ol> <pre><code>graph TD\n    subgraph \"Your Computer\"\n        IDE[PyCharm / VSCode] --&gt;|Runs| PY[Python Scripts]\n        PY --&gt;|Connects port 5432| PG[(PostgreSQL)]\n        PY --&gt;|Connects port 7077| SPARK[Spark Cluster]\n    end\n\n    subgraph \"Docker (Containers)\"\n        PG\n        SPARK\n        ADMIN[pgAdmin4]\n    end\n\n    subgraph \"Physical Storage\"\n        SSD[External SSD / Data Partition]\n        HDD[System Disk C:]\n    end\n\n    PG --&gt;|Stores| SSD\n    SPARK --&gt;|Reads| SSD\n    PY -.-&gt;|Junction Link| SSD</code></pre>"},{"location":"en/infraestructura/#the-storage-dilemma-hdd-vs-ssd","title":"The Storage Dilemma (HDD vs SSD)","text":"<p>In Data Projects, I/O (Disk Speed) is critical.</p>"},{"location":"en/infraestructura/#option-a-external-ssd-pro-mode","title":"Option A: External SSD (PRO Mode)","text":"<p>If you have the lab configured on an external SSD (<code>E:\\BIGDATA_LAB_STORAGE</code>): 1.  Speed: Docker has a dedicated lane for reading/writing data. 2.  Safety: If you download a 50GB dataset, your Windows won't run out of space. 3.  The \"Magic Tunnel\" (Junction): Windows creates a symbolic link. You see the <code>datos/</code> folder in your project, but physically the bytes are on the external SSD.</p>"},{"location":"en/infraestructura/#option-b-local-disk","title":"Option B: Local Disk","text":"<p>If you use your main disk (<code>C:</code>): - The system will work the same, but you must monitor free space. - Docker and Windows will compete for disk usage (lower performance).</p>"},{"location":"en/infraestructura/#docker-services-the-stack","title":"Docker Services (The \"Stack\")","text":"<p>The <code>setup_cluster.ps1</code> script automatically launches these services:</p>"},{"location":"en/infraestructura/#1-postgresql-the-data-warehouse","title":"1. PostgreSQL (The Data Warehouse)","text":"<ul> <li>Why SQL: Although we process with Spark/Dask, the final \"valuable\" data must reside in a structured database to be consumed by Dashboards or Analysts.</li> <li>Why not Mongo/Cassandra: For this data level (structured Gigabytes), PostgreSQL is more efficient with local resources and is the gold standard for the \"Serving\" layer.</li> <li>Access: <code>localhost:5432</code></li> </ul>"},{"location":"en/infraestructura/#2-apache-spark-the-processing-engine","title":"2. Apache Spark (The Processing Engine)","text":"<ul> <li>Master + Worker: We simulate a real cluster. The \"Master\" distributes tasks and the \"Worker\" executes them.</li> <li>Access: <code>localhost:8081</code> (Dashboard)</li> </ul>"},{"location":"en/infraestructura/#3-pgadmin-4-the-interface","title":"3. pgAdmin 4 (The Interface)","text":"<ul> <li>A web interface to manage your SQL database without commands.</li> <li>Access: <code>http://localhost:8080</code></li> </ul>"},{"location":"en/infraestructura/#docker-rescue","title":"Docker Rescue","text":"<p>Has Docker ever \"not started\" for you? The setup script includes a rescue module: 1.  Checks if the Windows service <code>com.docker.service</code> is running. 2.  If it is stopped, attempts to restart it with administrator permissions. 3.  Waits for the engine to be ready before attempting to launch the containers.</p>"},{"location":"en/infraestructura/#how-to-get-started","title":"How to Get Started","text":"<p>To launch the Docker stack, open PowerShell as Administrator and run:</p> <pre><code># Navigate to the project folder\ncd \"C:\\Users\\YOUR_USERNAME\\Documents\\ejercicios_bigdata\"\n\n# Launch the services\ndocker-compose up -d\n\n# Verify they are running\ndocker ps\n</code></pre> <p>Services available after starting: - PostgreSQL: <code>localhost:5432</code> - Spark Master UI: <code>localhost:8081</code> - pgAdmin: <code>localhost:8080</code></p>"},{"location":"en/dashboards/","title":"Visualization and Dashboard Gallery","text":"<p>Results from the Machine Learning and NLP algorithms of the course. Each page includes analysis, charts, and source code.</p>"},{"location":"en/dashboards/#full-analyses","title":"Full Analyses","text":"<ul> <li>PCA + K-Means Clustering: Iris Dataset -- Dimensionality reduction and clustering on Fisher's classic dataset</li> <li>Manual PCA FactoMineR Style -- Replicating the gold standard of multivariate analysis in Python</li> </ul>"},{"location":"en/dashboards/#time-series","title":"Time Series","text":"<ul> <li>ARIMA/SARIMA - Box-Jenkins Methodology -- Identification, estimation, diagnostics, and forecasting</li> <li>ARIMA Dashboard (interactive) -- 6 tabs: series, decomposition, ACF/PACF, diagnostics, forecast, radar</li> <li>ARIMA PRO Dashboard -- Bloomberg-style financial theme, KPIs, 7 tabs, model comparison</li> <li>ARIMA PRO Dashboard (interactive) -- Design inspired by OECD Explorer and Portfolio Optimizer</li> </ul>"},{"location":"en/dashboards/#computer-vision-transfer-learning","title":"Computer Vision (Transfer Learning)","text":"<ul> <li>Flower Classification with Transfer Learning -- MobileNetV2 + traditional ML on embeddings</li> <li>Flowers Dashboard (interactive) -- t-SNE, model comparison, confusion matrix, radar</li> </ul>"},{"location":"en/dashboards/#interactive-dashboards-html","title":"Interactive Dashboards (HTML)","text":"<ul> <li>PCA Iris Dashboard (interactive) -- Full-screen visualization</li> <li>FactoMineR Dashboard (interactive) -- Correlation circles and biplots</li> </ul>"},{"location":"en/dashboards/#nlp-and-text-mining","title":"NLP and Text Mining","text":""},{"location":"en/dashboards/#guided-exercises","title":"Guided Exercises","text":"<ul> <li>Exercise 01: Text Anatomy and Word Counting -- Frequency, tokenization, and visualization</li> <li>Exercise 02: Stopword Filtering -- Semantic noise cleaning</li> </ul>"},{"location":"en/dashboards/#full-analyses_1","title":"Full Analyses","text":"<ul> <li>Jaccard Similarity -- Document comparison and recommendation system</li> <li>Document Vectorization and Clustering -- TF-IDF, K-Means, and topic analysis</li> </ul>"},{"location":"en/dashboards/#panel-data-analysis-big-data","title":"Panel Data Analysis (Big Data)","text":""},{"location":"en/dashboards/#full-pipeline-spark-postgresql-ml","title":"Full Pipeline: Spark + PostgreSQL + ML","text":"<ul> <li>Module 06: QoG - 4 Research Lines + ML -- Central Asia, Water Security, Terrorism, Maghreb, PCA + K-Means</li> <li>QoG Dashboard (interactive) -- 5 tabs with interactive Plotly charts</li> </ul>"},{"location":"en/dashboards/#streaming-and-cloud-real-time","title":"Streaming and Cloud (Real-Time)","text":""},{"location":"en/dashboards/#live-dashboards","title":"Live Dashboards","text":"<ul> <li>Global Seismic Observatory -- Real-time earthquakes from USGS API</li> <li>Earthquake Dashboard (interactive) -- Leaflet map with live data, category filters</li> <li>ISS Tracker -- International Space Station tracking</li> <li>ISS Dashboard (interactive) -- Real-time position, pass predictor, orbital trajectory</li> </ul>"},{"location":"en/dashboards/#technologies","title":"Technologies","text":"<ul> <li>Live APIs: USGS Earthquakes, Where The ISS At, Open Notify</li> <li>Visualization: Leaflet.js, CARTO dark tiles, Lucide icons</li> <li>Refresh rate: Auto-refresh every 5-30 seconds</li> </ul>"},{"location":"en/dashboards/#source-code","title":"Source Code","text":"<p>The scripts that generate these visualizations are located in:</p> <ul> <li><code>ejercicios/04_machine_learning/</code> -- PCA, K-Means, Silhouette</li> <li><code>ejercicios/05_nlp_text_mining/</code> -- Counting, cleaning, sentiment, Jaccard</li> <li><code>ejercicios/06_an\u00e1lisis_datos_de_panel/</code> -- QoG Pipeline with Apache Spark</li> <li><code>ejercicios/07_infraestructura_bigdata/</code> -- Docker Compose, Spark Cluster</li> <li><code>ejercicios/08_streaming_kafka/</code> -- Kafka, Spark Structured Streaming</li> <li><code>ejercicios/09_cloud_localstack/</code> -- LocalStack, Terraform, AWS</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/","title":"PCA Manual in Python: FactoMineR Style","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#replicating-the-gold-standard-of-multivariate-analysis","title":"Replicating the Gold Standard of Multivariate Analysis","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#certification-and-reference-information","title":"Certification and Reference Information","text":"<p>Original author/Reference: @TodoEconometria Professor: Juan Marcelo Gutierrez Miranda Methodology: Advanced Courses in Big Data, Data Science, AI Application Development &amp; Applied Econometrics. Certification Hash ID: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Repository: https://github.com/TodoEconometria/certificaciones</p> <p>MAIN ACADEMIC REFERENCE:</p> <ul> <li>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. CRC Press.</li> <li>FactoMineR Tutorial: http://factominer.free.fr/course/FactoTuto.html</li> <li>Le, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: An R Package for Multivariate Analysis. Journal of Statistical Software, 25(1), 1-18.</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#1-introduction-why-factominer-is-the-gold-standard","title":"1. Introduction: Why FactoMineR is the Gold Standard","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#11-the-problem-of-multivariate-data","title":"1.1. The Problem of Multivariate Data","text":"<p>Imagine you have data from 41 athletes who competed in the decathlon (10 sporting events). Each athlete has 10 measurements: race times, jump distances, and throwing distances. The question is: How can we \"see\" patterns in 10 dimensions?</p> <p>The human brain can only visualize 2 or 3 dimensions. Principal Component Analysis (PCA) is the mathematical tool that allows us to \"compress\" those 10 dimensions into 2, losing as little information as possible.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#12-philosophy-factominer-vs-scikit-learn","title":"1.2. Philosophy: FactoMineR vs. Scikit-Learn","text":"<p>This manual replicates the logic of FactoMineR (the gold standard in R) using Python.</p> Aspect Scikit-learn FactoMineR / Prince Approach Machine Learning Data Exploration Objective Reduce dimensions for a predictive model Understand which variables drive which individuals Key metrics <code>explained_variance_ratio_</code> Contributions, \\(\\cos^2\\), correlations Supplementary variables Not supported Native support Plots Basic Correlation circle, biplot <p>The fundamental difference: Scikit-learn tells you \"these are your compressed data\". FactoMineR tells you \"why your data compressed this way, which variables are responsible, and how reliable the representation of each point is\".</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2-before-starting-missing-data-treatment","title":"2. BEFORE STARTING: Missing Data Treatment","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#21-the-most-common-error-ill-just-plug-in-the-mean","title":"2.1. The Most Common Error: \"I'll Just Plug in the Mean\"","text":"<p>One of the most frequent errors in multivariate analysis is imputing missing data with the mean without thinking. This practice, although common, can destroy the real structure of your data and lead you to erroneous conclusions.</p> <p>GOLDEN RULE: Before imputing any missing value, you must understand WHY that data is missing. The imputation strategy depends on the missingness mechanism.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#22-the-three-types-of-missing-data-rubin-1976","title":"2.2. The Three Types of Missing Data (Rubin, 1976)","text":"Type Name Meaning Example Can we use the mean? MCAR Missing Completely At Random The data is missing by pure chance, unrelated to any variable A sensor failed randomly YES, but with caution MAR Missing At Random The data is missing for a reason related to OTHER observed variables Young people don't answer salary questions (but we know their age) DEPENDS on the method MNAR Missing Not At Random The data is missing for a reason related to THE MISSING VALUE ITSELF The wealthy don't declare their assets BECAUSE they are high NO, it biases the results"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#23-why-the-mean-can-destroy-your-analysis","title":"2.3. Why the Mean Can Destroy Your Analysis","text":"<p>Imagine this scenario in the decathlon:</p> <pre><code>Athlete     | 100m  | Long Jump   | Throwing\n------------|-------|-------------|------------\nSprinter A  | 10.5s | 7.8m        | ???\nSprinter B  | 10.7s | 7.6m        | ???\nThrower C   | 11.8s | 6.2m        | 18.5m\nThrower D   | 12.0s | 6.0m        | 19.2m\n</code></pre> <p>Problem: The sprinters (A and B) did not complete the throwing event (due to injury).</p> <p>If you impute with the throwing mean: - Throwing mean = (18.5 + 19.2) / 2 = 18.85m - You assign 18.85m to sprinters A and B</p> <p>CATASTROPHIC CONSEQUENCE: - In the PCA, the sprinters appear as good throwers - The correlation circle will show that speed and throwing are positively correlated - THIS IS FALSE - in reality, sprinters tend to be worse throwers</p> <p>The problem: The data is NOT missing at random (MCAR). It is missing because the sprinters were injured in the throwing event (probably due to lack of technique). This is a case of MAR or MNAR.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#24-how-to-diagnose-the-type-of-missing-data","title":"2.4. How to Diagnose the Type of Missing Data","text":"<p>Before imputing, perform these diagnostic tests:</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#test-1-visual-missingness-pattern","title":"Test 1: Visual Missingness Pattern","text":"<pre><code>import missingno as msno\nimport matplotlib.pyplot as plt\n\n# Visualize missing data pattern\nmsno.matrix(df)\nplt.show()\n\n# Visualize correlation between missing data\nmsno.heatmap(df)\nplt.show()\n</code></pre> <p>Interpretation: - If the gaps are randomly dispersed: probably MCAR - If the gaps cluster in specific rows/columns: probably MAR or MNAR</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#test-2-littles-test-mcar-test","title":"Test 2: Little's Test (MCAR Test)","text":"<pre><code># Install: pip install pyampute\nfrom pyampute.exploration import mcar_test\n\n# Statistical test for MCAR\nresult = mcar_test(df)\nprint(f\"p-value: {result}\")\n# If p &gt; 0.05: MCAR not rejected (data MAY be random)\n# If p &lt; 0.05: MCAR rejected (data is NOT random)\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#test-3-compare-groups-complete-vs-incomplete-data","title":"Test 3: Compare Groups (Complete vs Incomplete Data)","text":"<pre><code># Create missing data indicator\ndf['has_na'] = df['throwing'].isna()\n\n# Compare means of other variables\nprint(df.groupby('has_na')['100m'].mean())\nprint(df.groupby('has_na')['long_jump'].mean())\n\n# If means are VERY different, the data is NOT MCAR\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#25-imputation-strategies-by-type","title":"2.5. Imputation Strategies by Type","text":"Missingness Type Recommended Strategy Strategy to AVOID MCAR (purely random) Mean, median, KNN, MICE - MAR (depends on other variables) KNN, MICE, Multiple regression Simple mean (biases) MNAR (depends on the missing value) Specific models, Sensitivity analysis Mean, KNN, MICE (all bias)"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#26-alternatives-to-the-mean-python-code","title":"2.6. Alternatives to the Mean (Python Code)","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#option-1-drop-rows-with-na-only-if-few-and-mcar","title":"Option 1: Drop Rows with NA (only if few and MCAR)","text":"<pre><code># Only if you have FEW missing data (&lt;5%) and they are MCAR\ndf_clean = df.dropna()\nprint(f\"Rows removed: {len(df) - len(df_clean)}\")\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#option-2-knn-imputation-respects-local-structure","title":"Option 2: KNN Imputation (respects local structure)","text":"<pre><code>from sklearn.impute import KNNImputer\n\n# KNN finds the K most similar neighbors and averages their values\nimputer = KNNImputer(n_neighbors=5)\ndf_imputed = pd.DataFrame(\n    imputer.fit_transform(df[numeric_columns]),\n    columns=numeric_columns\n)\n</code></pre> <p>Advantage: A sprinter with NA in throwing will receive the value from other similar sprinters, not the global average.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#option-3-multiple-imputation-mice-the-gold-standard","title":"Option 3: Multiple Imputation (MICE) - The Gold Standard","text":"<pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# MICE: Multiple Imputation by Chained Equations\nimputer = IterativeImputer(max_iter=10, random_state=42)\ndf_imputed = pd.DataFrame(\n    imputer.fit_transform(df[numeric_columns]),\n    columns=numeric_columns\n)\n</code></pre> <p>Advantage: Uses ALL variables to predict the missing value through iterative regressions.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#option-4-group-based-imputation-when-categories-exist","title":"Option 4: Group-Based Imputation (when categories exist)","text":"<pre><code># If you know the data depends on a category\ndf['throwing'] = df.groupby('athlete_type')['throwing'].transform(\n    lambda x: x.fillna(x.mean())\n)\n</code></pre> <p>Example: Impute a sprinter's throwing with the mean of other sprinters, not the global mean.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#27-checklist-before-performing-pca","title":"2.7. Checklist BEFORE Performing PCA","text":"<p>Before running your PCA analysis, verify:</p> <ul> <li> How many NAs do I have? <code>df.isna().sum()</code> - If &gt; 20% in a variable, consider removing it</li> <li> Where are the NAs? <code>msno.matrix(df)</code> - Look for patterns</li> <li> Are they random? Little's test or group comparison</li> <li> Which method should I use?</li> <li>MCAR + few NAs: Mean or drop rows</li> <li>MAR: KNN or MICE</li> <li>MNAR: Consult with a domain expert or sensitivity analysis</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#28-practical-example-what-to-do-in-the-decathlon","title":"2.8. Practical Example: What to Do in the Decathlon","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\n\n# Load data\ndf = pd.read_csv('decathlon.csv')\n\n# 1. Diagnosis\nprint(\"Missing data by column:\")\nprint(df.isna().sum())\n\n# 2. If there are NAs, check pattern\nif df.isna().any().any():\n    # Compare athletes with/without NAs\n    df['has_na'] = df.isna().any(axis=1)\n    print(\"\\nMean comparison (with NA vs without NA):\")\n    print(df.groupby('has_na')[['100m', 'Long.jump']].mean())\n\n    # If means are similar, probably MCAR -&gt; we can impute\n    # If means are very different, be careful -&gt; use KNN or MICE\n\n    # 3. Impute with KNN (respects structure)\n    imputer = KNNImputer(n_neighbors=3)\n    active_columns = ['100m', 'Long.jump', 'Shot.put', 'High.jump',\n                        '400m', '110m.hurdle', 'Discus', 'Pole.vault',\n                        'Javeline', '1500m']\n\n    df[active_columns] = imputer.fit_transform(df[active_columns])\n    print(\"\\nImputation completed with KNN\")\n\n# 4. Now we CAN perform PCA\nfrom prince import PCA\npca = PCA(n_components=5, rescale_with_std=True)\npca.fit(df[active_columns])\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#29-summary-the-3-step-rule","title":"2.9. Summary: The 3-Step Rule","text":"<pre><code>STEP 1: DIAGNOSE\n   \"Why is this data missing?\"\n   -&gt; MCAR, MAR, or MNAR?\n\nSTEP 2: DECIDE\n   \"Can I impute without biasing?\"\n   -&gt; If MNAR: DO NOT impute with mean\n   -&gt; If MAR: Use KNN or MICE\n   -&gt; If MCAR: Mean is acceptable\n\nSTEP 3: DOCUMENT\n   \"What did I do and why?\"\n   -&gt; Report the % of NAs\n   -&gt; Report the method used\n   -&gt; Report whether results change\n</code></pre> <p>MORAL: Plugging in the mean \"just because\" is like filling a hole in a puzzle with a piece from another puzzle. It might fit, but the final picture will be distorted.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#210-fundamental-why-0-is-valid-and-you-should-not-always-impute","title":"2.10. FUNDAMENTAL: Why 0 is Valid and You Should NOT Always Impute","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2101-the-core-problem-confusing-complete-database-with-correct-database","title":"2.10.1. The Core Problem: Confusing \"Complete Database\" with \"Correct Database\"","text":"<p>One of the most serious conceptual errors made by students (and many professionals) is thinking that:</p> <p>FALSE: \"A database must be complete to be analyzed\"</p> <p>This belief comes from programming (where <code>NULL</code> causes errors) and from machine learning (where many algorithms don't accept <code>NaN</code>). But in real data analysis, especially in surveys and Big Data, missing data IS INFORMATION, not a problem to \"fix\".</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#the-uncomfortable-truth","title":"The Uncomfortable Truth:","text":"Wrong Belief Reality \"I must fill all NAs\" NO. You must understand WHY they are missing \"A complete database is better\" NO. A database with incorrect imputations is WORSE than one with NAs \"0 means no data\" IT DEPENDS. In surveys, 0 can be a valid response \"NaN and 0 are the same\" FALSE. They are conceptually different"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2102-the-fundamental-difference-0-vs-nan-vs-no-response","title":"2.10.2. The Fundamental Difference: 0 vs NaN vs \"No Response\"","text":"<p>Imagine a survey about income and expenses:</p> Question Person A's Response Person B's Response Person C's Response \"How many cigarettes do you smoke per day?\" 0 (doesn't smoke) NaN (refused to answer) 20 (smokes 20) \"How much did you spend on tobacco this month?\" 0 (consistent: doesn't smoke) NaN (didn't respond) 150 (consistent: smokes)"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#analysis-of-each-case","title":"Analysis of Each Case:","text":"<p>Person A - The 0 is VALID: - <code>cigarettes = 0</code> means \"DOES NOT SMOKE\", it is a real response - <code>tobacco_expense = 0</code> is consistent with not smoking - IMPUTING these 0s with the mean would be CATASTROPHIC: you would turn a non-smoker into an average smoker</p> <p>Person B - The NaN is INFORMATIVE: - <code>cigarettes = NaN</code> means \"REFUSED TO ANSWER\" - It could be because:   - Smokes illegally (underage)   - Embarrassed to admit smoking heavily   - Simply doesn't want to share that information - IMPUTING with the mean assumes they are an average smoker, which may be FALSE</p> <p>Person C - Complete Data: - Answered everything, no ambiguity</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2103-sampling-concepts-why-missing-data-is-part-of-the-design","title":"2.10.3. Sampling Concepts: Why Missing Data is Part of the Design","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#the-survey-context","title":"The Survey Context","text":"<p>When working with surveys or Big Data databases, missing data is a natural consequence of the collection process. According to sampling theory (see TodoEconometria - Sampling Strategies):</p> <p>Simple Random Sampling: The selection of a representative sample is highly complex. For example, if we want to study people with type 2 diabetes, not everyone will answer every question, and that is expected and valid.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#the-three-real-scenarios","title":"The Three Real Scenarios:","text":"<p>Scenario 1: Control Questions (Filters)</p> <p>In well-designed surveys, there are questions that intentionally generate <code>NaN</code> in other variables:</p> <pre><code>Q1: \"Do you own a car?\"\n    [ ] Yes  -&gt; Go to Q2\n    [ ] No   -&gt; Skip to Q5\n\nQ2: \"What brand is your car?\" (Only if answered Yes in Q1)\n    _____________\n</code></pre> <p>Result in the database:</p> Person Has_Car Car_Brand John Yes Toyota Mary No NaN <p>Question: Should we impute Mary's car brand?</p> <p>Answer: NO. The <code>NaN</code> is correct. Mary doesn't have a car, therefore she cannot have a brand. Imputing \"Toyota\" (the mean/mode) would be absurd.</p> <p>Scenario 2: Non-Response Due to Sensitivity</p> <p>Some questions are sensitive:</p> <pre><code>Q10: \"What is your monthly income?\"\n     [ ] Less than $1000\n     [ ] $1000 - $3000\n     [ ] $3000 - $5000\n     [ ] More than $5000\n     [ ] Prefer not to answer\n</code></pre> <p>Result:</p> Person Income Peter \\(1000-\\)3000 Ana NaN (prefers not to answer) <p>Question: Should we impute Ana's income with the median?</p> <p>Answer: IT DEPENDS on the missingness type: - If Ana is wealthy and doesn't want to disclose it: MNAR (data is missing because it's high) - If Ana randomly chose \"prefer not to answer\": MCAR (random)</p> <p>Imputing with the median in the MNAR case will introduce downward bias (we assume that the wealthy who don't respond have average income).</p> <p>Scenario 3: Big Data with Sensors</p> <p>In IoT databases, sensors, or logs:</p> <pre><code>Temperature Sensor:\nTime  | Temperature\n------|------------\n10:00 | 25.3\u00b0C\n10:05 | 25.5\u00b0C\n10:10 | NaN  (sensor failed)\n10:15 | 26.1\u00b0C\n</code></pre> <p>Question: Should we impute 10:10 with the mean of 10:05 and 10:15?</p> <p>Answer: IT DEPENDS: - If the sensor fails randomly: MCAR -&gt; Linear imputation is reasonable - If the sensor fails when it's very hot (overheats): MNAR -&gt; Imputing with the mean will underestimate the actual temperature</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2104-the-conceptual-error-i-need-complete-data-to-analyze","title":"2.10.4. The Conceptual Error: \"I Need Complete Data to Analyze\"","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#myth-1-algorithms-dont-accept-nan","title":"Myth 1: \"Algorithms don't accept NaN\"","text":"<p>Reality: - Many algorithms DO accept NaN: decision trees, Random Forest, XGBoost - Those that don't accept NaN (linear regression, classic PCA) have robust versions:   - PCA with <code>prince</code> accepts NA   - Regression with <code>statsmodels</code> has <code>missing='drop'</code></p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#myth-2-more-data-better-model","title":"Myth 2: \"More data = Better model\"","text":"<p>Reality: - Incorrect data (poorly imputed) are worse than fewer correct data - Example:</p> <pre><code># Option A: Impute with mean (BAD if MNAR)\ndf['income'].fillna(df['income'].mean())  # 1000 rows, but with bias\n\n# Option B: Drop rows with NA (GOOD if few)\ndf.dropna(subset=['income'])  # 800 rows, but without bias\n</code></pre> <p>Which is better? It depends on the % of NAs: - If NA &lt; 5%: Dropping is safe - If NA &gt; 20%: Investigate why they are missing before deciding</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2105-practical-case-consumer-habits-survey","title":"2.10.5. Practical Case: Consumer Habits Survey","text":"<p>Imagine this database:</p> Person Smokes Cigarettes_Day Tobacco_Expense Drinks_Alcohol Drinks_Week A No 0 0 Yes 3 B Yes 10 50 No 0 C Yes NaN NaN Yes NaN D No 0 0 No 0"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#correct-analysis","title":"Correct Analysis:","text":"<p>Person A: - <code>Cigarettes_Day = 0</code> is VALID (doesn't smoke) - <code>Tobacco_Expense = 0</code> is VALID (consistent) - DO NOT IMPUTE</p> <p>Person B: - <code>Drinks_Week = 0</code> is VALID (doesn't drink) - DO NOT IMPUTE</p> <p>Person C: - <code>Cigarettes_Day = NaN</code> is PROBLEMATIC - Options:   1. Drop row (if it's the only one with NA)   2. Impute with KNN (find similar smokers)   3. Leave as NA and use algorithms that support it</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#incorrect-analysis-common-among-beginners","title":"INCORRECT Analysis (Common Among Beginners):","text":"<pre><code># ERROR: Impute EVERYTHING with the mean\ndf.fillna(df.mean())\n</code></pre> <p>Consequence: - Person A (non-smoker) now has <code>Cigarettes_Day = 5</code> (mean of B and C) - Person D (non-smoker, non-drinker) now smokes and drinks - The database is DESTROYED</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2106-golden-rules-for-students","title":"2.10.6. Golden Rules for Students","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#rule-1-before-touching-an-na-ask-yourself","title":"Rule 1: Before Touching an NA, Ask Yourself:","text":"<pre><code>1. Why is this data missing?\n   - Filter question? -&gt; NA is CORRECT, DO NOT impute\n   - Non-response? -&gt; Investigate type (MCAR/MAR/MNAR)\n   - Technical error? -&gt; Could be MCAR\n\n2. Is it a 0 or a NaN?\n   - 0 = Valid response (\"I don't have\", \"I don't do\")\n   - NaN = Absence of response\n\n3. What happens if I impute incorrectly?\n   - Bias in results\n   - Erroneous conclusions\n   - Invalid model\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#rule-2-decision-workflow","title":"Rule 2: Decision Workflow","text":"<pre><code>STEP 1: Count NAs\n   df.isna().sum()\n   -&gt; If &lt; 5%: Consider dropping rows\n   -&gt; If &gt; 20%: Investigate pattern\n\nSTEP 2: Visualize pattern\n   import missingno as msno\n   msno.matrix(df)\n   -&gt; Random? -&gt; MCAR\n   -&gt; Clustered? -&gt; MAR/MNAR\n\nSTEP 3: Statistical test\n   from pyampute.exploration import mcar_test\n   mcar_test(df)\n   -&gt; p &gt; 0.05: MCAR (you can impute)\n   -&gt; p &lt; 0.05: MAR/MNAR (be careful)\n\nSTEP 4: Decide strategy\n   - MCAR + few NAs: Mean or drop\n   - MAR: KNN or MICE\n   - MNAR: DO NOT impute or use specific model\n   - Filter question: LEAVE as NA\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#rule-3-always-document","title":"Rule 3: ALWAYS Document","text":"<pre><code># GOOD PRACTICE\nprint(\"=\"*50)\nprint(\"MISSING DATA REPORT\")\nprint(\"=\"*50)\nprint(f\"Total observations: {len(df)}\")\nprint(f\"Variables with NA: {df.isna().any().sum()}\")\nprint(f\"\\nDetail by variable:\")\nprint(df.isna().sum())\nprint(f\"\\nPercentage of NAs:\")\nprint((df.isna().sum() / len(df) * 100).round(2))\nprint(\"\\nStrategy applied:\")\nprint(\"- Variables with valid 0: ['Cigarettes_Day', 'Drinks_Week']\")\nprint(\"- Variables imputed with KNN: ['Income']\")\nprint(\"- Rows removed: 3 (0.5% of total)\")\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2107-full-example-correct-vs-incorrect-code","title":"2.10.7. Full Example: Correct vs Incorrect Code","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#incorrect-code-beginner","title":"INCORRECT Code (Beginner):","text":"<pre><code>import pandas as pd\n\n# Load data\ndf = pd.read_csv('survey.csv')\n\n# ERROR: Impute EVERYTHING with the mean\ndf = df.fillna(df.mean())\n\n# ERROR: Don't verify what happened\nprint(\"Done, complete database!\")\n</code></pre> <p>Problems: 1. Doesn't distinguish between valid 0 and NaN 2. Doesn't verify missingness type 3. Doesn't document what was done 4. May have destroyed the real structure</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#correct-code-professional","title":"CORRECT Code (Professional):","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# 1. Load and explore\ndf = pd.read_csv('survey.csv')\nprint(\"INITIAL DIAGNOSIS\")\nprint(\"=\"*50)\nprint(df.info())\nprint(\"\\nMissing data:\")\nprint(df.isna().sum())\n\n# 2. Visualize pattern\nmsno.matrix(df)\nplt.title(\"Missing Data Pattern\")\nplt.show()\n\n# 3. Separate variables with valid 0 from variables with NA\n# Variables where 0 is valid (do not impute)\nvars_with_valid_zero = ['Cigarettes_Day', 'Tobacco_Expense', 'Drinks_Week']\n\n# Variables where NA must be treated\nvars_to_impute = ['Income', 'Age', 'Work_Hours']\n\n# 4. Verify consistency (0 in dependent variables)\n# Example: If doesn't smoke, tobacco expense must be 0\nmask_non_smoker = df['Smokes'] == 'No'\nassert (df.loc[mask_non_smoker, 'Cigarettes_Day'] == 0).all(), \\\n    \"ERROR: There are non-smokers with cigarettes &gt; 0\"\n\n# 5. Impute only numeric variables that need it\nif df[vars_to_impute].isna().any().any():\n    print(\"\\nApplying KNN Imputer to:\", vars_to_impute)\n    imputer = KNNImputer(n_neighbors=5)\n    df[vars_to_impute] = imputer.fit_transform(df[vars_to_impute])\n    print(\"Imputation completed\")\n\n# 6. Document\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL REPORT\")\nprint(\"=\"*50)\nprint(f\"Total rows: {len(df)}\")\nprint(f\"Variables with valid 0 (NOT imputed): {vars_with_valid_zero}\")\nprint(f\"Variables imputed with KNN: {vars_to_impute}\")\nprint(f\"Remaining NAs: {df.isna().sum().sum()}\")\n</code></pre>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#2108-summary-the-data-analysts-philosophy","title":"2.10.8. Summary: The Data Analyst's Philosophy","text":"<p>\"It's not your job to fill gaps. It's your job to UNDERSTAND why there are gaps.\"</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#fundamental-principles","title":"Fundamental Principles:","text":"<ol> <li>Missing data is information, not an error</li> <li>0 is a valid response, not missing data</li> <li>NaN means \"unknown\", and sometimes \"unknown\" is the correct answer</li> <li>Imputing incorrectly is worse than not imputing</li> <li>Document EVERYTHING you do with NAs</li> </ol>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#final-checklist-before-performing-pca-or-any-analysis","title":"Final Checklist Before Performing PCA (or Any Analysis):","text":"<ul> <li> Have I identified all variables with valid 0?</li> <li> Have I verified that the 0s are consistent with other variables?</li> <li> Have I diagnosed the missingness type (MCAR/MAR/MNAR)?</li> <li> Have I decided on a justified strategy for each variable?</li> <li> Have I documented how many NAs there were and what I did with them?</li> <li> Have I verified that the results make sense after imputation?</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#3-the-dataset-decathlon-context-and-research-questions","title":"3. The Dataset: Decathlon - Context and Research Questions","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#31-the-real-scenario-what-problem-are-we-trying-to-solve","title":"3.1. The Real Scenario: What Problem Are We Trying to Solve?","text":"<p>Imagine you are the technical director of an athletics federation. You have data from 41 athletes who competed in the decathlon (10 events) in two different competitions: the Olympic Games and the Decastar Meeting.</p> <p>You face several business/research questions:</p> Question Why it matters Are there athlete \"profiles\"? (sprinters vs throwers vs all-rounders) To design specialized training programs Which events are related to each other? To optimize cross-training Do Olympic athletes have a different profile than Decastar athletes? To understand what characterizes elite athletes Which events are \"redundant\" (measure the same thing)? To simplify evaluations Which athletes have atypical profiles? To identify unique talents or detect anomalies <p>KEY: We don't apply PCA \"just because\" or \"because it's trendy\". We apply it because we have specific questions that this technique can answer efficiently and validly.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#32-why-pca-is-the-right-tool-and-not-another","title":"3.2. Why PCA is the Right Tool (and Not Another)","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#the-mathematical-problem","title":"The Mathematical Problem","text":"<p>We have 10 numerical variables (the events). Visualizing 10 dimensions is impossible. But:</p> <ul> <li>We don't want to lose important information</li> <li>We want to see patterns (groups of athletes, relationships between events)</li> <li>We want to reduce complexity without destroying the structure</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#why-not-other-techniques","title":"Why NOT Other Techniques?","text":"Technique Why it is NOT suitable here Regression We don't have a \"target\" variable to predict. We want to explore, not predict. Clustering (K-Means) Groups individuals but doesn't explain WHICH variables differentiate them or how variables relate to each other. Simple Correlation Only compares variables 2 at a time. With 10 variables, we'd have 45 correlations. Impossible to interpret. t-SNE / UMAP Good for visualization but don't provide interpretation of the dimensions (they are \"black boxes\")."},{"location":"en/dashboards/02_PCA_FactoMineR_style/#why-use-pca","title":"Why USE PCA?","text":"PCA Advantage Application in Decathlon Reduces dimensions preserving variance From 10 events to 2-3 interpretable dimensions Identifies which variables \"go together\" Discover that 100m, long jump, and 400m are correlated Detects atypical individuals Identify athletes with unique profiles Allows supplementary variables See if \"Competition\" explains differences without contaminating the analysis Is interpretable Each axis has a meaning (e.g., \"speed vs endurance\")"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#33-the-sporting-context-of-the-decathlon","title":"3.3. The Sporting Context of the Decathlon","text":"<p>The decathlon is known as \"the king of athletics events\" because it evaluates the most complete athlete. It consists of 10 events over 2 days:</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#day-1","title":"Day 1","text":"Event Type Primary Skill 100m Race Explosive speed Long jump Jump Leg power Shot put Throw Brute strength High jump Jump Technique + power 400m Race Speed-endurance"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#day-2","title":"Day 2","text":"Event Type Primary Skill 110m hurdles Race Speed + technique Discus throw Throw Strength + technique Pole vault Jump Advanced technique Javelin throw Throw Strength + coordination 1500m Race Pure endurance <p>Prior hypothesis (before PCA): We expect to find that events group by \"skill type\": speed (100m, 400m, hurdles), strength (shot put, discus, javelin), and technique (pole vault, high jump). PCA will tell us whether this hypothesis is correct.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#34-the-two-competitions-why-does-it-matter","title":"3.4. The Two Competitions: Why Does It Matter?","text":"Competition Level Characteristics Olympic Games World elite Only the best in the world. High pressure. Maximum preparation. Decastar Meeting High European level Elite competitors, but not necessarily the best in the world. <p>Key question: Do Olympic athletes have a different performance profile? Or are they simply \"better at everything\"?</p> <ul> <li>If they are \"better at everything\", they will be in the same direction as Decastar athletes but farther from the origin.</li> <li>If they have a different profile, they will be in a different zone of the map.</li> </ul> <p>This question is answered by using <code>Competition</code> as a supplementary variable: it doesn't participate in the PCA calculation, but we project it to see if the groups separate.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#35-data-structure","title":"3.5. Data Structure","text":"Variable Type Variables Role in the Analysis Active 100m, Long.jump, Shot.put, High.jump, 400m, 110m.hurdle, Discus, Pole.vault, Javeline, 1500m Build the PCA axes Quantitative Supplementary Rank, Points Projected but do NOT build axes Qualitative Supplementary Competition (Decastar/OlympicG) For coloring and segmenting"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#36-key-concept-active-vs-supplementary-variables","title":"3.6. Key Concept: Active vs. Supplementary Variables","text":"<p>This is one of the most powerful concepts in FactoMineR:</p> <p>Active Variables: Participate in the mathematical calculation of principal components. They are the ones that \"build\" the axes. In our case, the 10 sporting events.</p> <p>Supplementary Variables: Do NOT participate in the calculation, but are projected onto the factorial space to see how they relate to the found dimensions.</p> <p>Why is it useful? Imagine you want to know whether the type of competition (Olympic Games vs Decastar) influences the athletes' performance profile. If you include \"Competition\" as an active variable, you contaminate the analysis (you introduce a categorical variable in a numerical calculation). But if you use it as a supplementary variable, you can see if the groups separate without having forced that separation.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#37-other-classic-pca-datasets-and-their-contexts","title":"3.7. Other Classic PCA Datasets and Their Contexts","text":"<p>So you understand that PCA is applied in many domains, here are other classic datasets:</p> Dataset Context Research Question Variables Iris (Fisher, 1936) Botany Can petal/sepal measurements distinguish flower species? 4 flower measurements, 3 species Wine (UCI) Oenology What chemical properties characterize wines from different regions? 13 chemical properties, 3 origins Breast Cancer (UCI) Medicine What cell measurements distinguish benign from malignant tumors? 30 cell nucleus measurements MNIST (LeCun) Computer vision Can 784-pixel images be reduced to few dimensions? 784 pixels, 10 digits Social surveys Sociology What latent dimensions explain political opinions? Many survey questions <p>Common pattern: In all cases, we have many numerical variables and want to discover latent structure (hidden dimensions that explain the variation).</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#38-checklist-before-applying-pca-ask-yourself","title":"3.8. Checklist: Before Applying PCA, Ask Yourself...","text":"<ul> <li> Do I have a clear question? \"I want to explore patterns\" is valid. \"I want to predict Y\" is not for PCA.</li> <li> Are my variables numerical? PCA requires quantitative variables (or convertible to numerical).</li> <li> Do I have enough observations? Rule of thumb: at least 5-10 observations per variable.</li> <li> Are the variables on comparable scales? If not, I must standardize (Prince does this by default).</li> <li> Have I handled missing data? (See section 2)</li> <li> Do I have supplementary variables to validate? Very useful for interpreting results.</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#4-analysis-results-chart-panel","title":"4. Analysis Results: Chart Panel","text":"<p>Below is the complete chart panel generated by the <code>02_PCA_FactoMineR_style.py</code> script:</p> <p></p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#5-detailed-interpretation-of-each-chart","title":"5. Detailed Interpretation of Each Chart","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#41-scree-plot","title":"4.1. SCREE PLOT","text":"<p>Location: Upper left panel</p> <p>What it shows: The variance explained by each principal component.</p> <p></p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#reading-the-results","title":"Reading the Results:","text":"Dimension Explained Variance Cumulative Variance Dim.1 16.7% 16.7% Dim.2 14.5% 31.2% Dim.3 12.8% 44.0% Dim.4 11.9% 55.9% Dim.5 9.6% 65.5%"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation","title":"Interpretation:","text":"<ol> <li> <p>The gray dashed line (Threshold = 10%) represents the expected contribution if all variables were independent (100% / 10 variables = 10%). Dimensions above this line contribute more information than the average.</p> </li> <li> <p>The first 4 dimensions have eigenvalues greater than 1 (Kaiser Rule), suggesting retaining 4 components.</p> </li> <li> <p>The red curve (cumulative) shows that with 5 dimensions we capture 65.5% of the total variability. This is relatively low, indicating that the decathlon is a multidimensional sport where no single \"factor\" explains all performance.</p> </li> </ol> <p>Sporting Insight: Unlike sports such as swimming (where speed explains almost everything), the decathlon requires multiple independent skills: speed, strength, endurance, technique. That's why no dimension clearly dominates.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#42-correlation-circle-the-most-important-chart","title":"4.2. CORRELATION CIRCLE (The Most Important Chart)","text":"<p>Location: Upper center panel</p> <p>This is the most important chart in FactoMineR. It shows how variables relate to each other and to the axes.</p> <p></p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#how-to-read-the-circle","title":"How to Read the Circle:","text":"Situation in the Chart Meaning Arrow near the circle (edge) Variable well represented in the plane Short arrow (near the center) Variable poorly represented, its variance is in other dimensions Arrows in the same direction Variables positively correlated Opposite arrows (180 degrees) Variables negatively correlated Perpendicular arrows (90 degrees) Variables not correlated"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation-of-our-results","title":"Interpretation of Our Results:","text":"<p>Group 1 - Lower Left Quadrant (Speed/Agility): - <code>Long.jump</code> (long jump): Long arrow to the left - <code>400m</code>: Similar direction - <code>110m.hurdle</code>: Similar direction</p> <p>Interpretation: These events are positively correlated with each other. An athlete good at long jump tends to be good at 400m and hurdles. This makes sense: they all require explosive speed and leg power.</p> <p>Group 2 - Upper Left Quadrant (Endurance): - <code>1500m</code>: Arrow toward upper-left - <code>Shot.put</code> (shot put): Upward</p> <p>Interpretation: The 1500m (endurance) and shot put appear in different directions from the speed group. This suggests that endurance is a distinct skill from explosive speed.</p> <p>Group 3 - Lower Right Quadrant: - <code>Pole.vault</code> (pole vault): Long arrow downward - <code>Discus</code>: Similar direction</p> <p>Interpretation: These technical events form their own group.</p> <p>Special Variable - <code>Long.jump</code>: - It is the longest arrow toward the left - Correlation with Dim.1: -0.74 (very high)</p> <p>Conclusion: Long jump is the variable that best defines the first dimension. An athlete with a high score on Dim.1 (right side of the map) tends to have a worse long jump.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#43-individual-map","title":"4.3. INDIVIDUAL MAP","text":"<p>Location: Upper right panel</p> <p>Shows where each athlete is located in the space of the first two dimensions, colored by the supplementary variable <code>Competition</code>.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#reading-the-chart","title":"Reading the Chart:","text":"<ul> <li>Blue points: Athletes from the Decastar competition</li> <li>Red points: Athletes from the Olympic Games (OlympicG)</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation_1","title":"Interpretation:","text":"<ol> <li> <p>General spread: Athletes are widely distributed across the plane, without forming very compact groups. This confirms that there is diversity of profiles in the decathlon.</p> </li> <li> <p>Separation by competition: Observe that red points (Olympic) tend to be more dispersed and some are in extreme positions (e.g., BERNARD at the far right). Olympic athletes show greater variability in their profiles.</p> </li> <li> <p>Notable athletes:</p> </li> <li>BERNARD (far right): Very distinctive profile, different from the average</li> <li>Zsivoczky (below): Another atypical profile</li> <li> <p>BOURGUIGNON (upper left): Different profile</p> </li> <li> <p>Combined interpretation with the circle:</p> </li> <li>Athletes on the left of the map tend to be good at <code>Long.jump</code>, <code>400m</code>, <code>110m.hurdle</code> (speed events)</li> <li>Athletes above tend to be good at <code>Shot.put</code>, <code>1500m</code> (strength/endurance)</li> </ol>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#44-variable-contributions-to-dim1","title":"4.4. VARIABLE CONTRIBUTIONS TO DIM.1","text":"<p>Location: Lower left panel</p> <p>This chart answers the question: Which variables \"built\" the first axis?</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#results","title":"Results:","text":"Variable Contribution to Dim.1 Long.jump 32.9% 1500m 19.9% 400m 12.0% 110m.hurdle 10.8% High.jump 7.6% Javeline 6.6% Shot.put 5.0% 100m 4.0% Pole.vault 1.1% Discus 0.1%"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation_2","title":"Interpretation:","text":"<ul> <li>The dashed red line marks the expected contribution threshold (10% = 100%/10 variables)</li> <li>The red bars (above threshold) are the variables that truly define that dimension</li> <li>The blue bars (below threshold) contribute less than expected</li> </ul> <p>Conclusion for Dim.1: This dimension is dominated by <code>Long.jump</code> (32.9%), followed by <code>1500m</code> (19.9%) and <code>400m</code> (12.0%). This suggests that Dim.1 represents a continuum between explosive athletes (good at long jump, 400m) vs endurance athletes (1500m).</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#45-variable-contributions-to-dim2","title":"4.5. VARIABLE CONTRIBUTIONS TO DIM.2","text":"<p>Location: Lower center panel</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#results_1","title":"Results:","text":"Variable Contribution to Dim.2 Shot.put 34.1% Pole.vault 30.7% 1500m 10.6% High.jump 10.1% Javeline 8.7% Discus 3.0% Long.jump 1.2% 100m 0.9% 400m 0.7% 110m.hurdle 0.2%"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation_3","title":"Interpretation:","text":"<p>Conclusion for Dim.2: This dimension is dominated by <code>Shot.put</code> (34.1%) and <code>Pole.vault</code> (30.7%). It represents a contrast between throwing athletes (brute strength) vs technical athletes (pole vault).</p> <p>Important note: Observe that <code>Shot.put</code> and <code>Pole.vault</code> point in opposite directions in the correlation circle. This means: - Athletes above on the map: Good at shot put - Athletes below on the map: Good at pole vault</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#46-biplot-individuals-variables","title":"4.6. BIPLOT (Individuals + Variables)","text":"<p>Location: Lower right panel</p> <p>The biplot combines the individual map with variable arrows, allowing you to directly see which variables characterize which athletes.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#how-to-read-it","title":"How to Read It:","text":"<ol> <li>Mentally project each athlete onto each variable arrow</li> <li>If the projection falls in the direction of the arrow: the athlete has a high value in that variable</li> <li>If the projection falls in the opposite direction: the athlete has a low value in that variable</li> </ol>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation-example","title":"Interpretation Example:","text":"<ul> <li>BERNARD (far right):</li> <li>Projection onto <code>Long.jump</code> (left): opposite = poor long jump</li> <li> <p>Projection onto <code>Shot.put</code> (up): perpendicular = average</p> </li> <li> <p>Athletes upper-left:</p> </li> <li>Good at <code>1500m</code> and <code>Shot.put</code></li> <li>Endurance and strength profile</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#6-advanced-factominer-metrics","title":"6. Advanced FactoMineR Metrics","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#51-quality-of-representation-cos2","title":"5.1. Quality of Representation (\\(\\cos^2\\))","text":"<p>The \\(\\cos^2\\) measures how well represented a point is in the factorial plane.</p> \\[\\cos^2 = \\frac{\\text{distance to origin in the plane}^2}{\\text{distance to origin in the original space}^2}\\]"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#interpretation_4","title":"Interpretation:","text":"\\(\\cos^2\\) Value Meaning Close to 1 The point is perfectly represented in the plane Close to 0 The point is poorly represented, its information is in other dimensions"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#results-for-athletes-top-5-best-represented","title":"Results for Athletes (Top 5 Best Represented):","text":"Athlete \\(\\cos^2\\) in Dim1+Dim2 Interpretation Bourguignon 0.755 75.5% of its variability captured BERNARD 0.725 72.5% of its variability captured Warners 0.717 71.7% of its variability captured Lorenzo 0.679 67.9% of its variability captured Clay 0.638 63.8% of its variability captured <p>Practical application: If you are going to interpret a specific athlete's position, first check their \\(\\cos^2\\). If it is low (&lt; 0.5), their position on the map may be misleading.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#52-contributions","title":"5.2. Contributions","text":"<p>Contributions indicate how much each individual/variable contributes to the construction of an axis.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#for-variables","title":"For Variables:","text":"<p>We already saw that <code>Long.jump</code> contributes 32.9% to Dim.1. This means that if we removed long jump from the analysis, the first dimension would change drastically.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#for-individuals","title":"For Individuals:","text":"Athlete Contribution to Dim.1 BERNARD 12.82% Warners 12.42% Clay 8.62% <p>Interpretation: BERNARD and Warners are \"influential\" athletes who \"pull\" the axis toward their positions. They are athletes with extreme profiles.</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#7-rpython-equivalence-table","title":"7. R/Python Equivalence Table","text":"<p>For those coming from R who want to replicate FactoMineR analyses:</p> FactoMineR (R) Prince (Python) Description <code>res.pca$eig</code> <code>pca.eigenvalues_summary</code> Eigenvalue table <code>res.pca$ind$coord</code> <code>pca.row_coordinates(df)</code> Individual coordinates <code>res.pca$ind$contrib</code> <code>pca.row_contributions_</code> Individual contributions <code>res.pca$ind$cos2</code> <code>pca.row_cosine_similarities(df)</code> Individual cos2 <code>res.pca$var$coord</code> <code>pca.column_coordinates</code> Variable coordinates <code>res.pca$var$cor</code> <code>pca.column_correlations</code> Variable correlations <code>res.pca$var$contrib</code> <code>pca.column_contributions_</code> Variable contributions <code>res.pca$var$cos2</code> <code>pca.column_cosine_similarities_</code> Variable cos2"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#8-decathlon-analysis-conclusions","title":"8. Decathlon Analysis Conclusions","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#71-main-findings","title":"7.1. Main Findings","text":"<ol> <li> <p>The decathlon is multidimensional: No dimension explains more than 17% of the variance. At least 4 components are needed to capture 56% of the information.</p> </li> <li> <p>Dimension 1 - Speed/Agility vs Endurance:</p> </li> <li>Dominated by: <code>Long.jump</code> (32.9%), <code>1500m</code> (19.9%), <code>400m</code> (12.0%)</li> <li> <p>Separates explosive athletes from endurance athletes</p> </li> <li> <p>Dimension 2 - Strength vs Technique:</p> </li> <li>Dominated by: <code>Shot.put</code> (34.1%), <code>Pole.vault</code> (30.7%)</li> <li> <p>Separates throwing athletes from technical athletes</p> </li> <li> <p>Supplementary variable (Competition):</p> </li> <li>Olympic athletes show greater variability in their profiles</li> <li>There is no clear separation between competitions, suggesting that the level of competition does not determine the profile type</li> </ol>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#72-practical-implications","title":"7.2. Practical Implications","text":"<ul> <li>For coaches: Identifying which quadrant an athlete falls in helps design personalized training programs</li> <li>For analysts: Contributions reveal which variables are most important for characterizing overall performance</li> </ul>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#9-generated-files","title":"9. Generated Files","text":"File Description <code>02_PCA_FactoMineR_style.py</code> Complete Python code <code>02_PCA_FactoMineR_style.md</code> This manual <code>02_PCA_FactoMineR_graficos.png</code> Panel of 6 charts <code>02_PCA_circulo_correlacion.png</code> Individual correlation circle <code>02_PCA_resultados_FactoMineR.xlsx</code> Excel with 7 result sheets"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#10-proposed-exercise-for-the-student","title":"10. Proposed Exercise for the Student","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#comprehension-questions","title":"Comprehension Questions:","text":"<ol> <li> <p>If an athlete has a negative coordinate on Dim.1, what can you infer about their long jump?</p> </li> <li> <p>Why does <code>Discus</code> have an almost null contribution (0.1%) to Dim.1?</p> </li> <li> <p>If an athlete's \\(\\cos^2\\) in the Dim1-Dim2 plane is 0.30, should we trust their position on the map? Why?</p> </li> <li> <p>Looking at the biplot, what type of profile does athlete YURKOV (above on the map) have?</p> </li> </ol>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#answers","title":"Answers:","text":"<ol> <li> <p>They have a good long jump (Long.jump's correlation with Dim.1 is negative: -0.74)</p> </li> <li> <p>Because Discus variability is mainly represented in Dim.3 (contribution 39.4%), not in Dim.1</p> </li> <li> <p>We should not trust it much. Only 30% of their variability is captured in that plane. The remaining 70% is in other dimensions.</p> </li> <li> <p>YURKOV has high values in the variables that point upward (<code>Shot.put</code>, <code>1500m</code>, <code>High.jump</code>): a strength and endurance profile.</p> </li> </ol>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#11-academic-references","title":"11. Academic References","text":""},{"location":"en/dashboards/02_PCA_FactoMineR_style/#main-tutorial-reference","title":"Main Tutorial Reference","text":"<p>FactoMineR Tutorial Husson, F., &amp; Josse, J. http://factominer.free.fr/course/FactoTuto.html \"This course presents the main methods used in Exploratory Data Analysis with the FactoMineR package.\"</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#original-factominer-article","title":"Original FactoMineR Article","text":"<p>Le, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: An R Package for Multivariate Analysis. Journal of Statistical Software, 25(1), 1-18. DOI: 10.18637/jss.v025.i01</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#reference-book","title":"Reference Book","text":"<p>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. 2<sup>nd</sup> Edition. CRC Press. ISBN: 978-1138196346</p>"},{"location":"en/dashboards/02_PCA_FactoMineR_style/#institutional-information","title":"Institutional Information","text":"<p>Author/Reference: @TodoEconometria Professor: Juan Marcelo Gutierrez Miranda Area: Big Data, Data Science &amp; Applied Econometrics.</p> <p>Certification Hash ID: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code></p>"},{"location":"en/dashboards/02_pca_iris_clustering/","title":"PCA + K-Means Clustering: Iris Dataset","text":"<p>Author: @TodoEconometria | Professor: Juan Marcelo Gutierrez Miranda</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>The Iris Dataset: A Machine Learning Classic</li> <li>Why Combine PCA + Clustering</li> <li>Principal Component Analysis (PCA)</li> <li>K-Means Clustering</li> <li>Interpretation of Results</li> <li>Conclusions and Recommendations</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#1-introduction","title":"1. Introduction","text":"<p>This document presents a complete analysis of the famous Iris dataset combining two fundamental unsupervised Machine Learning techniques:</p> <ul> <li>PCA (Principal Component Analysis): Dimensionality reduction</li> <li>K-Means Clustering: Observation grouping</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#analysis-objectives","title":"Analysis Objectives","text":"<ol> <li>Reduce the 4 original dimensions to 2 principal dimensions</li> <li>Identify natural groups in the data (flower species)</li> <li>Visualize patterns and relationships in a 2D space</li> <li>Validate whether unsupervised clustering can discover the 3 known species</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#2-the-iris-dataset-a-machine-learning-classic","title":"2. The Iris Dataset: A Machine Learning Classic","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#history-and-context","title":"History and Context","text":"<p>The Iris dataset was introduced by Ronald Fisher in 1936 in his seminal paper:</p> <p>Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.</p> <p>It is one of the most widely used datasets in:</p> <ul> <li>Machine Learning education</li> <li>Classification algorithm validation</li> <li>Data visualization examples</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#dataset-description","title":"Dataset Description","text":"Feature Description Observations 150 flowers Species 3 (Setosa, Versicolor, Virginica) Variables 4 measurements in centimeters Distribution 50 flowers per species (balanced)"},{"location":"en/dashboards/02_pca_iris_clustering/#measured-variables","title":"Measured Variables","text":"<ol> <li>Sepal Length</li> <li>Sepal Width</li> <li>Petal Length</li> <li>Petal Width</li> </ol> <p>BOTANICAL NOTE: The sepal is the green part that protects the flower before it opens. The petal is the colorful part of the flower.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#why-is-this-dataset-important","title":"Why Is This Dataset Important?","text":"<ol> <li>Manageable Size: 150 observations are sufficient for learning without being overwhelming</li> <li>Well Balanced: 50 flowers of each species (no class imbalance)</li> <li>Separability: One species (Setosa) is linearly separable, the other two slightly overlap</li> <li>Multivariate: 4 variables allow practicing dimensionality reduction techniques</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#3-why-combine-pca-clustering","title":"3. Why Combine PCA + Clustering","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#the-dimensionality-problem","title":"The Dimensionality Problem","text":"<p>When we have more than 3 dimensions, it is impossible to visualize the data directly:</p> <ul> <li>1D: Line (easy)</li> <li>2D: Plane (easy)</li> <li>3D: 3D space (possible but difficult)</li> <li>4D+: Impossible to visualize</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#the-solution-pca-clustering","title":"The Solution: PCA + Clustering","text":"<pre><code>Original Data (4D)\n        \u2193\n    PCA (Reduction)\n        \u2193\nReduced Data (2D) \u2190 Now we can VISUALIZE\n        \u2193\n    K-Means (Grouping)\n        \u2193\n  Identified Clusters\n</code></pre>"},{"location":"en/dashboards/02_pca_iris_clustering/#advantages-of-this-combination","title":"Advantages of This Combination","text":"Advantage Explanation Visualization PCA reduces to 2D for plotting Noise Reduction PCA removes non-informative variance Better Clustering K-Means works better in lower-dimensional spaces Interpretability We can see and understand clusters in 2D"},{"location":"en/dashboards/02_pca_iris_clustering/#4-principal-component-analysis-pca","title":"4. Principal Component Analysis (PCA)","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#what-is-pca","title":"What is PCA?","text":"<p>PCA is a technique that:</p> <ol> <li>Finds the directions of maximum variance in the data</li> <li>Projects the data onto those directions (principal components)</li> <li>Reduces dimensionality while retaining the most information possible</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#pca-results-on-iris","title":"PCA Results on Iris","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#explained-variance","title":"Explained Variance","text":"Dimension Eigenvalue Variance (%) Cumulative Variance (%) Dim.1 ~2.92 ~73% ~73% Dim.2 ~0.91 ~23% ~96% Dim.3 ~0.15 ~4% ~99% Dim.4 ~0.02 ~1% ~100% <p>INTERPRETATION: The first 2 dimensions capture ~96% of the total variance. This means we can reduce from 4D to 2D losing only ~4% of information.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#kaiser-rule","title":"Kaiser Rule","text":"<p>The Kaiser Rule states: Retain components with eigenvalue &gt; 1</p> <ul> <li>Dim.1: Eigenvalue = 2.92 (Retain)</li> <li>Dim.2: Eigenvalue = 0.91 (Close to 1, retain for visualization)</li> <li>Dim.3: Eigenvalue = 0.15 (Discard)</li> <li>Dim.4: Eigenvalue = 0.02 (Discard)</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#interpretation-of-the-dimensions","title":"Interpretation of the Dimensions","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#dimension-1-73-of-variance","title":"Dimension 1 (~73% of variance)","text":"<p>Most contributing variables:</p> <ul> <li>Petal Length (~42%)</li> <li>Petal Width (~42%)</li> </ul> <p>Interpretation:</p> <p>Dim.1 represents \"petal size\". Flowers with high Dim.1 values have large petals; low values have small petals.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#dimension-2-23-of-variance","title":"Dimension 2 (~23% of variance)","text":"<p>Most contributing variables:</p> <ul> <li>Sepal Width (~72%)</li> </ul> <p>Interpretation:</p> <p>Dim.2 represents \"sepal width\". Flowers with high Dim.2 values have wide sepals; low values have narrow sepals.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#correlation-circle","title":"Correlation Circle","text":"<p>The correlation circle shows how original variables relate to the principal dimensions:</p> <pre><code>           Dim.2 (Sepal Width)\n                 \u2191\n                 |\n    Sepal Width  |\n         \u2191       |\n         |       |\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Dim.1 (Petal Size)\n         |       |\n         |   Petal Length \u2192\n         |   Petal Width \u2192\n         |\n</code></pre> <p>Observations:</p> <ul> <li>Petal Length and Petal Width are highly correlated (arrows in the same direction)</li> <li>Sepal Width is nearly perpendicular to petal measurements (low correlation)</li> <li>Sepal Length is between both dimensions</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#5-k-means-clustering","title":"5. K-Means Clustering","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#what-is-k-means","title":"What is K-Means?","text":"<p>K-Means is a clustering algorithm that:</p> <ol> <li>Divides the data into K groups (clusters)</li> <li>Minimizes the distance of each point to its centroid</li> <li>Iterates until convergence</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#determining-the-optimal-number-of-clusters","title":"Determining the Optimal Number of Clusters","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#elbow-method","title":"Elbow Method","text":"<p>We plot inertia (sum of squared distances) vs K:</p> <pre><code>Inertia\n  \u2502\n  \u2502 \u25cf\n  \u2502   \u25cf\n  \u2502     \u25cf  \u2190 \"Elbow\" at K=3\n  \u2502       \u25cf\n  \u2502         \u25cf\n  \u2502           \u25cf\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 K\n    2  3  4  5  6  7\n</code></pre> <p>Interpretation: The \"elbow\" is at K=3, suggesting 3 clusters.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#silhouette-score","title":"Silhouette Score","text":"<p>The Silhouette Score measures how well separated the clusters are:</p> <ul> <li>Value: Between -1 and 1</li> <li>Interpretation:</li> <li>Close to 1: Well separated clusters</li> <li>Close to 0: Overlapping clusters</li> <li>Negative: Misassigned points</li> </ul> <p>Result for Iris: Silhouette Score ~ 0.55 (good separation)</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#clustering-results","title":"Clustering Results","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#confusion-matrix-clusters-vs-real-species","title":"Confusion Matrix: Clusters vs Real Species","text":"Cluster 0 Cluster 1 Cluster 2 Setosa 50 0 0 Versicolor 0 48 2 Virginica 0 14 36 <p>Observations:</p> <ul> <li>Setosa: Perfectly separated (100% in Cluster 0)</li> <li>Versicolor: Mostly in Cluster 1 (96%)</li> <li>Virginica: Mostly in Cluster 2 (72%), but with overlap with Versicolor</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#cluster-purity","title":"Cluster Purity","text":"<p>Purity measures the percentage of correctly grouped observations:</p> <pre><code>Purity = (50 + 48 + 36) / 150 = 89.3%\n</code></pre> <p>INTERPRETATION: The K-Means algorithm correctly identified the species in 89.3% of cases, without knowing the real labels. This is excellent for an unsupervised method.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#cluster-visualization","title":"Cluster Visualization","text":"<p>In the PCA 2D space, the clusters look like this:</p> <pre><code>     Dim.2\n       \u2191\n       \u2502     \u25cf Cluster 2 (Virginica)\n       \u2502    \u25cf\u25cf\u25cf\n       \u2502   \u25cf\u25cf\u25cf\u25cf\n       \u2502  \u25cf\u25cf\u25cf\u25cf\n       \u2502 \u25cf\u25cf\u25cf\u25cf  \u25a0\u25a0\u25a0 Cluster 1 (Versicolor)\n       \u2502\u25cf\u25cf\u25cf   \u25a0\u25a0\u25a0\u25a0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Dim.1\n       \u2502\n       \u2502  \u25b2\u25b2\u25b2\n       \u2502 \u25b2\u25b2\u25b2\u25b2\u25b2\n       \u2502\u25b2\u25b2\u25b2\u25b2\u25b2\u25b2  Cluster 0 (Setosa)\n       \u2502\n</code></pre> <p>Centroids (marked with X):</p> <ul> <li>Cluster 0: (-2.7, 0.3) \u2192 Setosa</li> <li>Cluster 1: (0.3, -0.5) \u2192 Versicolor</li> <li>Cluster 2: (1.7, 0.2) \u2192 Virginica</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#6-interpretation-of-results","title":"6. Interpretation of Results","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#full-panel-pca-k-means-clustering","title":"Full Panel: PCA + K-Means Clustering","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#analysis-by-species","title":"Analysis by Species","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#setosa-cluster-0","title":"Setosa (Cluster 0)","text":"<p>Characteristics:</p> <ul> <li>Petal Length: Very small (~1.5 cm)</li> <li>Petal Width: Very small (~0.2 cm)</li> <li>Sepal Width: Relatively large</li> </ul> <p>PCA Position:</p> <ul> <li>Dim.1: Very negative values (small petals)</li> <li>Dim.2: Positive values (wide sepals)</li> </ul> <p>Separability: Perfect (100% correctly grouped)</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#versicolor-cluster-1","title":"Versicolor (Cluster 1)","text":"<p>Characteristics:</p> <ul> <li>Petal Length: Medium (~4.3 cm)</li> <li>Petal Width: Medium (~1.3 cm)</li> <li>Sepal Width: Medium</li> </ul> <p>PCA Position:</p> <ul> <li>Dim.1: Values close to 0 (medium petals)</li> <li>Dim.2: Slightly negative values</li> </ul> <p>Separability: Good (96% correctly grouped, 4% confused with Virginica)</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#virginica-cluster-2","title":"Virginica (Cluster 2)","text":"<p>Characteristics:</p> <ul> <li>Petal Length: Large (~5.5 cm)</li> <li>Petal Width: Large (~2.0 cm)</li> <li>Sepal Width: Medium</li> </ul> <p>PCA Position:</p> <ul> <li>Dim.1: Very positive values (large petals)</li> <li>Dim.2: Values close to 0</li> </ul> <p>Separability: Moderate (72% correctly grouped, 28% confused with Versicolor)</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#evaluation-metrics","title":"Evaluation Metrics","text":"Metric Value Interpretation Silhouette Score 0.55 Good separation between clusters Davies-Bouldin Index 0.66 Compact and separated clusters (lower is better) Calinski-Harabasz Index 561.63 High separation between clusters (higher is better) Purity 89.3% High agreement with real species"},{"location":"en/dashboards/02_pca_iris_clustering/#why-do-versicolor-and-virginica-overlap","title":"Why Do Versicolor and Virginica Overlap?","text":"<p>Biological Reason:</p> <ul> <li>Versicolor and Virginica are evolutionarily closer species</li> <li>They share similar morphological characteristics</li> <li>Setosa is more distinct (probably from a different lineage)</li> </ul> <p>Statistical Reason:</p> <ul> <li>The petal measurements of Versicolor and Virginica have overlapping ranges</li> <li>There is no clear boundary in the 4-dimensional space</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#7-conclusions-and-recommendations","title":"7. Conclusions and Recommendations","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#main-conclusions","title":"Main Conclusions","text":"<ol> <li>PCA is Effective:</li> <li>Reduces from 4D to 2D while retaining 96% of the information</li> <li> <p>The first 2 dimensions are sufficient for visualization and clustering</p> </li> <li> <p>Petal Measurements are Key:</p> </li> <li>Petal Length and Petal Width are the most discriminating variables</li> <li> <p>Dim.1 (which represents petal size) explains 73% of the variance</p> </li> <li> <p>K-Means Works Well:</p> </li> <li>Correctly identifies the 3 species in 89.3% of cases</li> <li>Setosa is perfectly separable</li> <li> <p>Versicolor and Virginica have some natural overlap</p> </li> <li> <p>Validation of the Unsupervised Method:</p> </li> <li>Without knowing the labels, K-Means discovers the 3 natural groups</li> <li>This validates that the species have real morphological differences</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#lessons-for-students","title":"Lessons for Students","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#lesson-1-the-importance-of-dimensionality-reduction","title":"Lesson 1: The Importance of Dimensionality Reduction","text":"<p>BEFORE PCA: 4 variables \u2192 Hard to visualize \u2192 Hard to interpret</p> <p>AFTER PCA: 2 dimensions \u2192 Easy to visualize \u2192 Clear patterns</p> <p>Takeaway: You don't always need all the variables. Sometimes, less is more.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#lesson-2-unsupervised-clustering-can-discover-real-structure","title":"Lesson 2: Unsupervised Clustering Can Discover Real Structure","text":"<p>WITHOUT LABELS: K-Means finds 3 groups</p> <p>WITH LABELS: There are 3 real species</p> <p>MATCH: 89.3%</p> <p>Takeaway: Data has natural structure. Algorithms can find it.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#lesson-3-not-all-groups-are-perfectly-separable","title":"Lesson 3: Not All Groups Are Perfectly Separable","text":"<p>Setosa: 100% separable</p> <p>Versicolor/Virginica: Natural overlap</p> <p>Takeaway: In real data, overlap is normal. Don't expect perfect clusters.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#lesson-4-validate-validate-validate","title":"Lesson 4: Validate, Validate, Validate","text":"<p>Elbow Method: Suggests K=3</p> <p>Silhouette Score: Confirms K=3</p> <p>Purity: Validates that K=3 is correct</p> <p>Takeaway: Use multiple metrics to validate your decisions.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#practical-recommendations","title":"Practical Recommendations","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#for-iris-species-classification","title":"For Iris Species Classification","text":"<ol> <li>Focus on petal measurements (they are the most discriminating)</li> <li>Use PCA for visualization (reduces complexity without losing information)</li> <li>K=3 is optimal (validated by multiple metrics)</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#for-similar-data-analyses","title":"For Similar Data Analyses","text":"<ol> <li>Always do EDA first (understand distributions and correlations)</li> <li>Standardize before PCA (variables on different scales bias results)</li> <li>Validate the number of clusters (don't assume K, use Elbow + Silhouette)</li> <li>Compare with ground truth (if available, as in this case)</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#possible-extensions","title":"Possible Extensions","text":"<ol> <li>Other Clustering Algorithms:</li> <li>DBSCAN (for arbitrarily shaped clusters)</li> <li>Hierarchical Clustering (for dendrograms)</li> <li> <p>Gaussian Mixture Models (for probabilistic clusters)</p> </li> <li> <p>Supervised Classification:</p> </li> <li>Use the known species to train a classifier</li> <li> <p>Compare with unsupervised clustering</p> </li> <li> <p>Supplementary Variable Analysis:</p> </li> <li>Add geographic location information</li> <li>Add collection season information</li> </ol>"},{"location":"en/dashboards/02_pca_iris_clustering/#references","title":"References","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#original-papers","title":"Original Papers","text":"<ul> <li>Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188.</li> <li> <p>The original paper that introduced the Iris dataset</p> </li> <li> <p>Anderson, E. (1935). The irises of the Gaspe Peninsula. Bulletin of the American Iris Society, 59, 2-5.</p> </li> <li>The botanist who collected the original data</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#reference-books","title":"Reference Books","text":"<ul> <li>Husson, F., Le, S., &amp; Pages, J. (2017). Exploratory Multivariate Analysis by Example Using R. CRC Press.</li> <li> <p>Main reference for FactoMineR-style PCA</p> </li> <li> <p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.</p> </li> <li>Chapters on PCA and Clustering</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#technical-articles","title":"Technical Articles","text":"<ul> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> <li> <p>Documentation for the libraries used</p> </li> <li> <p>Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.</p> </li> <li>Silhouette Score method</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#additional-resources","title":"Additional Resources","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#online-tutorials","title":"Online Tutorials","text":"<ul> <li>Scikit-learn: PCA Tutorial</li> <li>Scikit-learn: K-Means Tutorial</li> <li>FactoMineR Tutorial</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#similar-datasets","title":"Similar Datasets","text":"<ul> <li>Wine Dataset: 178 wines, 13 chemical variables, 3 classes</li> <li>Breast Cancer Dataset: 569 tumors, 30 variables, 2 classes (malignant/benign)</li> <li>Digits Dataset: 1797 digit images, 64 pixels, 10 classes</li> </ul> <p>Author: @TodoEconometria Professor: Juan Marcelo Gutierrez Miranda Date: January 2026 License: Educational use with attribution</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"en/dashboards/02_pca_iris_clustering/#why-standardize-before-pca","title":"Why standardize before PCA?","text":"<p>Answer: Because PCA is sensitive to the scale of variables. If one variable has much larger values than another (e.g., income in thousands vs age in tens), it will dominate the variance and bias the results.</p>"},{"location":"en/dashboards/02_pca_iris_clustering/#how-many-components-should-i-retain","title":"How many components should I retain?","text":"<p>Answer: It depends on the objective:</p> <ul> <li>Visualization: 2-3 components</li> <li>Kaiser Rule: Components with eigenvalue &gt; 1</li> <li>Cumulative Variance: Retain until reaching 80-95% of variance</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#does-k-means-always-find-the-correct-clusters","title":"Does K-Means always find the correct clusters?","text":"<p>Answer: No. K-Means has limitations:</p> <ul> <li>Assumes spherical clusters</li> <li>Sensitive to initialization (use high <code>n_init</code>)</li> <li>Requires specifying K in advance</li> </ul>"},{"location":"en/dashboards/02_pca_iris_clustering/#what-if-i-have-more-than-3-species","title":"What if I have more than 3 species?","text":"<p>Answer: The process is the same:</p> <ol> <li>Use Elbow + Silhouette to determine the optimal K</li> <li>Validate with metrics (purity, confusion matrix)</li> <li>Visualize in 2D with PCA (even with more than 3 clusters)</li> </ol> <p>END OF DOCUMENT</p>"},{"location":"en/dashboards/04_series_temporales_arima/","title":"Time Series: ARIMA/SARIMA (Box-Jenkins)","text":"<p>Complete time series analysis following the Box-Jenkins methodology, from model identification to forecasting with confidence intervals.</p> <p>Open Interactive Dashboard</p>"},{"location":"en/dashboards/04_series_temporales_arima/#contents","title":"Contents","text":"<p>The dashboard presents 6 interactive tabs:</p> Tab Content Original Series Monthly airline passengers 1949-1960 (144 observations) Decomposition Trend + Seasonality + Residual (multiplicative) ACF / PACF Autocorrelation and partial autocorrelation of the differenced series Diagnostics Residuals, histogram, residual ACF, Q-Q plot Final Forecast Original series + SARIMA fit + 12-month forecast + 95% CI Radar Metrics RMSE, MAE, MAPE, R2 of the model"},{"location":"en/dashboards/04_series_temporales_arima/#box-jenkins-methodology","title":"Box-Jenkins Methodology","text":"<ol> <li>Identification: ACF/PACF to determine orders (p, d, q)(P, D, Q)[s]</li> <li>Estimation: Maximum likelihood fitting</li> <li>Diagnostics: Ljung-Box and Jarque-Bera tests on residuals</li> <li>Forecasting: Forecast with 95% confidence intervals</li> </ol> <p>Selected model: SARIMA(1,1,0)(0,1,0)[12] -- AIC = -445.41</p>"},{"location":"en/dashboards/04_series_temporales_arima/#source-code","title":"Source Code","text":"<ul> <li>Full script: <code>ejercicios/04_machine_learning/07_series_temporales_arima/</code></li> <li>Theoretical guide: <code>07_series_temporales_arima/README.md</code></li> </ul>"},{"location":"en/dashboards/04_similitud_jaccard/","title":"Similarity Analysis: The Web Portal Mystery","text":"<p>\"A recommendation system is like a librarian who knows exactly which magazine you'll like without having read the full content, just by looking at the words that repeat.\"</p>"},{"location":"en/dashboards/04_similitud_jaccard/#the-portal-challenge","title":"The Portal Challenge","text":"<p>Imagine you manage a dynamic portal. Your boss has challenged you: \"Group these articles automatically. I don't have time to read them all.\"</p> <p></p> <p>To solve this, we use the Jaccard Index, a mathematical tool that converts text into \"sets\" and measures how much they overlap.</p> <p></p>"},{"location":"en/dashboards/04_similitud_jaccard/#the-heart-of-the-algorithm","title":"The Heart of the Algorithm","text":"<p>The magic happens by comparing what documents share versus everything they say.</p> Attribute Visual Explanation Intersection The words that appear in BOTH texts. Union All unique words from BOTH texts. Result A number between 0 (strangers) and 1 (soulmates). <p></p>"},{"location":"en/dashboards/04_similitud_jaccard/#the-mathematical-formula","title":"The Mathematical Formula","text":""},{"location":"en/dashboards/04_similitud_jaccard/#real-results-generated-by-your-script","title":"Real Results (Generated by Your Script)","text":"<p>This is where theory meets reality. When running <code>04_similitud_jaccard.py</code>, the system \"sees\" the portal like this:</p>"},{"location":"en/dashboards/04_similitud_jaccard/#1-the-knowledge-heatmap","title":"1. The Knowledge Heatmap","text":"<p>In this matrix, warm colors (reds) indicate high similarity. Notice how squares form along the diagonal. Those are your Football, Technology, and Cooking categories automatically detected!</p> <p></p> <p></p>"},{"location":"en/dashboards/04_similitud_jaccard/#2-algorithm-validation-clustermap","title":"2. Algorithm Validation (Clustermap)","text":"<p>Can artificial intelligence group the topics without help? The Dendrogram (the side tree) tells us yes. Articles with the same theme \"seek\" each other and cluster into common branches.</p> <p></p>"},{"location":"en/dashboards/04_similitud_jaccard/#3-clustermap-with-numerical-values","title":"3. Clustermap with Numerical Values","text":"<p>Another view of the same clustering, now with exact similarity values in each cell:</p> <p></p>"},{"location":"en/dashboards/04_similitud_jaccard/#real-world-applications","title":"Real-World Applications","text":"<p>This is not just an academic exercise. This technique is used every second in:</p> <ul> <li>Plagiarism Detection: Comparing student submissions to see if they share \"too much\" vocabulary.</li> <li>Recommenders: \"If you read about the new CPU, we recommend this article about RAM memory\".</li> <li>SEO and Search Engines: To understand whether two pages discuss the same topic and avoid duplicate content.</li> </ul> <p></p>"},{"location":"en/dashboards/04_similitud_jaccard/#final-reflection-for-the-student","title":"Final Reflection for the Student","text":"<p>Look at the charts saved in your folder:</p> <ol> <li>Do you see any red point outside the diagonal? That would indicate that two different topics share words.</li> <li>What would happen if the corpus had 10,000 documents? The heatmap would become unreadable, but the Clustermap would still give us the structure.</li> </ol> <p>Certification Hash: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Author: Juan Marcelo Gutierrez Miranda (@TodoEconometria)</p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/","title":"Educational Guide: Document Vectorization and Clustering","text":""},{"location":"en/dashboards/05_vectorizacion_y_clustering/#topic-analysis-with-artificial-intelligence-nlp-machine-learning","title":"Topic Analysis with Artificial Intelligence (NLP &amp; Machine Learning)","text":""},{"location":"en/dashboards/05_vectorizacion_y_clustering/#certification-and-reference-information","title":"Certification and Reference Information","text":"<p>Original author/Reference: @TodoEconometria Professor: Juan Marcelo Gutierrez Miranda Methodology: Advanced Courses in Big Data, Data Science, AI Application Development &amp; Applied Econometrics. Certification Hash ID: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Repository: https://github.com/TodoEconometria/certificaciones</p> <p>ACADEMIC REFERENCE:</p> <ul> <li>McKinney, W. (2012). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media.</li> <li>Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362.</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.</li> </ul>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#1-laboratory-introduction","title":"1. Laboratory Introduction","text":"<p>In the world of Big Data, most information is unstructured (texts, emails, news). The fundamental challenge is: How can a computer understand that two documents discuss the same topic without reading them?</p> <p>This exercise implements a complete Data Science pipeline to automatically group 1,200 documents in Spanish, using a combination of advanced mathematical techniques to transform human language into structures that machines can process.</p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#2-theoretical-foundations-what-do-we-use-and-why","title":"2. Theoretical Foundations (What Do We Use and Why?)","text":""},{"location":"en/dashboards/05_vectorizacion_y_clustering/#a-vectorization-the-bridge-between-text-and-mathematics","title":"A. Vectorization: The Bridge Between Text and Mathematics","text":"<p>Computers don't understand words, only numbers. We use TF-IDF (Term Frequency - Inverse Document Frequency) for its ability to discern relevance.</p> <ul> <li>What is it?: A statistical value that seeks to measure how important a word is to a document within a collection (corpus).</li> <li>How does it work?:     $\\(TF(t, d) = \\frac{\\text{Count of word } t \\text{ in document } d}{\\text{Total words in } d}\\)$     $\\(IDF(t) = \\log\\left(\\frac{\\text{Total documents}}{\\text{Documents containing word } t}\\right)\\)$</li> <li>Why do we use it?: Unlike simple counting (Bag-of-Words), TF-IDF penalizes words that appear everywhere (like \"the\", \"that\", \"is\") and rewards thematic words (\"processor\", \"investment\", \"vaccine\").</li> </ul>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#b-k-means-the-clustering-brain","title":"B. K-Means: The Clustering Brain","text":"<p>For Unsupervised Learning, the K-Means algorithm is the gold standard for finding patterns without prior labels.</p> <ul> <li>The Process:<ol> <li>Define \\(k\\) random points (centroids).</li> <li>Assign each document to the nearest centroid (using Euclidean distance in vector space).</li> <li>Recalculate the group center and repeat until the groups stabilize.</li> </ol> </li> <li>Why we use it: It is extremely efficient for large data volumes and allows us to segment markets, news, or legal documents automatically.</li> </ul>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#c-pca-visualizing-hyperspace","title":"C. PCA: Visualizing Hyperspace","text":"<p>Our TF-IDF matrix has hundreds of dimensions (one per unique word). Humans can only see in 2D or 3D.</p> <ul> <li>What does it mean?: Principal Component Analysis \"compresses\" the information. It finds the directions (components) where the data varies the most and projects everything onto them.</li> <li>Didactic utility: Without PCA, the clustering would be a list of abstract numbers. With PCA, we can \"see\" the concept separation in a chart.</li> </ul>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#3-interpreting-what-we-are-seeing","title":"3. Interpreting What We Are Seeing","text":"<p>When running the code, the visualization <code>05_visualizacion_clustering.png</code> is generated.</p> <p></p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#how-to-read-this-chart","title":"How to Read This Chart?","text":"<ol> <li>Proximity is Similarity: Two points that are close together represent documents that share keywords and, therefore, topics.</li> <li>Cluster Dispersion:<ul> <li>If the groups are well separated, the algorithm has been fully successful identifying unique topics.</li> <li>If there is overlap, it indicates that some documents share vocabulary from several topics (e.g., an article about \"Technology in Healthcare\").</li> </ul> </li> <li>The Axes (PCA): The X-axis (Component 1) usually captures the biggest difference between topics (e.g., medical terms vs financial terms).</li> </ol>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#dominant-words-per-cluster","title":"Dominant Words per Cluster","text":"<p>Each cluster has a \"semantic profile\" defined by the words that contribute most to its identity:</p> <p></p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#optimal-k-selection-elbow-and-silhouette","title":"Optimal k Selection (Elbow and Silhouette)","text":"<p>How do we know how many clusters to create? Two complementary methods:</p> <ul> <li>Elbow Method: We look for the point where inertia stops decreasing significantly.</li> <li>Silhouette Coefficient: A value close to 1 indicates well-separated clusters.</li> </ul> <p></p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#silhouette-analysis-by-cluster","title":"Silhouette Analysis by Cluster","text":"<p>Granular analysis shows the internal cohesion of each cluster. Uniform width indicates well-defined clusters:</p> <p></p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#validation-real-topics-vs-clusters","title":"Validation: Real Topics vs Clusters","text":"<p>Venn diagrams show the overlap between the corpus's real topics and the automatically detected clusters. 100% overlap confirms that K-Means has correctly identified the topics:</p> <p></p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#4-practical-case-titanic-clustering","title":"4. Practical Case: Titanic Clustering","text":"<p>To demonstrate that K-Means works beyond text, we apply the same algorithm to the Titanic dataset with demographic features (age, fare, class):</p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#k-selection-for-titanic","title":"k Selection for Titanic","text":""},{"location":"en/dashboards/05_vectorizacion_y_clustering/#passenger-clustering-pca","title":"Passenger Clustering (PCA)","text":""},{"location":"en/dashboards/05_vectorizacion_y_clustering/#cluster-profiling","title":"Cluster Profiling","text":"<p>Each cluster reveals a different passenger profile (average age, fare, survival rate):</p> <p></p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#5-didactic-guide-step-by-step","title":"5. Didactic Guide: Step by Step","text":""},{"location":"en/dashboards/05_vectorizacion_y_clustering/#step-1-corpus-generation","title":"Step 1: Corpus Generation","text":"<p>We create 1,200 synthetic documents. In a real training scenario, this simulates data ingestion from an API or SQL database.</p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#step-2-cleaning-and-tokenization","title":"Step 2: Cleaning and Tokenization","text":"<p>Although the script is straightforward, in real NLP we would remove punctuation, convert to lowercase, and remove Stop Words so that the model is not distracted by noise.</p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#step-3-model-training","title":"Step 3: Model Training","text":"<p>The command <code>kmeans.fit(tfidf_matrix)</code> is where the \"magic\" happens. The model \"learns\" the latent structure of the data without human assistance.</p>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#references-and-academic-citations","title":"References and Academic Citations","text":"<p>For further study of the methodology, the following fundamental sources are recommended:</p> <ul> <li>McKinney, W. (2012). Python for Data Analysis. O'Reilly Media. (Reference for matrix manipulation).</li> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR. (Official documentation of the framework used).</li> <li>Manning, C. D., et al. (2008). Introduction to Information Retrieval. Cambridge University Press. (Foundational theory for TF-IDF).</li> </ul>"},{"location":"en/dashboards/05_vectorizacion_y_clustering/#institutional-information","title":"Institutional Information","text":"<p>Author/Reference: @TodoEconometria Professor: Juan Marcelo Gutierrez Miranda Area: Big Data, Data Science &amp; Applied Econometrics.</p> <p>Certification Hash ID: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code></p>"},{"location":"en/dashboards/06_analisis_panel_qog/","title":"Module 06: Panel Data Analysis (QoG)","text":"<p>Interactive dashboard with results from the Big Data pipeline on the Quality of Government Standard Dataset (University of Gothenburg, January 2024).</p> <p>Open Interactive Dashboard</p>"},{"location":"en/dashboards/06_analisis_panel_qog/#contents","title":"Contents","text":"<p>The dashboard presents 5 interactive tabs:</p> Tab Research Line Countries Central Asia Post-Soviet institutional evolution KAZ, UZB, TKM, KGZ, TJK Water Security Aral Sea crisis KAZ, UZB, TKM, AFG, IRN Terrorism Political stability and state fragility 13 countries (Europe + Central Asia + Middle East) Maghreb Authoritarianism vs democracy, Arab Springs DZA, MAR, TUN, LBY, EGY, MRT ML: PCA Clusters Classification of 28 countries (K-Means + PCA) 28 countries from all research lines"},{"location":"en/dashboards/06_analisis_panel_qog/#infrastructure","title":"Infrastructure","text":"<ul> <li>Processing: Apache Spark 3.5.4 (Docker cluster: 1 master + 2 workers)</li> <li>Storage: PostgreSQL 15 + Parquet</li> <li>Clustering: scikit-learn (K-Means k=5 + PCA 2 components)</li> <li>Visualization: Plotly (interactive charts)</li> <li>Dataset: 924 observations (28 countries x 33 years), 40 variables</li> </ul>"},{"location":"en/dashboards/06_analisis_panel_qog/#main-variables","title":"Main Variables","text":"Variable Source What it measures <code>vdem_polyarchy</code> V-Dem Electoral democracy index <code>ti_cpi</code> Transparency International Corruption perception <code>wbgi_cce</code> World Bank Control of corruption <code>wbgi_rle</code> World Bank Rule of law <code>wbgi_pse</code> World Bank Political stability <code>wdi_gdppc</code> World Bank GDP per capita (USD 2015) <code>undp_hdi</code> UNDP Human Development Index <code>ffp_fsi</code> Fund for Peace State fragility index"},{"location":"en/dashboards/06_analisis_panel_qog/#source-code","title":"Source Code","text":"<ul> <li>ETL + ML Pipeline: <code>ejercicios/06_an\u00e1lisis_datos_de_panel/</code></li> <li>Theoretical material: <code>ejercicios/07_infraestructura_bigdata/</code></li> </ul>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/","title":"Exercise 1: Text Anatomy and the Rhythm of Words","text":"<p>\"Check out Guitar George, he knows-all the chords...\" Wrapped in the rhythm of Sultans of Swing, we'll learn to dissect language to find its hidden melody.</p>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/#the-opening-ritual-objectives","title":"The Opening Ritual (Objectives)","text":"<p>Turn a murmur of words into a symphony of data. In this first step of NLP (Natural Language Processing), you will learn to:</p> <ol> <li>Expose the Structure: Understand that text is not just letters, but an architecture that must be unified (Merge).</li> <li>Normalize the Frequency: Force the text to lowercase so the algorithm doesn't get confused between \"Solo\" and \"solo\".</li> <li>Tokenize the Chaos: Use regular expressions (<code>re.findall</code>) like a scalpel to separate each term (token) from ambient noise.</li> <li>Count the Pulse: Use <code>Counter</code> to measure how strongly each word beats in the corpus.</li> </ol>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/#the-dissection-machine","title":"The Dissection Machine","text":"<p>Imagine a high-precision shredder that takes an ancient scroll and converts it into pure data blocks.</p> <p></p>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/#the-processing-purgatory","title":"The Processing Purgatory","text":"<ol> <li>Unification: We join all phrases into a single block of textual steel.</li> <li>Normalization: We apply <code>lower()</code> to standardize the signal.</li> <li>Tokenization: We extract the tokens, removing punctuation that doesn't contribute to the main \"riff\".</li> </ol>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/#live-results-python-output","title":"Live Results (Python Output)","text":"<p>When running <code>01_conteo_palabras.py</code>, you'll see the Top 10 words that dominate the stage emerge.</p>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/#the-truth-chart","title":"The Truth Chart","text":"<p>This is where we visualize the frequency spectrum. See those giant bars? Those are the most repeated words.</p>"},{"location":"en/dashboards/EJERCICIO_01_CONTEO/#backstage-reflection","title":"Backstage Reflection","text":"<ul> <li>Do you see the noise? Articles like \"the\", \"a\", or \"of\" tend to dominate the chart. They're like amplifier feedback: they're there, but they're not the melody.</li> <li>Sultans of Swing: Just like in a great Knopfler solo, every word has its place, but some (the stopwords) appear too often and drown out the real message.</li> </ul> <p>Certification Hash: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Master of Ceremonies: Juan Marcelo Gutierrez Miranda (@TodoEconometria) Vibe: Rock &amp; Data</p>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/","title":"Exercise 2: The Noise Filter (Anti-Stopwords)","text":"<p>\"You hear the guitar, it's a-clean and it's a-pure...\" Like going from chaotic distortion to a crystal-clear solo, in this exercise we'll filter out language noise so the essence shines through.</p>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/#the-challenge-cleaning-the-mix","title":"The Challenge: Cleaning the Mix","text":"<p>In the previous exercise we saw that the most common words are \"semantic garbage\" (stopwords). Your mission is to remove them so that the words with real weight rise to the surface.</p> <ol> <li>Stopword Filtering: Engage the filter pedal to ignore words like \"the\", \"is\", \"and\".</li> <li>Visual Impact: Compare the \"Before\" and \"After\" to see how the true meaning emerges.</li> <li>Primitive Sentiment Analysis: By cleaning the noise, words like \"fantastic\" or \"terrible\" take center stage.</li> </ol>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/#the-filter-pedal-stopword-filter","title":"The Filter Pedal (Stopword Filter)","text":"<p>Imagine that every common word is a burst of static. Our algorithm acts like a noise gate pedal that only lets through the high-impact frequencies.</p> <p></p>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/#the-cleaning-ritual","title":"The Cleaning Ritual","text":"<ul> <li>Input: A dirty text full of articles and prepositions.</li> <li>Filter: A blacklist of forbidden words (<code>stopwords_es</code>).</li> <li>Output: A pure signal where every word tells a story.</li> </ul>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/#stage-comparison-python-data","title":"Stage Comparison (Python Data)","text":"<p>When running <code>02_limpieza_texto.py</code>, you'll see the brutal contrast between the two worlds.</p>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/#the-before-vs-the-after","title":"The Before vs The After","text":"<p>Observe how in the left chart empty words dominate, while on the right the pure sentiment appears.</p>"},{"location":"en/dashboards/EJERCICIO_02_LIMPIEZA/#prophecy-for-the-analyst","title":"Prophecy for the Analyst","text":"<ul> <li>Silence is Power: By removing 70% of useless words, the analysis becomes 100% more precise.</li> <li>Sultans of Data: Now that you've cleaned the track, we're ready for the next level: sentiment analysis and similarity.</li> </ul> <p>Certification Hash: <code>4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</code> Master of Ceremonies: Juan Marcelo Gutierrez Miranda (@TodoEconometria) Vibe: Heavy Clean Sound</p>"},{"location":"en/dashboards/arima-sarima-pro/","title":"PRO Dashboard: ARIMA/SARIMA Time Series","text":"<p>Interactive dashboard with a professional design inspired by Bloomberg-style financial terminals, for complete time series analysis following the Box-Jenkins methodology.</p> <p>Open PRO Dashboard</p>"},{"location":"en/dashboards/arima-sarima-pro/#dashboard-features","title":"Dashboard Features","text":"Element Description Theme Dark financial terminal style (inspired by Bloomberg/OECD Explorer) KPIs Cards with real-time metrics: RMSE, MAE, MAPE, R2, AIC Tabs 7 interactive sections with smooth transitions Responsive Adaptable to different screen sizes"},{"location":"en/dashboards/arima-sarima-pro/#content-by-tab","title":"Content by Tab","text":"# Tab Description 1 Original Series Airline passengers 1949-1960 with 12M moving average 2 Decomposition Trend + Seasonality + Residual (multiplicative) 3 ACF / PACF Autocorrelation of the differenced series (d=1, D=1, s=12) 4 Diagnostics Residuals, histogram, residual ACF, Q-Q plot 5 Forecast Series + SARIMA fit + 24-month forecast + 95% CI 6 Radar Metrics Polar visualization of normalized metrics 7 Comparison Comparison of different SARIMA orders"},{"location":"en/dashboards/arima-sarima-pro/#design-inspiration","title":"Design Inspiration","text":"<p>This dashboard was created following best practices in financial visualization:</p> <ul> <li>OECD Pension Explorer - Official OECD Plotly App</li> <li>Portfolio Optimizer - Panel/HoloViz Gallery</li> <li>Dash Bootstrap Templates - Professional themes for Dash</li> </ul>"},{"location":"en/dashboards/arima-sarima-pro/#box-jenkins-methodology","title":"Box-Jenkins Methodology","text":"<ol> <li>Identification: ACF/PACF analysis to determine orders (p, d, q)(P, D, Q)[s]</li> <li>Estimation: Maximum likelihood fitting with SARIMAX</li> <li>Diagnostics: Ljung-Box, Jarque-Bera tests, Q-Q plot</li> <li>Forecasting: Forecast with 95% confidence intervals</li> </ol>"},{"location":"en/dashboards/arima-sarima-pro/#source-code","title":"Source Code","text":"<ul> <li>Exercise script: <code>ejercicios/04_machine_learning/07_series_temporales_arima/</code></li> <li>Dashboard exporter: <code>.profesor/soluciones/TRABAJO_FINAL/export_arima_pro.py</code></li> <li>Theoretical guide: ARIMA Time Series</li> </ul> <p>Course: Big Data with Python - From Zero to Production Professor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Academic references:</p> <ul> <li>Box, G.E.P. &amp; Jenkins, G.M. (1976). Time Series Analysis: Forecasting and Control. Holden-Day.</li> <li>Hyndman, R.J. &amp; Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3<sup>rd</sup> ed.). OTexts.</li> <li>Hamilton, J.D. (1994). Time Series Analysis. Princeton University Press.</li> </ul>"},{"location":"en/dashboards/dashboard_iss_tracker/","title":"ISS Tracker - International Space Station","text":"<p>Real-time tracking of the International Space Station</p>"},{"location":"en/dashboards/dashboard_iss_tracker/#description","title":"Description","text":"<p>This dashboard allows you to track the position of the International Space Station (ISS) in real time, view its orbital trajectory, and predict when it will pass over your location.</p>"},{"location":"en/dashboards/dashboard_iss_tracker/#features","title":"Features","text":"<ul> <li>Real-time map with the current ISS position</li> <li>Orbital trajectory past and future</li> <li>Pass predictor: enter your city and find out when to see the ISS</li> <li>Live data: latitude, longitude, altitude, speed</li> <li>Current crew: astronauts aboard the ISS</li> </ul>"},{"location":"en/dashboards/dashboard_iss_tracker/#view-dashboard","title":"View Dashboard","text":"Open ISS Tracker"},{"location":"en/dashboards/dashboard_iss_tracker/#how-to-see-the-iss","title":"How to See the ISS","text":"<p>The ISS is visible to the naked eye when it passes over your location at night. It looks like a bright star moving quickly across the sky.</p> <p>Tips: 1. Use the dashboard's predictor to know when it will pass 2. Find a dark spot, away from city lights 3. Look toward the indicated direction (N, NE, E, etc.) 4. The ISS will appear as a bright point moving from horizon to horizon</p>"},{"location":"en/dashboards/dashboard_iss_tracker/#iss-data","title":"ISS Data","text":"Feature Value Altitude ~420 km above Earth Speed 27,600 km/h (7.66 km/s) Orbital period 92.68 minutes Orbits per day 15.5 Inclination 51.6 degrees Size 109m x 73m (football field)"},{"location":"en/dashboards/dashboard_iss_tracker/#apis-used","title":"APIs Used","text":"<ul> <li>Where The ISS At: Real-time position</li> <li>Open Notify: Astronauts and position</li> <li>Nominatim: Address geocoding</li> </ul>"},{"location":"en/dashboards/dashboard_iss_tracker/#technologies-used","title":"Technologies Used","text":"<ul> <li>LocalStack: AWS simulation (S3, Lambda, DynamoDB)</li> <li>Terraform: Infrastructure as Code</li> <li>Kinesis: Position streaming</li> <li>Plotly: Interactive visualizations</li> </ul> <p>Course: Big Data with Python - From Zero to Production Professor: Juan Marcelo Gutierrez Miranda | @TodoEconometria</p>"},{"location":"en/dashboards/dashboard_sismos_global/","title":"Global Seismic Observatory","text":"<p>Interactive dashboard of worldwide seismic activity in real time</p>"},{"location":"en/dashboards/dashboard_sismos_global/#description","title":"Description","text":"<p>This dashboard displays seismic activity from the last 24 hours using data from the USGS Earthquake Hazards Program (United States Geological Survey).</p>"},{"location":"en/dashboards/dashboard_sismos_global/#features","title":"Features","text":"<ul> <li>Interactive global map with all detected earthquakes</li> <li>Real-time statistics: total earthquakes, maximum magnitude, average</li> <li>Activity timeline by hour</li> <li>Top 10 regions with the highest seismic activity</li> <li>Classification by magnitude: Micro, Minor, Light, Moderate, Major</li> </ul>"},{"location":"en/dashboards/dashboard_sismos_global/#view-dashboard","title":"View Dashboard","text":"Open Seismic Observatory"},{"location":"en/dashboards/dashboard_sismos_global/#data-source","title":"Data Source","text":"<p>Data comes from the USGS public API:</p> <ul> <li>URL: earthquake.usgs.gov</li> <li>Format: GeoJSON</li> <li>Update frequency: Every minute</li> <li>Coverage: Global</li> </ul>"},{"location":"en/dashboards/dashboard_sismos_global/#magnitude-scale","title":"Magnitude Scale","text":"Category Magnitude Color Description MICRO &lt; 2.5 Green Not felt, only detected by instruments MINOR 2.5 - 4.0 Blue Generally not felt, but recorded LIGHT 4.0 - 5.0 Orange Felt, minor damage unlikely MODERATE 5.0 - 7.0 Red Can cause significant damage MAJOR &gt; 7.0 Purple Major earthquake, potential severe damage"},{"location":"en/dashboards/dashboard_sismos_global/#technologies-used","title":"Technologies Used","text":"<ul> <li>Apache Kafka: Real-time data streaming</li> <li>Spark Structured Streaming: Distributed processing</li> <li>PostgreSQL: Alert storage</li> <li>Plotly: Interactive visualizations</li> <li>USGS API: Official data source</li> </ul> <p>Course: Big Data with Python - From Zero to Production Professor: Juan Marcelo Gutierrez Miranda | @TodoEconometria</p>"},{"location":"en/dashboards/flores-transfer-learning/","title":"Dashboard: Flower Classification with Transfer Learning","text":""},{"location":"en/dashboards/flores-transfer-learning/#description","title":"Description","text":"<p>Computer Vision pipeline that classifies flower images using Transfer Learning with MobileNetV2.</p> <p>What is Transfer Learning? Instead of training a neural network from scratch (which would require millions of images), we use a network already trained on ImageNet and adapt it to our problem.</p>"},{"location":"en/dashboards/flores-transfer-learning/#pipeline","title":"Pipeline","text":"<pre><code>1. DOWNLOAD           2. EMBEDDINGS         3. CLASSIFICATION      4. VISUALIZATION\n   3,670 flowers         MobileNetV2           Traditional ML         Dashboard\n   5 classes             1280 features         KNN/SVM/RF             Plotly\n</code></pre>"},{"location":"en/dashboards/flores-transfer-learning/#results","title":"Results","text":"Model Accuracy SVM 89.9% Random Forest 86.5% KNN 86.2%"},{"location":"en/dashboards/flores-transfer-learning/#visualizations","title":"Visualizations","text":"<p>The dashboard includes 4 interactive tabs:</p> <ol> <li>t-SNE: 2D projection of embeddings - similar flowers appear together</li> <li>Comparison: Bars with accuracy for each model</li> <li>Confusion Matrix: Hits/errors by class (percentages)</li> <li>Distribution: Radar chart of the dataset</li> </ol>"},{"location":"en/dashboards/flores-transfer-learning/#view-dashboard","title":"View Dashboard","text":"Open Interactive Dashboard"},{"location":"en/dashboards/flores-transfer-learning/#run-the-exercise","title":"Run the Exercise","text":"<pre><code>cd ejercicios/04_machine_learning/flores_transfer_learning/\npip install -r requirements.txt\npython 01_flores_transfer_learning.py\n</code></pre> <p>Requirements: TensorFlow (GPU recommended but works on CPU)</p> <p>Course: Big Data with Python - From Zero to Production Professor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Academic references:</p> <ul> <li>Sandler, M., et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR.</li> <li>Yosinski, J., et al. (2014). How transferable are features in deep neural networks? NeurIPS.</li> <li>van der Maaten, L. &amp; Hinton, G. (2008). Visualizing Data using t-SNE. JMLR.</li> </ul>"},{"location":"en/ejercicios/","title":"Exercises","text":"<p>Complete list of all available exercises in the course.</p>"},{"location":"en/ejercicios/#exercise-roadmap","title":"Exercise Roadmap","text":""},{"location":"en/ejercicios/#module-1-databases","title":"Module 1: Databases","text":"# Exercise Technology Level Status 1.1 Introduction to SQLite SQLite + Pandas Basic Available 2.1 PostgreSQL HR PostgreSQL Intermediate Available 2.2 PostgreSQL Gardening PostgreSQL Intermediate Available 2.3 SQLite to PostgreSQL Migration PostgreSQL + Python Intermediate Available 3.1 Oracle HR Oracle Database Advanced Available 5.1 Excel/Python Analysis Pandas + Excel Basic Available"},{"location":"en/ejercicios/#module-2-data-cleaning-and-etl","title":"Module 2: Data Cleaning and ETL","text":"# Exercise Technology Level Status 02 ETL Pipeline QoG PostgreSQL + Pandas Advanced Available"},{"location":"en/ejercicios/#module-3-distributed-processing","title":"Module 3: Distributed Processing","text":"# Exercise Technology Level Status 03 Distributed Processing with Dask Dask + Parquet Intermediate Available"},{"location":"en/ejercicios/#module-4-machine-learning","title":"Module 4: Machine Learning","text":"# Exercise Technology Level Status 04 Machine Learning (PCA, K-Means) Scikit-Learn, PCA, K-Means Advanced Available 04.2 Transfer Learning Flowers TensorFlow, MobileNetV2 Advanced Available ARIMA Time Series ARIMA/SARIMA statsmodels, Box-Jenkins Advanced Available"},{"location":"en/ejercicios/#module-5-nlp-and-text-mining","title":"Module 5: NLP and Text Mining","text":"# Exercise Technology Level Status 05 NLP and Text Mining NLTK, TF-IDF, Jaccard, Sentiment Advanced Available"},{"location":"en/ejercicios/#module-6-panel-data-analysis","title":"Module 6: Panel Data Analysis","text":"# Exercise Technology Level Status 06 Panel Data Analysis linearmodels, Panel OLS, Altair Advanced Available"},{"location":"en/ejercicios/#module-7-big-data-infrastructure","title":"Module 7: Big Data Infrastructure","text":"# Exercise Technology Level Status 07 Big Data Infrastructure Docker Compose, Apache Spark Intermediate-Advanced Available"},{"location":"en/ejercicios/#module-8-streaming-with-kafka","title":"Module 8: Streaming with Kafka","text":"# Exercise Technology Level Status 08 Streaming with Kafka Apache Kafka, Spark Streaming, KRaft Advanced Available"},{"location":"en/ejercicios/#module-9-cloud-with-localstack","title":"Module 9: Cloud with LocalStack","text":"# Exercise Technology Level Status 09 Cloud with LocalStack LocalStack, Terraform, AWS Advanced Available"},{"location":"en/ejercicios/#capstone-project","title":"Capstone Project","text":"# Exercise Technology Level Status TF Capstone Integrative Project Docker + Spark + PostgreSQL + QoG Advanced Available"},{"location":"en/ejercicios/#module-1-databases_1","title":"MODULE 1: Databases","text":""},{"location":"en/ejercicios/#exercise-11-introduction-to-sqlite","title":"Exercise 1.1: Introduction to SQLite","text":"<p>Details</p> <ul> <li>Level: Basic</li> <li>Dataset: NYC Taxi (10MB sample)</li> <li>Technologies: SQLite, Pandas</li> </ul> <p>What you'll learn:</p> <ul> <li>Load CSV data into a SQLite database</li> <li>Basic SQL queries (SELECT, WHERE, GROUP BY)</li> <li>Optimization with indexes</li> <li>Export results to CSV</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#exercise-21-postgresql-with-hr-database","title":"Exercise 2.1: PostgreSQL with HR Database","text":"<p>Details</p> <ul> <li>Level: Intermediate</li> <li>Database: HR (Human Resources) from Oracle</li> <li>Technologies: PostgreSQL, SQL</li> </ul> <p>What you'll learn:</p> <ul> <li>Install and configure PostgreSQL</li> <li>Load databases from SQL scripts</li> <li>Complex queries with multiple JOINs</li> <li>PostgreSQL-specific functions</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#exercise-22-postgresql-gardening","title":"Exercise 2.2: PostgreSQL Gardening","text":"<p>Details</p> <ul> <li>Level: Intermediate</li> <li>Database: Gardening sales system</li> <li>Technologies: PostgreSQL, Window Functions</li> </ul> <p>What you'll learn:</p> <ul> <li>Sales analysis with SQL</li> <li>Complex aggregations (GROUP BY, HAVING)</li> <li>Window Functions for rankings</li> <li>Materialized views</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#exercise-23-sqlite-to-postgresql-migration","title":"Exercise 2.3: SQLite to PostgreSQL Migration","text":"<p>Details</p> <ul> <li>Level: Intermediate</li> <li>Technologies: SQLite, PostgreSQL, Python</li> </ul> <p>What you'll learn:</p> <ul> <li>Differences between database engines</li> <li>Migrate schemas and data</li> <li>Adapt data types</li> <li>Validate integrity</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#exercise-31-oracle-with-hr-database","title":"Exercise 3.1: Oracle with HR Database","text":"<p>Advanced</p> <ul> <li>Level: Advanced</li> <li>Database: HR on native Oracle</li> <li>Technologies: Oracle Database, PL/SQL</li> </ul> <p>What you'll learn:</p> <ul> <li>Install Oracle Database XE</li> <li>Oracle-specific syntax</li> <li>PL/SQL (procedures, functions)</li> <li>Sequences and triggers</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#exercise-51-excelpython-analysis","title":"Exercise 5.1: Excel/Python Analysis","text":"<p>Details</p> <ul> <li>Level: Basic-Intermediate</li> <li>Technologies: Python, Pandas, Excel</li> </ul> <p>What you'll learn:</p> <ul> <li>Read Excel files with Python</li> <li>Exploratory Data Analysis (EDA)</li> <li>Visualizations with matplotlib/seaborn</li> <li>Automate analyses</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-2-data-cleaning-and-etl_1","title":"MODULE 2: Data Cleaning and ETL","text":""},{"location":"en/ejercicios/#professional-etl-pipeline-quality-of-government","title":"Professional ETL Pipeline - Quality of Government","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Dataset: QoG (1289 variables, 194+ countries)</li> <li>Technologies: PostgreSQL, Pandas, psycopg2</li> </ul> <p>What you'll learn:</p> <ul> <li>Design a modular ETL architecture</li> <li>Work with PostgreSQL for longitudinal analysis</li> <li>Clean complex datasets (&gt;1000 variables)</li> <li>Prepare panel data for econometrics</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-3-distributed-processing_1","title":"MODULE 3: Distributed Processing","text":""},{"location":"en/ejercicios/#distributed-processing-with-dask","title":"Distributed Processing with Dask","text":"<p>Details</p> <ul> <li>Level: Intermediate</li> <li>Technologies: Dask, Parquet, LocalCluster</li> </ul> <p>What you'll learn:</p> <ul> <li>Set up a Local Cluster with Dask</li> <li>Read Parquet files in a partitioned manner</li> <li>Execute complex aggregations in parallel</li> <li>Compare performance vs Pandas</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-4-machine-learning_1","title":"MODULE 4: Machine Learning","text":""},{"location":"en/ejercicios/#machine-learning-in-big-data","title":"Machine Learning in Big Data","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Technologies: Scikit-Learn, PCA, K-Means</li> <li>Scripts: PCA Iris, FactoMineR, Breast Cancer, Wine, TF-IDF</li> </ul> <p>What you'll learn:</p> <ul> <li>Dimensionality reduction with PCA</li> <li>Clustering with K-Means and Hierarchical Clustering</li> <li>Principal component interpretation</li> <li>Cluster profiling</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#transfer-learning-flower-classification","title":"Transfer Learning: Flower Classification","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Technologies: TensorFlow, MobileNetV2, Scikit-Learn</li> <li>Dataset: TensorFlow Flowers (3,670 images, 5 classes)</li> </ul> <p>What you'll learn:</p> <ul> <li>Transfer Learning with pre-trained networks (ImageNet)</li> <li>Embedding extraction with CNNs</li> <li>Image classification with traditional ML (KNN, SVM, Random Forest)</li> <li>t-SNE visualization of high-dimensional spaces</li> </ul> <p>View Interactive Dashboard</p>"},{"location":"en/ejercicios/#time-series-arimasarima","title":"Time Series: ARIMA/SARIMA","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Dataset: AirPassengers (144 observations, 1949-1960)</li> <li>Technologies: statsmodels, Box-Jenkins Methodology</li> </ul> <p>What you'll learn:</p> <ul> <li>Complete Box-Jenkins methodology (Identification, Estimation, Diagnostics, Forecasting)</li> <li>ARIMA and SARIMA models with seasonality</li> <li>ACF/PACF for order identification</li> <li>Residual diagnostics and forecasts</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-5-nlp-and-text-mining_1","title":"MODULE 5: NLP and Text Mining","text":""},{"location":"en/ejercicios/#nlp-and-text-mining","title":"NLP and Text Mining","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Technologies: NLTK, TF-IDF, Jaccard, Sentiment Analysis</li> <li>Scripts: Counting, Cleaning, Sentiment, Similarity</li> </ul> <p>What you'll learn:</p> <ul> <li>Tokenization and text cleaning</li> <li>Stopword removal</li> <li>Jaccard similarity between documents</li> <li>Lexicon-based sentiment analysis</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-6-panel-data-analysis_1","title":"MODULE 6: Panel Data Analysis","text":""},{"location":"en/ejercicios/#panel-data-analysis","title":"Panel Data Analysis","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Datasets: Guns (gun laws), Fatalities (traffic mortality)</li> <li>Technologies: linearmodels, Panel OLS, Altair</li> </ul> <p>What you'll learn:</p> <ul> <li>Panel data: country x year structure</li> <li>Fixed Effects vs Random Effects</li> <li>Two-Way Fixed Effects</li> <li>Hausman test for model selection</li> <li>Odds Ratios and Marginal Effects</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-7-big-data-infrastructure_1","title":"MODULE 7: Big Data Infrastructure","text":""},{"location":"en/ejercicios/#big-data-infrastructure-docker-and-spark","title":"Big Data Infrastructure: Docker and Spark","text":"<p>Details</p> <ul> <li>Level: Intermediate-Advanced</li> <li>Type: Theoretical-Conceptual with practical examples</li> <li>Technologies: Docker, Docker Compose, Apache Spark</li> </ul> <p>What you'll learn:</p> <ul> <li>Docker: containers, images, Dockerfile, orchestration with Compose</li> <li>Networks, volumes, healthchecks, production patterns</li> <li>Apache Spark: Master-Worker architecture, cluster with Docker</li> <li>SparkSession, Lazy Evaluation, DAG, Catalyst optimizer</li> <li>Spark + PostgreSQL via JDBC</li> <li>From Standalone to production (Kubernetes, EMR, Dataproc)</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-8-streaming-with-kafka_1","title":"MODULE 8: Streaming with Kafka","text":""},{"location":"en/ejercicios/#streaming-with-apache-kafka","title":"Streaming with Apache Kafka","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Technologies: Apache Kafka (KRaft), Python, Spark Streaming</li> <li>API: USGS Earthquakes (real-time)</li> </ul> <p>What you'll learn:</p> <ul> <li>Kafka architecture: Brokers, Topics, Partitions</li> <li>KRaft mode (no ZooKeeper)</li> <li>Producers and Consumers in Python</li> <li>Spark Structured Streaming</li> <li>Real-time alert system</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#module-9-cloud-with-localstack_1","title":"MODULE 9: Cloud with LocalStack","text":""},{"location":"en/ejercicios/#cloud-with-localstack-and-terraform","title":"Cloud with LocalStack and Terraform","text":"<p>Details</p> <ul> <li>Level: Advanced</li> <li>Technologies: LocalStack, Terraform, AWS (S3, Lambda, DynamoDB)</li> <li>API: ISS Tracker (real-time)</li> </ul> <p>What you'll learn:</p> <ul> <li>Cloud Computing: IaaS, PaaS, SaaS</li> <li>Simulate AWS locally with LocalStack</li> <li>Infrastructure as Code with Terraform</li> <li>Serverless Lambda functions</li> <li>Data Lake architecture (Medallion)</li> </ul> <p>View Full Exercise</p>"},{"location":"en/ejercicios/#capstone-project_1","title":"CAPSTONE PROJECT","text":""},{"location":"en/ejercicios/#capstone-project-big-data-pipeline-with-docker","title":"Capstone Project: Big Data Pipeline with Docker","text":"<p>Integrative Project</p> <ul> <li>Level: Advanced</li> <li>Technologies: Docker, Apache Spark, PostgreSQL, QoG</li> <li>Evaluation: Infrastructure 30% + ETL 25% + Analysis 25% + AI Reflection 20%</li> </ul> <p>What you'll do:</p> <ul> <li>Build Docker infrastructure (Spark + PostgreSQL)</li> <li>Design and execute an ETL pipeline with Apache Spark</li> <li>Analyze QoG data with your own research question</li> <li>Document your learning process with AI</li> </ul> <p>View Full Assignment</p>"},{"location":"en/ejercicios/#datasets-used","title":"Datasets Used","text":""},{"location":"en/ejercicios/#nyc-taxi-limousine-commission-tlc","title":"NYC Taxi &amp; Limousine Commission (TLC)","text":"<ul> <li>Source: NYC Open Data</li> <li>Period: 2021</li> <li>Records: 10M+ trips</li> </ul>"},{"location":"en/ejercicios/#quality-of-government-qog","title":"Quality of Government (QoG)","text":"<ul> <li>Source: University of Gothenburg</li> <li>Variables: 1289 institutional quality indicators</li> <li>Countries: 194+ with data since 1946</li> </ul>"},{"location":"en/ejercicios/#airpassengers","title":"AirPassengers","text":"<ul> <li>Source: Box &amp; Jenkins (1976)</li> <li>Period: 1949-1960 (144 monthly observations)</li> <li>Use: ARIMA/SARIMA time series</li> </ul>"},{"location":"en/ejercicios/#how-to-work-through-exercises","title":"How to Work Through Exercises","text":""},{"location":"en/ejercicios/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Read the full assignment - Do not start coding without reading everything</li> <li>Understand the objectives - What are you expected to achieve?</li> <li>Create a working branch - <code>git checkout -b your-lastname-exercise-XX</code></li> <li>Work in small steps - Do not try to do everything at once</li> <li>Test frequently - Run your code each time you complete a section</li> <li>Make regular commits - Save your progress frequently</li> <li>Push with git push - When you finish, the system evaluates your PROMPTS.md</li> </ol>"},{"location":"en/ejercicios/#next-steps","title":"Next Steps","text":"<p>Start with the first exercise:</p> <p>Exercise 01: Introduction to SQLite</p> <p>Or jump to the capstone project:</p> <p>Capstone Project: Big Data Pipeline</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/","title":"Exercise 01: Introduction to SQLite","text":"<p>Learn how to load and query data using SQLite and Pandas.</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/#general-information","title":"General Information","text":"Field Value Level \ud83d\udfe2 Basic Estimated time 2-3 hours Technologies Python, SQLite, Pandas Dataset NYC Taxi (10MB sample) Prerequisites Basic Python, basic SQL knowledge"},{"location":"en/ejercicios/01-introduccion-sqlite/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this exercise, you will be able to:</p> <ul> <li> Load large CSV files into SQLite using chunks</li> <li> Create and manage SQLite databases</li> <li> Execute basic and advanced SQL queries</li> <li> Optimize performance with indexes</li> <li> Export query results to CSV</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#the-problem","title":"The Problem","text":"<p>You have a CSV file with 100,000 records of NYC taxi trips. If you try to load it all into memory with Pandas, your computer may run out of memory.</p> <p>Your mission: Load this data into a SQLite database efficiently and perform analysis.</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/#nyc-taxi-trip-records","title":"NYC Taxi Trip Records","text":"<ul> <li>File: <code>datos/muestra_taxi.csv</code></li> <li>Size: ~10 MB</li> <li>Records: ~100,000 trips</li> <li>Period: January 2021</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#data-structure","title":"Data Structure","text":"<pre><code>Columnas:\n- tpep_pickup_datetime    # Fecha/hora de inicio del viaje\n- tpep_dropoff_datetime   # Fecha/hora de fin del viaje\n- passenger_count         # Numero de pasajeros\n- trip_distance           # Distancia en millas\n- pickup_longitude        # Longitud de origen\n- pickup_latitude         # Latitud de origen\n- dropoff_longitude       # Longitud de destino\n- dropoff_latitude        # Latitud de destino\n- payment_type            # Tipo de pago (1=Credit, 2=Cash, ...)\n- fare_amount             # Tarifa base\n- tip_amount              # Propina\n- total_amount            # Total pagado\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#tasks","title":"Tasks","text":""},{"location":"en/ejercicios/01-introduccion-sqlite/#task-1-load-csv-into-sqlite-in-chunks","title":"Task 1: Load CSV into SQLite in Chunks","text":"<p>Objective: Load the complete CSV into SQLite without running out of memory.</p> <p>Requirements:</p> <ul> <li>Use <code>pandas.read_csv()</code> with the <code>chunksize</code> parameter</li> <li>Process the CSV in chunks of 10,000 records</li> <li>Insert each chunk into the <code>trips</code> table</li> <li>Display loading progress</li> </ul> <p>Initial code example:</p> <pre><code>import sqlite3\nimport pandas as pd\n\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"\n    Carga un CSV grande a SQLite en chunks\n\n    Args:\n        csv_path: Ruta al archivo CSV\n        db_path: Ruta a la base de datos SQLite\n        chunksize: Numero de registros por chunk\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # TODO: Implementar carga por chunks\n    # Pista: usa pd.read_csv con chunksize\n    # y itera sobre los chunks\n\n    conn.close()\n</code></pre> <p>Hint</p> <pre><code>chunks = pd.read_csv(csv_path, chunksize=chunksize)\nfor i, chunk in enumerate(chunks):\n    # Insertar chunk en SQLite\n    chunk.to_sql('trips', conn, if_exists='append', index=False)\n    print(f\"Chunk {i+1} cargado\")\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#task-2-create-indexes","title":"Task 2: Create Indexes","text":"<p>Objective: Optimize queries by adding indexes to frequently queried columns.</p> <p>Requirements:</p> <ul> <li>Create an index on <code>tpep_pickup_datetime</code></li> <li>Create an index on <code>payment_type</code></li> <li>Measure query time before and after creating indexes</li> </ul> <p>Example:</p> <pre><code>def crear_indices(db_path):\n    \"\"\"\n    Crea indices para optimizar queries\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # TODO: Crear indices\n    # Ejemplo:\n    # cursor.execute(\"CREATE INDEX idx_pickup ON trips(tpep_pickup_datetime)\")\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#task-3-analysis-queries","title":"Task 3: Analysis Queries","text":"<p>Objective: Extract insights from the data using SQL.</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/#query-1-average-revenue-by-hour-of-day","title":"Query 1: Average Revenue by Hour of Day","text":"<p>Calculate the average revenue for each hour of the day.</p> <p>Expected SQL:</p> <pre><code>SELECT\n    strftime('%H', tpep_pickup_datetime) as hora,\n    AVG(total_amount) as promedio_ingreso,\n    COUNT(*) as num_viajes\nFROM trips\nGROUP BY hora\nORDER BY hora\n</code></pre> <p>Expected result:</p> <pre><code>hora  promedio_ingreso  num_viajes\n00    15.23             2340\n01    14.89             1982\n02    16.45             1657\n...\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#query-2-payment-method-distribution","title":"Query 2: Payment Method Distribution","text":"<p>Calculate the percentage of each payment method.</p> <pre><code>SELECT\n    payment_type,\n    COUNT(*) as total,\n    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM trips), 2) as porcentaje\nFROM trips\nGROUP BY payment_type\nORDER BY total DESC\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#query-3-top-10-most-profitable-routes","title":"Query 3: Top 10 Most Profitable Routes","text":"<p>Find the routes (pickup \u2192 dropoff) with the highest average revenue.</p> <p>Challenge</p> <p>This query requires grouping by rounded coordinates. Think about how to do it.</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/#task-4-export-results","title":"Task 4: Export Results","text":"<p>Objective: Save your analysis results to CSV files.</p> <pre><code>def exportar_resultados(db_path, query, output_path):\n    \"\"\"\n    Ejecuta una query y exporta a CSV\n\n    Args:\n        db_path: Ruta a la BD SQLite\n        query: Query SQL a ejecutar\n        output_path: Ruta del CSV de salida\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # TODO: Ejecutar query y exportar\n    # Pista: usa pd.read_sql_query() y df.to_csv()\n\n    conn.close()\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"en/ejercicios/01-introduccion-sqlite/#functionality-40-points","title":"Functionality (40 points)","text":"<ul> <li> Complete data loading without errors (10 pts)</li> <li> Indexes created correctly (10 pts)</li> <li> Queries execute and return correct results (15 pts)</li> <li> Export works (5 pts)</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#clean-code-30-points","title":"Clean Code (30 points)","text":"<ul> <li> Well-documented functions (10 pts)</li> <li> Readable and organized code (10 pts)</li> <li> Error handling (10 pts)</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#performance-20-points","title":"Performance (20 points)","text":"<ul> <li> Efficient loading with chunks (10 pts)</li> <li> Indexes improve performance measurably (10 pts)</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#analysis-10-points","title":"Analysis (10 points)","text":"<ul> <li> Interpretation of results (5 pts)</li> <li> Additional insights (5 pts)</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#deliverables","title":"Deliverables","text":"<p>You must submit by pushing to your fork (git push):</p> <ol> <li>Python code: <code>01_cargar_sqlite.py</code></li> <li>Database: <code>datos/taxi.db</code> (DO NOT upload to GitHub, too large)</li> <li>Results: <code>resultados/</code> folder with exported CSVs</li> <li>Analysis: <code>ANALISIS.md</code> with your findings</li> </ol>"},{"location":"en/ejercicios/01-introduccion-sqlite/#folder-structure","title":"Folder structure","text":"<pre><code>entregas/01_sqlite/tu_apellido_nombre/\n\u251c\u2500\u2500 01_cargar_sqlite.py\n\u251c\u2500\u2500 resultados/\n\u2502   \u251c\u2500\u2500 ingresos_por_hora.csv\n\u2502   \u251c\u2500\u2500 distribucion_pagos.csv\n\u2502   \u2514\u2500\u2500 top_rutas.csv\n\u2514\u2500\u2500 ANALISIS.md\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#partial-solution-example","title":"Partial Solution Example","text":"<p>Sample Code</p> <pre><code>import sqlite3\nimport pandas as pd\nimport time\n\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"Carga CSV a SQLite en chunks\"\"\"\n    conn = sqlite3.connect(db_path)\n\n    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n\n    for i, chunk in enumerate(chunks):\n        chunk.to_sql('trips', conn, if_exists='append', index=False)\n        print(f\"Chunk {i+1} cargado ({len(chunk)} registros)\")\n\n    conn.close()\n    print(\"Carga completa!\")\n\ndef crear_indices(db_path):\n    \"\"\"Crea indices para optimizacion\"\"\"\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    print(\"Creando indices...\")\n    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_pickup ON trips(tpep_pickup_datetime)\")\n    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_payment ON trips(payment_type)\")\n\n    conn.commit()\n    conn.close()\n    print(\"Indices creados!\")\n\ndef analizar_ingresos_por_hora(db_path):\n    \"\"\"Query: Ingresos promedio por hora\"\"\"\n    conn = sqlite3.connect(db_path)\n\n    query = \"\"\"\n        SELECT\n            strftime('%H', tpep_pickup_datetime) as hora,\n            ROUND(AVG(total_amount), 2) as promedio_ingreso,\n            COUNT(*) as num_viajes\n        FROM trips\n        GROUP BY hora\n        ORDER BY hora\n    \"\"\"\n\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return df\n\nif __name__ == \"__main__\":\n    # Rutas\n    csv_path = \"datos/muestra_taxi.csv\"\n    db_path = \"datos/taxi.db\"\n\n    # 1. Cargar datos\n    print(\"=== CARGANDO DATOS ===\")\n    start = time.time()\n    cargar_datos_sqlite(csv_path, db_path)\n    print(f\"Tiempo: {time.time() - start:.2f} segundos\\n\")\n\n    # 2. Crear indices\n    print(\"=== CREANDO INDICES ===\")\n    crear_indices(db_path)\n    print()\n\n    # 3. Analisis\n    print(\"=== ANALISIS: INGRESOS POR HORA ===\")\n    df = analizar_ingresos_por_hora(db_path)\n    print(df)\n\n    # Exportar\n    df.to_csv(\"resultados/ingresos_por_hora.csv\", index=False)\n    print(\"\\nResultados exportados!\")\n</code></pre>"},{"location":"en/ejercicios/01-introduccion-sqlite/#reflection-questions","title":"Reflection Questions","text":"<p>Answer in your <code>ANALISIS.md</code>:</p> <ol> <li> <p>Performance:</p> <ul> <li>How long did it take to load 100,000 records?</li> <li>How much did the indexes improve query time?</li> </ul> </li> <li> <p>Insights:</p> <ul> <li>At what time of day do taxis earn the most?</li> <li>What is the most common payment method?</li> <li>What pattern do you observe in the data?</li> </ul> </li> <li> <p>Improvements:</p> <ul> <li>How would you further optimize the loading?</li> <li>What other analyses could be done?</li> <li>What limitations does SQLite have for this dataset?</li> </ul> </li> </ol>"},{"location":"en/ejercicios/01-introduccion-sqlite/#additional-resources","title":"Additional Resources","text":""},{"location":"en/ejercicios/01-introduccion-sqlite/#documentation","title":"Documentation","text":"<ul> <li>SQLite Python Tutorial</li> <li>Pandas to_sql Documentation</li> <li>SQL Tutorial</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#tutorials","title":"Tutorials","text":"<ul> <li>Working with Large CSV Files</li> <li>SQLite Indexes</li> </ul>"},{"location":"en/ejercicios/01-introduccion-sqlite/#common-issues","title":"Common Issues","text":"Error: MemoryError when loading CSV <p>Cause: Trying to load the entire file at once.</p> <p>Solution: Use chunks: <pre><code>chunks = pd.read_csv(csv_path, chunksize=10000)\n</code></pre></p> Very slow query <p>Cause: Missing index on queried columns.</p> <p>Solution: Create indexes: <pre><code>cursor.execute(\"CREATE INDEX idx_nombre ON tabla(columna)\")\n</code></pre></p> Error: database is locked <p>Cause: Another process has the database open.</p> <p>Solution: - Close all connections: <code>conn.close()</code> - Make sure the database is not open in another program</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/#the-big-picture-data-ecosystems","title":"The Big Picture: Data Ecosystems","text":"<p>SQLite is just the first step. In this course you will explore complete database ecosystems:</p>"},{"location":"en/ejercicios/01-introduccion-sqlite/#key-value-and-columnar-models","title":"Key-Value and Columnar Models","text":""},{"location":"en/ejercicios/01-introduccion-sqlite/#the-mongodb-ecosystem","title":"The MongoDB Ecosystem","text":""},{"location":"en/ejercicios/01-introduccion-sqlite/#next-steps","title":"Next Steps","text":"<p>Once you have completed this exercise:</p> <ul> <li>Exercise 02: Data Cleaning - Next exercise</li> <li>Submission Guide - How to submit your work</li> <li>Roadmap - See all exercises</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/","title":"Exercise 02: Professional ETL Pipeline - Quality of Government","text":"<p>Level: Advanced | Duration: 15-20 hours | Mode: Group or Individual</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#general-description","title":"General Description","text":"<p>You will build a professional ETL pipeline working with the Quality of Government dataset, a longitudinal database with over 1000 variables on institutional quality, democracy, and economic development.</p> <p>Objective: Apply software engineering and data science techniques to clean, transform, and analyze real academic research data.</p> <p></p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#the-data-life-cycle","title":"The Data Life Cycle","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#dataset-quality-of-government-qog","title":"Dataset: Quality of Government (QoG)","text":"<p>What is it?</p> <p>A database maintained by the University of Gothenburg that aggregates variables from multiple international sources.</p> <p>Characteristics: - 1289 variables on institutional quality, economy, society - 194+ countries with data since 1946 - Sources: World Bank, V-Dem, Transparency International, Freedom House, UNDP</p> <p>Source: https://www.qog.pol.gu.se/</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>\u2705 Design a modular ETL architecture</li> <li>\u2705 Work with PostgreSQL for longitudinal analysis</li> <li>\u2705 Clean complex datasets (&gt;1000 variables)</li> <li>\u2705 Prepare panel data for econometrics</li> <li>\u2705 Apply software engineering best practices</li> <li>\u2705 Write production-ready code</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#analysis-topics","title":"Analysis Topics","text":"<p>Choose ONE:</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#topic-1-post-authoritarian-institutional-evolution","title":"Topic 1: Post-Authoritarian Institutional Evolution","text":"<p>Question: How does institutional quality evolve during democratic transitions?</p> <p>Key variables: - Democracy indices (V-Dem, Polity) - Institutional quality (Transparency International) - Economic development (GDP, HDI)</p> <p>Cases: Eastern Europe, Latin America, Central Asia</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#topic-2-natural-resources-and-development","title":"Topic 2: Natural Resources and Development","text":"<p>Question: Does dependence on natural resources affect development?</p> <p>Key variables: - Oil/gas production (Ross dataset) - Natural resource rents (World Bank) - Access to basic services (water, sanitation) - Institutional quality</p> <p>Cases: Oil-producing countries, resource curse, water security</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#project-architecture","title":"Project Architecture","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#expected-structure","title":"Expected Structure","text":"<pre><code>tu_apellido_nombre/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 database/          # Conexi\u00f3n PostgreSQL\n\u2502   \u251c\u2500\u2500 etl/               # Extract, Transform, Load\n\u2502   \u251c\u2500\u2500 analysis/          # An\u00e1lisis de datos\n\u2502   \u2514\u2500\u2500 utils/             # Logging, helpers\n\u251c\u2500\u2500 scripts/               # CLI ejecutables\n\u251c\u2500\u2500 sql/                   # Queries complejas\n\u251c\u2500\u2500 tests/                 # Tests (opcional)\n\u2514\u2500\u2500 docs/                  # Documentaci\u00f3n\n</code></pre> <p>See details: Full architecture</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#technologies","title":"Technologies","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#required","title":"Required","text":"<ul> <li>Python 3.11+</li> <li>PostgreSQL 14+</li> <li>pandas - Data manipulation</li> <li>psycopg2 - PostgreSQL connection</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#recommended","title":"Recommended","text":"<ul> <li>SQLAlchemy - ORM</li> <li>pytest - Testing</li> <li>Black - Code formatting</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#ingestion-architecture","title":"Ingestion Architecture","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#data-cleaning-a-practical-approach","title":"Data Cleaning: A Practical Approach","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#project-phases","title":"Project Phases","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#1-extract-e","title":"1. Extract (E)","text":"<ul> <li>Download QoG dataset</li> <li>Filter by topic and period</li> <li>Validate integrity</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#2-transform-t","title":"2. Transform (T)","text":"<ul> <li>Rename columns</li> <li>Create derived variables</li> <li>Handle missing values</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#3-load-l","title":"3. Load (L)","text":"<ul> <li>Load into PostgreSQL</li> <li>Validate referential integrity</li> <li>Optimize with indexes</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#4-analysis","title":"4. Analysis","text":"<ul> <li>Descriptive statistics</li> <li>Prepare balanced panel</li> <li>Export for econometrics (.dta, .csv)</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#panel-data","title":"Panel Data","text":"<p>This exercise prepares you for econometric analysis.</p> <p>Panel data = Cross-section x Time-series</p> <pre><code>| country | year | democracy | gdp_pc |\n|---------|------|-----------|--------|\n| ESP     | 2000 | 0.85      | 24000  |\n| ESP     | 2001 | 0.86      | 24500  |\n</code></pre> <p>Allows for: - Fixed Effects (control for heterogeneity) - Random Effects - Difference-in-Differences - Dynamic models</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#technical-specifications","title":"Technical Specifications","text":"<p>All detailed technical documentation is in:</p> <pre><code>ejercicios/02_limpieza_datos/especificaciones/\n\u251c\u2500\u2500 ARQUITECTURA.md           # Estructura de proyecto\n\u251c\u2500\u2500 ESQUEMA_DB.sql            # Schema PostgreSQL completo\n\u251c\u2500\u2500 FUNCIONES_REQUERIDAS.md   # Firmas de funciones\n\u251c\u2500\u2500 VARIABLES_TEMA1.md        # Variables + prompts AI\n\u251c\u2500\u2500 VARIABLES_TEMA2.md        # Variables + prompts AI\n\u2514\u2500\u2500 VALIDACIONES.md           # Checks obligatorios\n</code></pre> <p>Especially useful: VARIABLES_TEMA*.md includes prompts for Claude/ChatGPT to research additional variables.</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#evaluation-criteria","title":"Evaluation Criteria","text":"Criterion Weight What We Evaluate Functionality 40% Pipeline runs without errors, correct data Architecture 25% Modular code, separation of concerns Code Quality 20% PEP 8, type hints, docstrings Documentation 10% README, methodology, comments Innovation 5% Tests, visualizations, additional analysis"},{"location":"en/ejercicios/02-pipeline-etl-qog/#deliverables","title":"Deliverables","text":"<p>Location: <code>entregas/02_limpieza_datos/tu_apellido_nombre/</code></p> <p>Minimum required: - Modular source code (src/) - Executable scripts (scripts/) - SQL queries (sql/) - Complete README - METODOLOGIA.md (design decisions) - requirements.txt</p> <p>DO NOT include: Data, logs, venv/, .env</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#support-resources","title":"Support Resources","text":""},{"location":"en/ejercicios/02-pipeline-etl-qog/#project-guides","title":"Project Guides","text":"<ul> <li>Setup PostgreSQL</li> <li>Submission Instructions</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>PostgreSQL Docs</li> <li>pandas Docs</li> <li>psycopg2 Docs</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#docker-preparation","title":"Docker Preparation","text":"<p>This project is designed to be dockerized in future exercises.</p> <p>Your modular architecture will facilitate: - PostgreSQL container - Python application container - docker-compose orchestration</p> <p>For now: Local PostgreSQL installation.</p>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#how-to-get-started","title":"How to Get Started","text":"<ol> <li>Read all the documentation in <code>ejercicios/02_limpieza_datos/</code></li> <li>Install PostgreSQL (see POSTGRESQL_SETUP.md)</li> <li>Choose a topic (1 or 2)</li> <li>Research variables using prompts in VARIABLES_TEMA*.md</li> <li>Implement step by step: Extract \u2192 Transform \u2192 Load \u2192 Analysis</li> <li>Test frequently</li> <li>Document as you code</li> </ol>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#tips","title":"Tips","text":"<ul> <li>Start with a basic pipeline, optimize later</li> <li>Logging everywhere (your best friend for debugging)</li> <li>Frequent Git commits</li> <li>Test with a small subset first (not 1M rows at once)</li> <li>Read the QoG codebook - it is your bible</li> <li>Ask early if something is unclear</li> </ul>"},{"location":"en/ejercicios/02-pipeline-etl-qog/#faq","title":"FAQ","text":"<p>Q: Individual or group? A: Your choice. Groups of 2-5 people.</p> <p>Q: Do I have to use ALL the suggested variables? A: No, they are suggestions. Research and choose the relevant ones.</p> <p>Q: Can I use Docker? A: Not for this exercise. Local PostgreSQL. Docker will come later.</p> <p>Q: What if I cannot find a variable? A: Use the AI prompts in VARIABLES_TEMA*.md to search for alternatives.</p> <p>Publication date: To be defined Last updated: 2025-12-17</p>"},{"location":"en/ejercicios/02-postgresql-hr/","title":"Exercise 2.1: PostgreSQL with HR Database","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/02-postgresql-hr/#general-description","title":"General Description","text":"<p>You will learn to work with PostgreSQL using the HR (Human Resources) database adapted from Oracle.</p> <p>Estimated duration: 4-6 hours Level: Intermediate Prerequisites: Basic SQL, having completed Exercise 1.1</p>"},{"location":"en/ejercicios/02-postgresql-hr/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this exercise you will be able to:</p> <ul> <li>Install and configure PostgreSQL on your system</li> <li>Create databases and users in PostgreSQL</li> <li>Load schemas and data from SQL scripts</li> <li>Perform complex queries with multiple JOINs</li> <li>Use PostgreSQL-specific functions</li> <li>Compare SQL syntax: Oracle vs PostgreSQL</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#hr-database","title":"HR Database","text":"<p>The HR database is an official Oracle sample database that models a human resources management system.</p>"},{"location":"en/ejercicios/02-postgresql-hr/#main-entities","title":"Main Entities","text":"<ul> <li>Employees - Employee information</li> <li>Departments - Company departments</li> <li>Jobs - Job positions</li> <li>Locations - Geographic locations</li> <li>Countries - Countries</li> <li>Regions - Regions</li> <li>Job_History - Employee work history</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#simplified-er-diagram","title":"Simplified ER Diagram","text":"<pre><code>erDiagram\n    REGIONS ||--o{ COUNTRIES : contains\n    COUNTRIES ||--o{ LOCATIONS : has\n    LOCATIONS ||--o{ DEPARTMENTS : located_in\n    DEPARTMENTS ||--o{ EMPLOYEES : employs\n    EMPLOYEES ||--o| EMPLOYEES : manages\n    JOBS ||--o{ EMPLOYEES : has_role</code></pre>"},{"location":"en/ejercicios/02-postgresql-hr/#technical-requirements","title":"Technical Requirements","text":""},{"location":"en/ejercicios/02-postgresql-hr/#required-software","title":"Required Software","text":"<ol> <li>PostgreSQL 14+</li> <li>Download for Windows</li> <li>Download for Mac</li> <li> <p>Download for Linux</p> </li> <li> <p>SQL Client (choose one):</p> </li> <li>pgAdmin (included with PostgreSQL)</li> <li>DBeaver (recommended for beginners)</li> <li> <p>VS Code with PostgreSQL extension</p> </li> <li> <p>Python (optional):</p> </li> <li>psycopg2 for connecting from Python    <pre><code>pip install psycopg2-binary\n</code></pre></li> </ol>"},{"location":"en/ejercicios/02-postgresql-hr/#exercise-content","title":"Exercise Content","text":"<p>The complete exercise is located at:</p> <pre><code>ejercicios/01_bases_de_datos/2.1_postgresql_hr/\n</code></pre>"},{"location":"en/ejercicios/02-postgresql-hr/#structure","title":"Structure","text":"<ul> <li><code>README.md</code> - Detailed instructions</li> <li><code>scripts/</code> - SQL scripts (to be completed by students)</li> <li><code>soluciones/</code> - Reference solutions</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#data","title":"Data","text":"<p>HR database scripts are located at: <code>datos/oracle_hr/</code></p>"},{"location":"en/ejercicios/02-postgresql-hr/#topics-covered","title":"Topics Covered","text":""},{"location":"en/ejercicios/02-postgresql-hr/#1-installation-and-configuration","title":"1. Installation and Configuration","text":"<ul> <li>Install PostgreSQL</li> <li>Create user and database</li> <li>Configure connection</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#2-data-loading","title":"2. Data Loading","text":"<ul> <li>Execute DDL scripts (structure)</li> <li>Execute DML scripts (data)</li> <li>Verify integrity</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#3-basic-queries","title":"3. Basic Queries","text":"<ul> <li>SELECT with filters</li> <li>Sorting and limits</li> <li>Aggregate functions</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#4-advanced-queries","title":"4. Advanced Queries","text":"<ul> <li>Multiple JOINs</li> <li>Subqueries</li> <li>CTEs (Common Table Expressions)</li> <li>Window Functions</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#5-business-analysis","title":"5. Business Analysis","text":"<ul> <li>Salaries by department</li> <li>Employee hierarchies</li> <li>Work history</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#submissions","title":"Submissions","text":"<p>Check the submission instructions to know which files to upload.</p> <p>Submission folder: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.1_postgresql_hr/\n</code></pre></p>"},{"location":"en/ejercicios/02-postgresql-hr/#support-resources","title":"Support Resources","text":""},{"location":"en/ejercicios/02-postgresql-hr/#official-documentation","title":"Official Documentation","text":"<ul> <li>PostgreSQL Documentation</li> <li>PostgreSQL Tutorial</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#tutorials","title":"Tutorials","text":"<ul> <li>PostgreSQL Installation Guide</li> <li>Advanced SQL in PostgreSQL</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#comparisons","title":"Comparisons","text":"<ul> <li>Oracle vs PostgreSQL - Syntax Differences</li> </ul>"},{"location":"en/ejercicios/02-postgresql-hr/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Exercise 2.2 - PostgreSQL Jardineria (more complex queries)</li> <li>Exercise 2.3 - Migration from SQLite to PostgreSQL</li> <li>Exercise 3.1 - Oracle with HR Database (compare with PostgreSQL)</li> </ol> <p>Publication date: To be defined Last updated: 2025-12-17</p>"},{"location":"en/ejercicios/03-postgresql-jardineria/","title":"Exercise 2.2: PostgreSQL with Jardineria Database","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/03-postgresql-jardineria/#general-description","title":"General Description","text":"<p>You will practice advanced SQL queries with a garden sales management database.</p> <p>Estimated duration: 4-6 hours Level: Intermediate Prerequisites: Basic SQL, Exercise 2.1 (PostgreSQL HR)</p>"},{"location":"en/ejercicios/03-postgresql-jardineria/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this exercise you will be able to:</p> <ul> <li>Analyze sales data with SQL</li> <li>Generate business reports</li> <li>Use complex aggregations (GROUP BY, HAVING)</li> <li>Apply Window Functions for rankings and temporal analysis</li> <li>Optimize queries with indexes</li> <li>Create materialized views</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#jardineria-database","title":"Jardineria Database","text":"<p>Database for a company that sells gardening products and plants.</p>"},{"location":"en/ejercicios/03-postgresql-jardineria/#main-entities","title":"Main Entities","text":"<ul> <li>Clientes - Company clients</li> <li>Empleados - Organization and sales</li> <li>Oficinas - Company locations</li> <li>Pedidos - Purchase orders</li> <li>Detalle_Pedidos - Line items for each order</li> <li>Productos - Product catalog</li> <li>Gamas_Producto - Categories</li> <li>Pagos - Transactions</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#er-diagram","title":"ER Diagram","text":"<pre><code>erDiagram\n    OFICINAS ||--o{ EMPLEADOS : trabaja_en\n    EMPLEADOS ||--o{ CLIENTES : atiende\n    CLIENTES ||--o{ PEDIDOS : realiza\n    CLIENTES ||--o{ PAGOS : hace\n    GAMAS_PRODUCTO ||--o{ PRODUCTOS : tiene\n    PEDIDOS ||--o{ DETALLE_PEDIDOS : contiene\n    PRODUCTOS ||--o{ DETALLE_PEDIDOS : incluido_en</code></pre>"},{"location":"en/ejercicios/03-postgresql-jardineria/#real-world-use-cases","title":"Real-World Use Cases","text":"<p>This exercise simulates analyses you would perform in a real company:</p>"},{"location":"en/ejercicios/03-postgresql-jardineria/#1-sales-analysis","title":"1. Sales Analysis","text":"<ul> <li>Total sales by client</li> <li>Best-selling products</li> <li>Sales trends by month/year</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#2-client-management","title":"2. Client Management","text":"<ul> <li>Clients with highest purchase volume</li> <li>Inactive clients (no recent orders)</li> <li>Geographic distribution</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#3-employee-performance","title":"3. Employee Performance","text":"<ul> <li>Sales per employee</li> <li>Clients assigned per employee</li> <li>Performance by office</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#4-inventory","title":"4. Inventory","text":"<ul> <li>Low-stock products</li> <li>Turnover analysis</li> <li>Products with no sales</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>PostgreSQL 14+ installed (from Exercise 2.1)</li> <li>SQL Client (pgAdmin, DBeaver)</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#exercise-content","title":"Exercise Content","text":"<p>The complete exercise is located at:</p> <pre><code>ejercicios/01_bases_de_datos/2.2_postgresql_jardineria/\n</code></pre>"},{"location":"en/ejercicios/03-postgresql-jardineria/#data","title":"Data","text":"<p>SQL scripts are located at: <code>datos/jardineria/</code></p>"},{"location":"en/ejercicios/03-postgresql-jardineria/#topics-covered","title":"Topics Covered","text":""},{"location":"en/ejercicios/03-postgresql-jardineria/#1-analytical-queries","title":"1. Analytical Queries","text":"<ul> <li>Aggregations with GROUP BY</li> <li>Filtering with HAVING</li> <li>Multiple JOINs</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#2-window-functions","title":"2. Window Functions","text":"<ul> <li>ROW_NUMBER() for rankings</li> <li>LAG/LEAD for temporal comparisons</li> <li>PARTITION BY for group analysis</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#3-subqueries","title":"3. Subqueries","text":"<ul> <li>Correlated subqueries</li> <li>EXISTS / NOT EXISTS</li> <li>IN / NOT IN</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#4-optimization","title":"4. Optimization","text":"<ul> <li>EXPLAIN for analyzing execution plans</li> <li>Index creation</li> <li>Performance analysis</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#5-views","title":"5. Views","text":"<ul> <li>Simple views</li> <li>Materialized views</li> <li>View updates</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#submissions","title":"Submissions","text":"<p>Check the submission instructions to know which files to upload.</p> <p>Submission folder: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.2_postgresql_jardineria/\n</code></pre></p>"},{"location":"en/ejercicios/03-postgresql-jardineria/#support-resources","title":"Support Resources","text":""},{"location":"en/ejercicios/03-postgresql-jardineria/#documentation","title":"Documentation","text":"<ul> <li>PostgreSQL Window Functions</li> <li>Query Optimization</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#tutorials","title":"Tutorials","text":"<ul> <li>Sales Analysis with SQL</li> <li>Window Functions Explained</li> </ul>"},{"location":"en/ejercicios/03-postgresql-jardineria/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Exercise 2.3 - Migration SQLite to PostgreSQL</li> <li>Exercise 3.2 - Oracle Jardineria (compare implementation)</li> </ol> <p>Publication date: To be defined Last updated: 2025-12-17</p>"},{"location":"en/ejercicios/03-procesamiento-distribuido/","title":"Exercise 03: Distributed Processing with Dask","text":"<p>In this module you will learn to scale your computing capacity beyond your machine's RAM, using local clusters.</p> <p></p>"},{"location":"en/ejercicios/03-procesamiento-distribuido/#objectives","title":"Objectives","text":"<ol> <li>Set up a Local Cluster with Dask.</li> <li>Read Parquet files in a partitioned manner.</li> <li>Execute complex aggregations (GroupBy) in parallel.</li> </ol>"},{"location":"en/ejercicios/03-procesamiento-distribuido/#hadoop-the-democratization-of-processing","title":"Hadoop: The Democratization of Processing","text":""},{"location":"en/ejercicios/03-procesamiento-distribuido/#instructions","title":"Instructions","text":"<p>The main script is located at <code>ejercicios/03_procesamiento_distribuido/esqueleto.py</code>. Your task is to complete the functions marked with <code>TODO</code> to build a functional pipeline.</p>"},{"location":"en/ejercicios/03-procesamiento-distribuido/#programming-task","title":"Programming Task","text":"<p>You must implement the <code>procesamiento_dask()</code> function so that it: 1. Starts a local client (<code>LocalCluster</code>). 2. Reads the QoG dataset processed in the previous exercise. 3. Calculates the annual average of the Democracy Index. 4. Compares execution time vs traditional Pandas.</p>"},{"location":"en/ejercicios/04-machine-learning/","title":"Module 04: Machine Learning","text":"<p>Machine learning techniques applied to Big Data: from traditional clustering to Computer Vision with Deep Learning.</p>"},{"location":"en/ejercicios/04-machine-learning/#exercise-41-pca-and-clustering","title":"Exercise 4.1: PCA and Clustering","text":"<p>We apply unsupervised learning techniques to detect patterns in complex data.</p>"},{"location":"en/ejercicios/04-machine-learning/#scope","title":"Scope","text":"<ul> <li>Dimensionality Reduction: Principal Component Analysis (PCA)</li> <li>Clustering: K-Means and Hierarchical Clustering (HCA)</li> </ul>"},{"location":"en/ejercicios/04-machine-learning/#tasks","title":"Tasks","text":"<ol> <li>PCA: Reduce variables to 2 principal components for visualization</li> <li>Clustering: Implement K-Means and determine optimal K (elbow method, Silhouette)</li> <li>Interpretation: Generate a profile for each cluster</li> </ol>"},{"location":"en/ejercicios/04-machine-learning/#resources","title":"Resources","text":"<ul> <li>Dashboard PCA + K-Means Iris</li> <li>Dashboard PCA FactoMineR style</li> </ul>"},{"location":"en/ejercicios/04-machine-learning/#exercise-42-transfer-learning-flower-classification","title":"Exercise 4.2: Transfer Learning - Flower Classification","text":"<p>Computer Vision pipeline that classifies flower images using Transfer Learning with MobileNetV2.</p>"},{"location":"en/ejercicios/04-machine-learning/#what-is-transfer-learning","title":"What is Transfer Learning?","text":"<p>Instead of training a neural network from scratch (which would require millions of images), we use a network already trained on ImageNet and adapt it:</p> <pre><code>ImageNet (14M imgs) \u2192 MobileNetV2 \u2192 Embeddings (1280D) \u2192 ML Classifier\n</code></pre> <p>The first layers of the CNN have already learned universal patterns (edges, textures, shapes) that are useful for any image.</p>"},{"location":"en/ejercicios/04-machine-learning/#pipeline","title":"Pipeline","text":"<pre><code>1. DOWNLOAD           2. EMBEDDINGS         3. CLASSIFICATION     4. VISUALIZATION\n   3,670 flowers         MobileNetV2           Traditional ML        Dashboard\n   5 classes             1280 features         KNN/SVM/RF            Plotly\n</code></pre>"},{"location":"en/ejercicios/04-machine-learning/#results","title":"Results","text":"Model Accuracy SVM 89.9% Random Forest 86.5% KNN 86.2%"},{"location":"en/ejercicios/04-machine-learning/#run","title":"Run","text":"<pre><code>cd ejercicios/04_machine_learning/flores_transfer_learning/\npip install -r requirements.txt\npython 01_flores_transfer_learning.py\n</code></pre> <p>Requirements: TensorFlow (GPU recommended but works on CPU)</p>"},{"location":"en/ejercicios/04-machine-learning/#resources_1","title":"Resources","text":"<ul> <li>Transfer Learning Flowers Dashboard - Gallery, t-SNE, Comparison, Confusion Matrix</li> <li>Interactive Dashboard</li> </ul>"},{"location":"en/ejercicios/04-machine-learning/#exercise-43-time-series-arimasarima","title":"Exercise 4.3: Time Series (ARIMA/SARIMA)","text":"<p>Box-Jenkins methodology for time series analysis and forecasting.</p>"},{"location":"en/ejercicios/04-machine-learning/#resources_2","title":"Resources","text":"<ul> <li>ARIMA/SARIMA - Box-Jenkins Methodology</li> <li>Interactive ARIMA Dashboard</li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c</p> <p>Academic references:</p> <ul> <li>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12.</li> <li>Sandler, M., et al. (2018). MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR.</li> <li>van der Maaten, L. &amp; Hinton, G. (2008). Visualizing Data using t-SNE. JMLR.</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/","title":"Exercise 2.3: SQLite to PostgreSQL Migration","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#general-description","title":"General Description","text":"<p>You will learn to migrate databases from SQLite to PostgreSQL, understanding the differences between both engines.</p> <p>Estimated duration: 3-4 hours Level: Intermediate-Advanced Prerequisites: Exercise 1.1 (SQLite), Exercise 2.1 (PostgreSQL)</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this exercise you will be able to:</p> <ul> <li>\u2705 Identify differences between SQLite and PostgreSQL</li> <li>\u2705 Adapt data types between engines</li> <li>\u2705 Migrate schemas (DDL)</li> <li>\u2705 Migrate data (DML)</li> <li>\u2705 Write migration scripts in Python</li> <li>\u2705 Validate integrity after migration</li> <li>\u2705 Compare performance between engines</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#exercise-context","title":"Exercise Context","text":"<p>You will use the databases you created in Exercise 1.1 (computer store):</p> <ul> <li><code>tienda_modelo_a.db</code> - 26 independent tables</li> <li><code>tienda_modelo_b.db</code> - Normalized model</li> <li><code>tienda_modelo_c.db</code> - Complete e-commerce</li> </ul> <p>Goal: Migrate them to PostgreSQL and compare.</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#key-differences","title":"Key Differences","text":""},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#data-types","title":"Data Types","text":"SQLite PostgreSQL <code>INTEGER</code> <code>INTEGER</code> or <code>BIGINT</code> <code>REAL</code> <code>NUMERIC(p,s)</code> or <code>DOUBLE PRECISION</code> <code>TEXT</code> <code>VARCHAR(n)</code> or <code>TEXT</code> <code>BLOB</code> <code>BYTEA</code> No type PostgreSQL is strict"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#auto-increment","title":"Auto-increment","text":"SQLite PostgreSQL <code>AUTOINCREMENT</code> <code>SERIAL</code> or <code>IDENTITY</code>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#constraints","title":"Constraints","text":"<p>PostgreSQL has stricter constraints and supports more types.</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#performance","title":"Performance","text":"<p>PostgreSQL is much faster with large volumes and complex queries.</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#exercise-content","title":"Exercise Content","text":"<p>The complete exercise is located at:</p> <pre><code>ejercicios/01_bases_de_datos/2.3_postgresql_tienda/\n</code></pre>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#files","title":"Files","text":"<ul> <li><code>migracion_desde_sqlite.py</code> - Migration script (template)</li> <li><code>comparativa_sqlite_vs_postgres.md</code> - Comparative analysis</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#migration-process","title":"Migration Process","text":""},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#1-source-schema-analysis","title":"1. Source Schema Analysis","text":"<pre><code># Conectar a SQLite\nimport sqlite3\nconn = sqlite3.connect('tienda_modelo_b.db')\n\n# Obtener lista de tablas\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n</code></pre>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#2-data-type-adaptation","title":"2. Data Type Adaptation","text":"<p>Convert SQLite types to PostgreSQL: - Dynamically detect types - Map to equivalent PostgreSQL types - Adjust NUMERIC precision</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#3-schema-recreation","title":"3. Schema Recreation","text":"<p>Generate DDL for PostgreSQL: - CREATE TABLE with adapted types - PRIMARY KEY - FOREIGN KEY - CONSTRAINTS</p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#4-data-migration","title":"4. Data Migration","text":"<pre><code># Leer datos de SQLite\ndf = pd.read_sql(\"SELECT * FROM productos\", sqlite_conn)\n\n# Insertar en PostgreSQL\ndf.to_sql('productos', postgres_engine, if_exists='append')\n</code></pre>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#5-validation","title":"5. Validation","text":"<ul> <li>Verify row counts: SQLite vs PostgreSQL</li> <li>Validate referential integrity</li> <li>Test complex queries</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#tasks-to-complete","title":"Tasks to Complete","text":""},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#part-1-migration-script","title":"Part 1: Migration Script","text":"<p>Create <code>migracion_desde_sqlite.py</code> that:</p> <ol> <li>Reads the SQLite schema</li> <li>Generates DDL for PostgreSQL</li> <li>Migrates data table by table</li> <li>Validates the migration</li> <li>Generates a success/error report</li> </ol>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#part-2-comparative-analysis","title":"Part 2: Comparative Analysis","text":"<p>Create <code>comparativa_sqlite_vs_postgres.md</code> with:</p> <ol> <li>Schema Differences</li> <li>Modified data types</li> <li>Added constraints</li> <li> <p>Created indexes</p> </li> <li> <p>Performance Tests</p> </li> <li>Same query on both engines</li> <li>Execution times</li> <li> <p>Memory usage</p> </li> <li> <p>Conclusions</p> </li> <li>When to use SQLite?</li> <li>When to use PostgreSQL?</li> <li>Recommendations</li> </ol>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#technical-requirements","title":"Technical Requirements","text":""},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#software","title":"Software","text":"<ul> <li>SQLite (already installed)</li> <li>PostgreSQL 14+</li> <li>Python with:   <pre><code>pip install pandas psycopg2-binary sqlite3\n</code></pre></li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#source-data","title":"Source Data","text":"<p>Your databases from Exercise 1.1: - <code>tienda_modelo_a.db</code> - <code>tienda_modelo_b.db</code> - <code>tienda_modelo_c.db</code></p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#submissions","title":"Submissions","text":"<p>Check the submission instructions to find out which files you need to upload.</p> <p>Submission folder: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/2.3_postgresql_tienda/\n\u251c\u2500\u2500 migracion_desde_sqlite.py\n\u251c\u2500\u2500 comparativa_sqlite_vs_postgres.md\n\u2514\u2500\u2500 capturas/\n    \u251c\u2500\u2500 sqlite_query.png\n    \u2514\u2500\u2500 postgres_query.png\n</code></pre></p>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#support-resources","title":"Support Resources","text":""},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#migration-tools","title":"Migration Tools","text":"<ul> <li>pgloader - Automatic migration</li> <li>SQLite to PostgreSQL Converter</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#documentation","title":"Documentation","text":"<ul> <li>PostgreSQL Data Types</li> <li>Database Migration</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#tutorials","title":"Tutorials","text":"<ul> <li>SQLite vs PostgreSQL Comparison</li> <li>Psycopg2 Tutorial</li> </ul>"},{"location":"en/ejercicios/04-migracion-sqlite-postgresql/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Exercise 3.1 - Oracle with HR Database</li> <li>Exercise 4.1 - SQL Server with Store</li> </ol> <p>Publication date: To be defined Last updated: 2025-12-17</p>"},{"location":"en/ejercicios/05-nlp-mining/","title":"Module 05: NLP and Text Mining","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/05-nlp-mining/#general-description","title":"General Description","text":"<p>Introduction to unstructured data processing using Natural Language Processing techniques. From basic tokenization to sentiment analysis and document similarity.</p> <p>Level: Intermediate-Advanced Technologies: Python, NLTK, TF-IDF, Scikit-learn Prerequisites: Intermediate Python, basic statistics</p> <p></p>"},{"location":"en/ejercicios/05-nlp-mining/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand text tokenization and normalization</li> <li>Remove stopwords and apply noise filters</li> <li>Calculate document similarity with Jaccard Distance</li> <li>Vectorize text with TF-IDF</li> <li>Apply clustering on vectorized documents</li> <li>Perform basic sentiment analysis</li> </ul>"},{"location":"en/ejercicios/05-nlp-mining/#course-visual-material","title":"Course Visual Material","text":""},{"location":"en/ejercicios/05-nlp-mining/#beyond-bag-of-words","title":"Beyond Bag-of-Words","text":""},{"location":"en/ejercicios/05-nlp-mining/#the-challenge-of-multilingual-tokenization","title":"The Challenge of Multilingual Tokenization","text":""},{"location":"en/ejercicios/05-nlp-mining/#dependency-parsing-the-hierarchy-of-thought","title":"Dependency Parsing: The Hierarchy of Thought","text":""},{"location":"en/ejercicios/05-nlp-mining/#parsing-algorithms","title":"Parsing Algorithms","text":""},{"location":"en/ejercicios/05-nlp-mining/#key-concepts","title":"Key Concepts","text":"Concept Description Tokenization Splitting text into processable units (tokens) Stopwords Noise removal (articles, prepositions) Jaccard Similarity Mathematical measurement of how similar two documents are TF-IDF Vectorization that weights term importance in a corpus Sentiment Analysis Polar classification (Positive/Negative) based on lexicons Clustering Grouping documents by semantic similarity"},{"location":"en/ejercicios/05-nlp-mining/#module-content","title":"Module Content","text":"<p>The complete module is located at:</p> <pre><code>ejercicios/05_nlp_text_mining/\n\u251c\u2500\u2500 01_conteo_palabras.py          # Tokenization and counting\n\u251c\u2500\u2500 02_limpieza_texto.py           # Stopwords and filtering\n\u251c\u2500\u2500 03_sentimiento.py              # Sentiment analysis\n\u251c\u2500\u2500 04_similitud_jaccard.py        # Document similarity\n\u251c\u2500\u2500 05_vectorizacion_clustering.py # TF-IDF + K-Means\n\u2514\u2500\u2500 requirements.txt               # nltk, scikit-learn\n</code></pre>"},{"location":"en/ejercicios/05-nlp-mining/#practical-exercises","title":"Practical Exercises","text":""},{"location":"en/ejercicios/05-nlp-mining/#01-word-count","title":"01 - Word Count","text":"<p>Basic tokenization, normalization, and term frequency.</p>"},{"location":"en/ejercicios/05-nlp-mining/#02-stopword-filtering","title":"02 - Stopword Filtering","text":"<p>Noise removal to reveal words with real weight.</p>"},{"location":"en/ejercicios/05-nlp-mining/#03-sentiment-analysis","title":"03 - Sentiment Analysis","text":"<p>Polar classification using predefined lexicons.</p>"},{"location":"en/ejercicios/05-nlp-mining/#04-jaccard-similarity","title":"04 - Jaccard Similarity","text":"<p>Compare documents using Jaccard Distance.</p> <p>Main Challenge: Implement a system that compares public policy descriptions from different countries and detects which ones are semantically similar.</p>"},{"location":"en/ejercicios/05-nlp-mining/#05-vectorization-and-clustering","title":"05 - Vectorization and Clustering","text":"<p>TF-IDF for numerical representation + K-Means for grouping.</p>"},{"location":"en/ejercicios/05-nlp-mining/#dashboards","title":"Dashboards","text":"<p>Explore the results visually:</p> <ul> <li>Word Count</li> <li>Stopword Filtering</li> <li>Jaccard Similarity</li> <li>Vectorization and Clustering</li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic references:</p> <ul> <li>Jurafsky, D., &amp; Martin, J. H. (2024). Speech and Language Processing (3<sup>rd</sup> ed.). Prentice Hall.</li> <li>Manning, C. D., Raghavan, P., &amp; Schutze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.</li> <li>Bird, S., Klein, E., &amp; Loper, E. (2009). Natural Language Processing with Python. O'Reilly Media.</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/","title":"Exercise 3.1: Oracle with HR Database","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/05-oracle-hr/#general-description","title":"General Description","text":"<p>You will work with Oracle Database using the HR database in its original native environment.</p> <p>Estimated duration: 5-7 hours Level: Advanced Prerequisites: Exercise 2.1 (PostgreSQL HR)</p>"},{"location":"en/ejercicios/05-oracle-hr/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this exercise you will be able to:</p> <ul> <li>\u2705 Install and configure Oracle Database Express Edition (XE)</li> <li>\u2705 Use SQL Developer or SQL*Plus</li> <li>\u2705 Work with Oracle-specific syntax</li> <li>\u2705 Create sequences and triggers</li> <li>\u2705 Write stored procedures in PL/SQL</li> <li>\u2705 Compare Oracle with PostgreSQL</li> <li>\u2705 Understand Oracle enterprise features</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#oracle-database","title":"Oracle Database","text":"<p>Oracle is the leading relational database engine in the enterprise market.</p> <p>Features: - PL/SQL (procedural language) - Advanced partitioning - Replication and high availability - Enterprise security - Very powerful query optimizer</p>"},{"location":"en/ejercicios/05-oracle-hr/#differences-oracle-vs-postgresql","title":"Differences: Oracle vs PostgreSQL","text":""},{"location":"en/ejercicios/05-oracle-hr/#syntax","title":"Syntax","text":"Aspect Oracle PostgreSQL Auto-increment SEQUENCE SERIAL String concat <code>\\|\\|</code> or <code>CONCAT()</code> <code>\\|\\|</code> VARCHAR types <code>VARCHAR2</code> <code>VARCHAR</code> LIMIT <code>ROWNUM</code> or <code>FETCH FIRST</code> <code>LIMIT</code> Outer Join <code>(+)</code> (legacy) <code>LEFT/RIGHT JOIN</code>"},{"location":"en/ejercicios/05-oracle-hr/#functionality","title":"Functionality","text":"<ul> <li>PL/SQL vs PL/pgSQL: Oracle has a more mature PL/SQL</li> <li>Packages: Oracle supports packages (grouping of procedures)</li> <li>Triggers: Different syntax</li> <li>Performance: Oracle optimized for enterprise workloads</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#exercise-content","title":"Exercise Content","text":"<p>The complete exercise is located at:</p> <pre><code>ejercicios/01_bases_de_datos/3.1_oracle_hr/\n</code></pre>"},{"location":"en/ejercicios/05-oracle-hr/#data","title":"Data","text":"<p>Original Oracle SQL scripts are in: <code>datos/oracle_hr/</code></p>"},{"location":"en/ejercicios/05-oracle-hr/#topics-covered","title":"Topics Covered","text":""},{"location":"en/ejercicios/05-oracle-hr/#1-installation-and-configuration","title":"1. Installation and Configuration","text":"<ul> <li>Install Oracle XE 21c</li> <li>Configure listener</li> <li>Create users and permissions</li> <li>Connect with SQL Developer</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#2-oracle-syntax","title":"2. Oracle Syntax","text":"<ul> <li>Specific data types</li> <li>Oracle built-in functions</li> <li>ROWNUM and pagination</li> <li>Optimizer hints</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#3-basic-plsql","title":"3. Basic PL/SQL","text":"<ul> <li>Anonymous blocks</li> <li>Variables and types</li> <li>Control structures</li> <li>Exception handling</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#4-database-objects","title":"4. Database Objects","text":"<ul> <li>Sequences</li> <li>Triggers</li> <li>Views</li> <li>Synonyms</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#5-procedures-and-functions","title":"5. Procedures and Functions","text":"<ul> <li>Create stored procedures</li> <li>IN/OUT/IN OUT parameters</li> <li>Functions that return values</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#technical-requirements","title":"Technical Requirements","text":""},{"location":"en/ejercicios/05-oracle-hr/#required-software","title":"Required Software","text":"<ol> <li>Oracle Database 21c Express Edition (XE) - Free</li> <li>Download Oracle XE</li> <li>Requires an Oracle account (free)</li> <li> <p>XE limitations: 12GB RAM, 2 CPUs, 12GB user data</p> </li> <li> <p>SQL Developer - Official Oracle graphical client</p> </li> <li>Download SQL Developer</li> <li> <p>Alternative: DBeaver with Oracle driver</p> </li> <li> <p>Oracle Instant Client (optional for remote connections)</p> </li> </ol>"},{"location":"en/ejercicios/05-oracle-hr/#operating-system","title":"Operating System","text":"<ul> <li>Windows: Direct installation</li> <li>Mac/Linux: Use Docker   <pre><code>docker pull container-registry.oracle.com/database/express:21.3.0-xe\n</code></pre></li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#submissions","title":"Submissions","text":"<p>Check the submission instructions to find out which files you need to upload.</p> <p>Submission folder: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/3.1_oracle_hr/\n</code></pre></p>"},{"location":"en/ejercicios/05-oracle-hr/#support-resources","title":"Support Resources","text":""},{"location":"en/ejercicios/05-oracle-hr/#official-documentation","title":"Official Documentation","text":"<ul> <li>Oracle Database Documentation</li> <li>PL/SQL Language Reference</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#tutorials","title":"Tutorials","text":"<ul> <li>Oracle Live SQL - Free online practice</li> <li>PL/SQL Tutorial</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#comparisons","title":"Comparisons","text":"<ul> <li>Oracle vs PostgreSQL</li> <li>Oracle to PostgreSQL Migration</li> </ul>"},{"location":"en/ejercicios/05-oracle-hr/#next-steps","title":"Next Steps","text":"<p>After completing this exercise:</p> <ol> <li>Exercise 3.2 - Oracle Gardening (more practice with Oracle)</li> <li>Exercise 4.1 - SQL Server (another enterprise engine)</li> </ol> <p>Publication date: To be defined Last updated: 2025-12-17</p>"},{"location":"en/ejercicios/06-analisis-excel-python/","title":"Exercise 5.1: Data Analysis with Excel and Python","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/06-analisis-excel-python/#general-description","title":"General Description","text":"<p>You will learn to analyze Excel data using Python, comparing manual vs automated analysis.</p> <p>Estimated duration: 3-4 hours Level: Basic-Intermediate Prerequisites: Basic Python, pandas</p>"},{"location":"en/ejercicios/06-analisis-excel-python/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this exercise you will be able to:</p> <ul> <li>\u2705 Read Excel files with pandas and openpyxl</li> <li>\u2705 Perform exploratory data analysis (EDA)</li> <li>\u2705 Generate descriptive statistics</li> <li>\u2705 Create visualizations (charts)</li> <li>\u2705 Automate analysis you would do manually in Excel</li> <li>\u2705 Export results to formatted Excel</li> <li>\u2705 Compare manual vs programmatic analysis</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#data-file","title":"Data File","text":"<p>You will work with: <code>datos/Ejercicio-de-Excel-resuelto-nivel-medio.xlsx</code></p> <p>This file contains real data that you would normally analyze in Excel.</p>"},{"location":"en/ejercicios/06-analisis-excel-python/#exercise-content","title":"Exercise Content","text":"<p>The complete exercise is in:</p> <pre><code>ejercicios/01_bases_de_datos/5.1_analisis_excel/\n</code></pre>"},{"location":"en/ejercicios/06-analisis-excel-python/#files","title":"Files","text":"<ul> <li><code>analisis_exploratorio.py</code> - Script template</li> <li><code>INSTRUCCIONES.md</code> - Step-by-step guide</li> <li><code>informe_analisis.md</code> - Template for your report</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#analysis-process","title":"Analysis Process","text":""},{"location":"en/ejercicios/06-analisis-excel-python/#1-initial-exploration","title":"1. Initial Exploration","text":"<pre><code>import pandas as pd\n\n# Leer Excel\ndf = pd.read_excel('datos/Ejercicio-de-Excel-resuelto-nivel-medio.xlsx')\n\n# Ver estructura\nprint(df.info())\nprint(df.describe())\nprint(df.head())\n</code></pre>"},{"location":"en/ejercicios/06-analisis-excel-python/#2-data-cleaning","title":"2. Data Cleaning","text":"<ul> <li>Detect null values</li> <li>Fix data types</li> <li>Remove duplicates</li> <li>Normalize text</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#3-descriptive-statistics","title":"3. Descriptive Statistics","text":"<ul> <li>Measures of central tendency (mean, median)</li> <li>Dispersion (standard deviation, quartiles)</li> <li>Correlations</li> <li>Distributions</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#4-visualizations","title":"4. Visualizations","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Gr\u00e1fico de barras\ndf.groupby('categoria')['ventas'].sum().plot(kind='bar')\n\n# Histograma\ndf['precio'].hist(bins=20)\n\n# Heatmap de correlaci\u00f3n\nsns.heatmap(df.corr(), annot=True)\n</code></pre>"},{"location":"en/ejercicios/06-analisis-excel-python/#5-export-results","title":"5. Export Results","text":"<pre><code># Crear Excel con formato\nwith pd.ExcelWriter('analisis_resultados.xlsx', engine='openpyxl') as writer:\n    df_resumen.to_excel(writer, sheet_name='Resumen')\n    df_detalle.to_excel(writer, sheet_name='Detalle')\n</code></pre>"},{"location":"en/ejercicios/06-analisis-excel-python/#tasks-to-complete","title":"Tasks to Complete","text":""},{"location":"en/ejercicios/06-analisis-excel-python/#part-1-exploratory-analysis","title":"Part 1: Exploratory Analysis","text":"<p>Create <code>analisis_exploratorio.py</code> that:</p> <ol> <li>Reads the Excel file</li> <li>Performs a complete EDA</li> <li>Generates descriptive statistics</li> <li>Creates visualizations</li> <li>Exports results to formatted Excel</li> </ol>"},{"location":"en/ejercicios/06-analisis-excel-python/#part-2-analysis-report","title":"Part 2: Analysis Report","text":"<p>Create <code>informe_analisis.md</code> with:</p> <ol> <li>Executive Summary</li> <li>Key findings</li> <li> <p>Key data points</p> </li> <li> <p>Detailed Analysis</p> </li> <li>Data structure</li> <li>Data quality</li> <li> <p>Patterns found</p> </li> <li> <p>Visualizations</p> </li> <li>Include generated charts</li> <li> <p>Interpret results</p> </li> <li> <p>Manual vs Automated Comparison</p> </li> <li>Which is faster?</li> <li>Which is more accurate?</li> <li> <p>When to use each?</p> </li> <li> <p>Conclusions</p> </li> </ol>"},{"location":"en/ejercicios/06-analisis-excel-python/#technical-requirements","title":"Technical Requirements","text":""},{"location":"en/ejercicios/06-analisis-excel-python/#python-libraries","title":"Python Libraries","text":"<pre><code>pip install pandas openpyxl matplotlib seaborn jupyter\n</code></pre>"},{"location":"en/ejercicios/06-analisis-excel-python/#optional-software","title":"Optional Software","text":"<ul> <li>Excel or LibreOffice Calc (to compare manual analysis)</li> <li>Jupyter Notebook (for interactive analysis)</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#comparison-excel-vs-python","title":"Comparison: Excel vs Python","text":""},{"location":"en/ejercicios/06-analisis-excel-python/#advantages-of-excel","title":"Advantages of Excel","text":"<ul> <li>\u2705 Intuitive visual interface</li> <li>\u2705 Fast for small ad-hoc analysis</li> <li>\u2705 No programming required</li> <li>\u2705 Easy interactive charts</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#advantages-of-python","title":"Advantages of Python","text":"<ul> <li>\u2705 Scalable to millions of rows</li> <li>\u2705 Reproducible (script = documentation)</li> <li>\u2705 Automatable</li> <li>\u2705 More advanced statistical analysis</li> <li>\u2705 Integration with databases</li> <li>\u2705 Version control (Git)</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#when-to-use-each","title":"When to Use Each?","text":"<p>Use Excel when: - Small dataset (&lt; 100k rows) - Quick one-time analysis - Non-technical audience</p> <p>Use Python when: - Large dataset (&gt; 100k rows) - Repetitive analysis - You need automation - Complex analysis</p>"},{"location":"en/ejercicios/06-analisis-excel-python/#submissions","title":"Submissions","text":"<p>Check the submission instructions to learn which files you need to upload.</p> <p>Submission folder: <pre><code>entregas/01_bases_de_datos/tu_apellido_nombre/5.1_analisis_excel/\n\u251c\u2500\u2500 analisis_exploratorio.py\n\u251c\u2500\u2500 informe_analisis.md\n\u251c\u2500\u2500 graficos/\n\u2502   \u251c\u2500\u2500 distribucion.png\n\u2502   \u251c\u2500\u2500 correlacion.png\n\u2502   \u2514\u2500\u2500 tendencias.png\n\u2514\u2500\u2500 analisis_resultados.xlsx\n</code></pre></p>"},{"location":"en/ejercicios/06-analisis-excel-python/#support-resources","title":"Support Resources","text":""},{"location":"en/ejercicios/06-analisis-excel-python/#documentation","title":"Documentation","text":"<ul> <li>Pandas Documentation</li> <li>Openpyxl Documentation</li> <li>Matplotlib Gallery</li> <li>Seaborn Examples</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#tutorials","title":"Tutorials","text":"<ul> <li>Pandas Tutorial</li> <li>Excel with Python</li> <li>Exploratory Analysis with Pandas</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#videos","title":"Videos","text":"<ul> <li>Data Analysis with Pandas</li> </ul>"},{"location":"en/ejercicios/06-analisis-excel-python/#next-steps","title":"Next Steps","text":"<p>After completing this exercise, you will have covered:</p> <ul> <li>Databases (SQLite, PostgreSQL, Oracle, SQL Server)</li> <li>Data analysis (Python + Excel)</li> </ul> <p>Next level: Big Data with PySpark, Dask, etc.</p> <p>Publication date: To be defined Last updated: 2025-12-17</p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/","title":"Capstone Project: Big Data Pipeline with Docker Infrastructure","text":"<p>Course: Big Data with Python - Prof. Juan Marcelo Gutierrez Miranda (@TodoEconometria)</p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#objective","title":"Objective","text":"<p>Build from scratch a data processing infrastructure using Docker, Apache Spark, and PostgreSQL. Starting from the Quality of Government (QoG) dataset, design and execute an ETL + analysis pipeline that answers a research question formulated by you.</p> <p>What is evaluated: Not just the code, but your learning process. You may use AI tools (ChatGPT, Copilot, Claude, etc.) but you must document how you used them and what you learned.</p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#dataset","title":"Dataset","text":"<p>Quality of Government Standard Dataset (QoG) - January 2024</p> <ul> <li>~15,500 rows (countries x years) x ~1,990 columns</li> <li>Variables: democracy, corruption, GDP, health, education, political stability...</li> <li>Documentation: QoG Data</li> </ul>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#structure-4-blocks","title":"Structure: 4 Blocks","text":""},{"location":"en/ejercicios/06-trabajo-final-capstone/#block-a-docker-infrastructure-30","title":"Block A: Docker Infrastructure (30%)","text":"<p>Write a <code>docker-compose.yml</code> that launches a mini-cluster:</p> Service Minimum requirement PostgreSQL Database to store results Spark Master Cluster coordinator Spark Worker At least 1 processing node <p>Steps:</p> <ol> <li>Research what Docker Compose is and how a YAML file is structured</li> <li>Write your <code>docker-compose.yml</code> with the 3 minimum services</li> <li>Add <code>healthcheck</code> at least for PostgreSQL</li> <li>Run <code>docker compose up -d</code> and verify everything starts</li> <li>Open the Spark UI and take a screenshot showing the connected worker</li> <li>Write <code>02_INFRAESTRUCTURA.md</code> explaining each section of your YAML in your own words</li> </ol> <p>Hints:</p> <ul> <li>Spark image: <code>apache/spark:3.5.4-python3</code> (or <code>bitnami/spark:3.5</code>)</li> <li>PostgreSQL image: <code>postgres:15-alpine</code></li> <li>The Spark Master uses port 7077 for communication and 8080 for the web UI</li> </ul> <p>Deliverables: <code>docker-compose.yml</code> + <code>02_INFRAESTRUCTURA.md</code></p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#block-b-etl-pipeline-with-spark-25","title":"Block B: ETL Pipeline with Spark (25%)","text":"<p>Write a Python script that processes QoG using Apache Spark.</p> <p>Steps:</p> <ol> <li>Choose 5 countries that interest you (they cannot be the ones from the instructor's example: KAZ, UZB, TKM, KGZ, TJK)</li> <li>Choose 5 numerical variables from the QoG dataset</li> <li>Formulate a research question</li> <li>Write <code>pipeline.py</code> that:<ul> <li>Creates a SparkSession</li> <li>Reads the CSV with <code>spark.read.csv()</code></li> <li>Selects your countries and variables</li> <li>Filters a range of years (e.g., 2000-2023)</li> <li>Creates at least 1 derived variable</li> <li>Saves the result as Parquet</li> </ul> </li> </ol> <p>Important: Your selection of countries and variables must be UNIQUE. If two students submit the same 5 countries, it will be considered plagiarism.</p> <p>Deliverable: <code>pipeline.py</code></p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#block-c-analysis-and-visualization-25","title":"Block C: Analysis and Visualization (25%)","text":"<p>Analyze your processed data and answer your research question.</p> <p>Choose ONE option:</p> Option What to do Example Clustering K-Means on your countries \"Which countries are similar based on democracy + GDP?\" Time series Evolution chart by country \"How did corruption change between 2000-2023?\" Comparison Before/after an event \"Did GDP change after the 2008 crisis?\" <p>Minimum requirements:</p> <ul> <li>2 charts (matplotlib, plotly, or seaborn)</li> <li>Each chart with title, labeled axes, and legend</li> <li>Interpretation paragraph for each chart</li> </ul> <p>Deliverable: Charts and interpretation in <code>03_RESULTADOS.md</code></p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#block-d-ai-reflection-3-key-moments-20","title":"Block D: AI Reflection - \"3 Key Moments\" (20%)","text":"<p>Document your learning process and share your prompts.</p> <p>For each block (A, B, C), answer:</p> Moment Question Start What was the first thing you asked the AI (or searched for)? Error What failed and how did you solve it? Learning What did you learn that you did NOT know before? <p>Additionally, paste the exact text of the AI prompt that helped you the most in each block.</p> <p>What is evaluated:</p> <ul> <li>That your prompts are real (pasted as-is, not made up afterward)</li> <li>That your answers are specific</li> <li>That the errors are real (documenting them does not lower your grade)</li> <li>That the process is consistent with your code</li> </ul> <p>Deliverable: <code>04_REFLEXION_IA.md</code></p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#comprehension-questions-mandatory","title":"Comprehension Questions (mandatory)","text":"<p>Answer in <code>05_RESPUESTAS.md</code>:</p> <ol> <li>Infrastructure: If your worker has 2 GB of RAM and the CSV weighs 3 GB, what happens? How would you solve it?</li> <li>ETL: Why does <code>spark.read.csv()</code> not execute anything until you call <code>.count()</code> or <code>.show()</code>?</li> <li>Analysis: Interpret your main chart: what pattern do you see and why do you think it occurs?</li> <li>Scalability: If you had to repeat this with a 50 GB dataset, what would you change in your infrastructure?</li> </ol>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#submission-format","title":"Submission Format","text":"<pre><code>entregas/trabajo_final/apellido_nombre/\n    PROMPTS.md                 &lt;- THE MOST IMPORTANT (your AI prompts)\n    01_README.md               &lt;- Your data + research question\n    02_INFRAESTRUCTURA.md      &lt;- YAML explanation\n    03_RESULTADOS.md           &lt;- Charts + interpretation\n    04_REFLEXION_IA.md         &lt;- 3 Key Moments x 3 blocks\n    05_RESPUESTAS.md           &lt;- 4 comprehension questions\n    docker-compose.yml         &lt;- Your working YAML\n    pipeline.py                &lt;- ETL + Analysis\n    requirements.txt           &lt;- Dependencies (pip freeze)\n    .gitignore                 &lt;- Exclude data, venv, __pycache__\n</code></pre> <p>Copy the template from <code>trabajo_final/plantilla/</code> to your submission folder.</p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#process-no-pull-request","title":"Process (NO Pull Request)","text":"<ol> <li>Sync your fork: <code>git fetch upstream &amp;&amp; git merge upstream/main</code></li> <li>Copy the template: <code>cp -r trabajo_final/plantilla/ entregas/trabajo_final/apellido_nombre/</code></li> <li>Fill in PROMPTS.md as you work - This file is what gets evaluated</li> <li>Complete the files (01 through 05) + <code>docker-compose.yml</code> + <code>pipeline.py</code></li> <li>Push to your fork: <code>git add . &amp;&amp; git commit -m \"Trabajo Final\" &amp;&amp; git push</code></li> <li>Done! The system evaluates your PROMPTS.md automatically</li> </ol> <p>You do not need to create a Pull Request</p> <p>The automated system evaluates your PROMPTS.md file directly in your fork. Just make sure to upload your work with <code>git push</code>.</p>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#prohibited-items","title":"Prohibited items","text":"<ul> <li>Data files (.csv, .parquet, .db)</li> <li>Virtual environments (venv/, .venv/)</li> <li>.env files with real credentials</li> <li>pycache/ folders</li> </ul>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#evaluation","title":"Evaluation","text":"Block Weight What is evaluated A. Infrastructure 30% Working YAML + explanation in your own words B. ETL Pipeline 25% Spark API + your own countries/variables + question C. Analysis 25% Charts + interpretation that answers your question D. AI Reflection 20% Real and specific learning process <p>Penalties:</p> <ul> <li>Copying the same countries/variables as another student: -50%</li> <li>Copying the countries from the instructor's example (Central Asia): -30%</li> <li>YAML that does not work without an explanation of why: -15%</li> <li>Absent or generic AI reflection: -20%</li> </ul>"},{"location":"en/ejercicios/06-trabajo-final-capstone/#resources","title":"Resources","text":"<ul> <li>Spark Documentation: spark.apache.org</li> <li>Docker Compose: docs.docker.com/compose</li> <li>QoG Codebook: qog.pol.gu.se (download codebook to see variables)</li> <li>Quick Start Guide: <code>trabajo_final/GUIA_INICIO_RAPIDO.md</code></li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic references:</p> <ul> <li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.</li> <li>Teorell, J., et al. (2024). The Quality of Government Standard Dataset. University of Gothenburg.</li> <li>Merkel, D. (2014). Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux Journal, 2014(239), 2.</li> </ul>"},{"location":"en/ejercicios/07-infraestructura-bigdata/","title":"Module 07: Big Data Infrastructure","text":"<p>Level: Intermediate-Advanced | Type: Theoretical-Conceptual with practical examples</p>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#objective","title":"Objective","text":"<p>Understand how the infrastructure is built that supports processing large volumes of data. Not just use the tools, but understand why they exist, how they connect to each other, and how they are orchestrated.</p> <p>This module lays the foundation for the Capstone Project, where each student builds their own Docker + Spark infrastructure from scratch.</p> <p></p>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#module-structure","title":"Module Structure","text":"Section Topic Question it answers 7.1 Docker Compose How do I package and connect services? 7.2 Apache Spark Cluster How do I process data in parallel? <p>Study order</p> <p>Study in order. Docker is the foundation on which Spark is built.</p>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#71-docker-compose-containers-and-orchestration","title":"7.1 Docker Compose: Containers and Orchestration","text":""},{"location":"en/ejercicios/07-infraestructura-bigdata/#the-problem-dependency-hell","title":"The Problem: Dependency Hell","text":""},{"location":"en/ejercicios/07-infraestructura-bigdata/#containers-vs-virtual-machines","title":"Containers vs Virtual Machines","text":""},{"location":"en/ejercicios/07-infraestructura-bigdata/#images-vs-containers","title":"Images vs Containers","text":""},{"location":"en/ejercicios/07-infraestructura-bigdata/#orchestration-with-docker-compose","title":"Orchestration with Docker Compose","text":""},{"location":"en/ejercicios/07-infraestructura-bigdata/#what-you-will-learn","title":"What you will learn","text":"<ul> <li>What Docker is and why it solves \"dependency hell\"</li> <li>Difference between containers and virtual machines</li> <li>Images, containers, and Dockerfile (layers, cache, Docker Hub)</li> <li>Docker Compose: orchestrate multiple services with a single YAML file</li> <li>Key directives: <code>services</code>, <code>image</code>, <code>build</code>, <code>ports</code>, <code>environment</code>, <code>volumes</code>, <code>networks</code>, <code>depends_on</code>, <code>healthcheck</code>, <code>restart</code></li> <li>Docker networking: types (bridge, host, overlay), internal DNS, communication by service name</li> <li>Volumes: named volumes, bind mounts, tmpfs, data persistence</li> <li>Essential commands: <code>docker compose up/down/ps/logs/exec</code></li> <li>Common errors: port in use, connection refused, lost data, depends_on without healthcheck</li> <li>Orchestration principle: Docker Compose vs Docker Swarm vs Kubernetes</li> <li>Useful patterns: <code>.env</code> files, <code>docker-compose.override.yml</code>, multi-stage builds, profiles</li> </ul>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#example-complete-docker-compose-stack","title":"Example: Complete Docker Compose Stack","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: bigdata_postgres\n    environment:\n      POSTGRES_USER: alumno\n      POSTGRES_PASSWORD: bigdata2026\n      POSTGRES_DB: curso_bigdata\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    networks:\n      - red_bigdata\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U alumno\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  pgadmin:\n    image: dpage/pgadmin4\n    ports:\n      - \"8080:80\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n    networks:\n      - red_bigdata\n\nvolumes:\n  pgdata:\n\nnetworks:\n  red_bigdata:\n    driver: bridge\n</code></pre>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#key-docker-concepts","title":"Key Docker Concepts","text":"Concept Description Image Immutable template with everything needed to run an app Container Running instance of an image Dockerfile Recipe to build an image (cacheable layers) Volume Data persistence beyond the container lifecycle Network Internal network for communication between containers (automatic DNS) Healthcheck Verification that a service is ready to accept connections"},{"location":"en/ejercicios/07-infraestructura-bigdata/#72-apache-spark-cluster-architecture-and-distributed-computing","title":"7.2 Apache Spark Cluster: Architecture and Distributed Computing","text":""},{"location":"en/ejercicios/07-infraestructura-bigdata/#what-you-will-learn_1","title":"What you will learn","text":"<ul> <li>Spark history: from MapReduce to in-memory processing (100x faster)</li> <li>Master-Worker Architecture: Driver, SparkSession, Cluster Manager, Workers, Executors, Tasks</li> <li>Spark Standalone Mode: master, workers, resource allocation, Web UI</li> <li>Building a cluster with Docker: containers as nodes, shared volumes, <code>apache/spark</code> image (official)</li> <li>SparkSession: entry point, local vs cluster modes, key configurations</li> <li>Lazy Evaluation and DAG: transformations vs actions, Catalyst optimizer, predicate pushdown</li> <li>Deployment modes: Local, Standalone, YARN, Kubernetes</li> <li>Basic tuning: sizing executors, partitions, small file problem, data locality</li> <li>Monitoring with Spark UI: Jobs, Stages, Storage, Executors, REST API</li> <li>Spark + PostgreSQL: JDBC connector, reading and writing data between Spark and PostgreSQL</li> <li>From Standalone to production: Kubernetes, managed services (EMR, Dataproc, Databricks)</li> </ul>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#example-spark-cluster-with-docker-compose","title":"Example: Spark Cluster with Docker Compose","text":"<pre><code>services:\n  spark-master:\n    image: apache/spark:3.5.4-python3\n    container_name: spark-master\n    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master\n    environment:\n      - SPARK_NO_DAEMONIZE=true\n    ports:\n      - \"7077:7077\"    # Cluster communication\n      - \"8080:8080\"    # Master Web UI\n\n  spark-worker-1:\n    image: apache/spark:3.5.4-python3\n    command: &gt;\n      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker\n      spark://spark-master:7077\n    environment:\n      - SPARK_WORKER_MEMORY=4G\n      - SPARK_WORKER_CORES=2\n      - SPARK_NO_DAEMONIZE=true\n    depends_on:\n      - spark-master\n</code></pre> <p>Official image</p> <p>We use <code>apache/spark</code> (the official Apache image). The <code>bitnami/spark</code> image was discontinued in September 2025 and no longer receives updates.</p>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#spark-architecture","title":"Spark Architecture","text":"<pre><code>Driver Program (tu script Python)\n    \u2502\n    \u25bc\nSparkSession \u2192 SparkContext\n    \u2502\n    \u25bc\nCluster Manager (Standalone / YARN / K8s)\n    \u2502\n    \u251c\u2500\u2500 Worker 1 \u2192 Executor \u2192 Tasks (procesan particiones)\n    \u251c\u2500\u2500 Worker 2 \u2192 Executor \u2192 Tasks\n    \u2514\u2500\u2500 Worker N \u2192 Executor \u2192 Tasks\n</code></pre>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#transformations-vs-actions","title":"Transformations vs Actions","text":"Transformations (LAZY) Actions (trigger execution) <code>select()</code>, <code>filter()</code>, <code>groupBy()</code> <code>show()</code>, <code>count()</code>, <code>collect()</code> <code>join()</code>, <code>orderBy()</code>, <code>withColumn()</code> <code>write</code>, <code>toPandas()</code>, <code>take(n)</code> Do not execute anything immediately Trigger the entire chain of transformations"},{"location":"en/ejercicios/07-infraestructura-bigdata/#relationship-with-other-modules","title":"Relationship with Other Modules","text":"Module Connection Module 2 (ETL) ETL pipelines on Docker infrastructure Module 3 (Distributed Processing) Practical exercises with PySpark and Dask Module 4 (Panel Data) Complete Spark pipeline on QoG dataset Module 6 (Streaming) Kafka and Spark Streaming on Docker Capstone Project The student builds Docker + Spark infrastructure from scratch"},{"location":"en/ejercicios/07-infraestructura-bigdata/#requirements","title":"Requirements","text":"<ul> <li>Docker Desktop installed</li> <li>Having completed at least Module 1 (Databases)</li> <li>Basic terminal/PowerShell knowledge</li> </ul>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#module-materials","title":"Module Materials","text":"<p>The complete content for each section is in the README files of the module folder:</p> <ul> <li>7.1 Docker Compose - Containers, images, orchestration, networks, volumes</li> <li>7.2 Cluster Spark - Distributed architecture, Docker cluster, tuning, JDBC</li> </ul>"},{"location":"en/ejercicios/07-infraestructura-bigdata/#glossary","title":"Glossary","text":"Term Definition Container Isolated instance that shares the host kernel (lightweight, fast) Image Immutable template for creating containers (cacheable layers) Docker Compose Tool for orchestrating multiple containers with a YAML file Driver Main Spark process that coordinates the cluster Executor JVM process on a Worker that runs tasks Task Minimum unit of work; processes one partition DAG Directed Acyclic Graph of the execution plan Lazy Evaluation Transformations are not executed until an action triggers them Catalyst Spark SQL query optimizer Shuffle Data redistribution between nodes (network-intensive) <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic references: - Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. - Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65. - Merkel, D. (2014). Docker: Lightweight Linux Containers for Consistent Development and Deployment. Linux Journal, 2014(239), 2. - Chambers, B., &amp; Zaharia, M. (2018). Spark: The Definitive Guide. O'Reilly Media. - Dean, J. &amp; Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1), 107-113.</p>"},{"location":"en/ejercicios/07-series-temporales-arima/","title":"Time Series: ARIMA/SARIMA with Box-Jenkins Methodology","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/07-series-temporales-arima/#general-description","title":"General Description","text":"<p>We will learn to model time series using the complete Box-Jenkins Methodology: identification, estimation, diagnostics, and forecasting. We will work with ARIMA and SARIMA models to capture both trends and seasonality.</p> <p>Level: Advanced Dataset: AirPassengers (144 monthly observations, 1949-1960) Technologies: Python, statsmodels, matplotlib</p>"},{"location":"en/ejercicios/07-series-temporales-arima/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the Box-Jenkins Methodology (4 phases)</li> <li>Identify components of a series: trend, seasonality, noise</li> <li>Use ACF and PACF to determine orders p, d, q</li> <li>Estimate ARIMA and SARIMA models</li> <li>Diagnose residuals (Ljung-Box, normality)</li> <li>Generate forecasts with confidence intervals</li> </ul>"},{"location":"en/ejercicios/07-series-temporales-arima/#exercise-content","title":"Exercise Content","text":"<p>The complete exercise is located at:</p> <pre><code>ejercicios/04_machine_learning/07_series_temporales_arima/\n\u251c\u2500\u2500 README.md                    # Complete Box-Jenkins theory (829 lines)\n\u251c\u2500\u2500 serie_temporal_completa.py   # 10-part script (1,286 lines)\n\u251c\u2500\u2500 output/                      # Directory for generated plots\n\u2514\u2500\u2500 .gitignore                   # Excludes output/*.png and *.csv\n</code></pre>"},{"location":"en/ejercicios/07-series-temporales-arima/#the-script-serie_temporal_completapy-covers","title":"The script <code>serie_temporal_completa.py</code> covers:","text":"<ol> <li>Loading and visualization of the original series</li> <li>Decomposition (trend + seasonality + residual)</li> <li>Stationarity tests (ADF, KPSS)</li> <li>Differencing (regular and seasonal)</li> <li>ACF/PACF for order identification</li> <li>ARIMA estimation with AIC-based selection</li> <li>SARIMA estimation with seasonal component</li> <li>Residual diagnostics (Ljung-Box, QQ-plot, residual ACF)</li> <li>Forecasting with confidence intervals</li> <li>Model comparison and metrics (MAPE, RMSE)</li> </ol>"},{"location":"en/ejercicios/07-series-temporales-arima/#theory-box-jenkins-methodology","title":"Theory: Box-Jenkins Methodology","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#anatomy-of-the-signal-the-data-generating-process","title":"Anatomy of the Signal: The Data Generating Process","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#stationarity-and-white-noise","title":"Stationarity and White Noise","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#series-decomposition","title":"Series Decomposition","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#transformation-and-differencing","title":"Transformation and Differencing","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#the-classic-decoder-arima","title":"The Classic Decoder: ARIMA","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#deseasonalization-and-sarima","title":"Deseasonalization and SARIMA","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#the-box-jenkins-workflow","title":"The Box-Jenkins Workflow","text":""},{"location":"en/ejercicios/07-series-temporales-arima/#phase-1-identification","title":"Phase 1: Identification","text":"<ul> <li>Visualize the series and detect trend/seasonality</li> <li>Apply differencing to achieve stationarity</li> <li>Analyze ACF and PACF to determine orders (p, d, q)</li> </ul>"},{"location":"en/ejercicios/07-series-temporales-arima/#phase-2-estimation","title":"Phase 2: Estimation","text":"<ul> <li>Fit ARIMA(p,d,q) or SARIMA(p,d,q)(P,D,Q)[s] model</li> <li>Compare candidate models using AIC/BIC</li> </ul>"},{"location":"en/ejercicios/07-series-temporales-arima/#phase-3-diagnostics","title":"Phase 3: Diagnostics","text":"<ul> <li>Verify that residuals are white noise</li> <li>Ljung-Box test (autocorrelation)</li> <li>Normality test</li> <li>QQ plot</li> </ul>"},{"location":"en/ejercicios/07-series-temporales-arima/#phase-4-forecasting","title":"Phase 4: Forecasting","text":"<ul> <li>Generate predictions with confidence intervals</li> <li>Evaluate accuracy with metrics (MAPE, RMSE)</li> </ul>"},{"location":"en/ejercicios/07-series-temporales-arima/#exercise-results","title":"Exercise Results","text":"<p>Selected model: SARIMA(1,1,0)(0,1,0)[12]</p> <ul> <li>AIC: -445.41</li> <li>MAPE: 7.41%</li> <li>Correctly captures trend and monthly seasonality</li> </ul>"},{"location":"en/ejercicios/07-series-temporales-arima/#interactive-dashboard","title":"Interactive Dashboard","text":"<p>You can explore the results in the interactive dashboard:</p> <p>View ARIMA/SARIMA Dashboard</p> <p>The dashboard includes 6 tabs with interactive Plotly charts: original series, decomposition, stationarity, ACF/PACF, diagnostics, and forecasting.</p>"},{"location":"en/ejercicios/07-series-temporales-arima/#resources","title":"Resources","text":"<ul> <li>statsmodels ARIMA Documentation</li> <li>statsmodels SARIMAX</li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic references:</p> <ul> <li>Box, G. E. P., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control (5<sup>th</sup> ed.). Wiley.</li> <li>Hyndman, R. J., &amp; Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3<sup>rd</sup> ed.). OTexts.</li> <li>Hamilton, J. D. (1994). Time Series Analysis. Princeton University Press.</li> </ul>"},{"location":"en/ejercicios/08-panel-data/","title":"Module 06: Panel Data Analysis","text":"<p>Status: Available</p>"},{"location":"en/ejercicios/08-panel-data/#general-description","title":"General Description","text":"<p>We will learn panel data econometrics: the combination of cross-sectional data (countries, individuals) with time series (years). We will work with Fixed Effects, Random Effects, and Two-Way Fixed Effects models applied to real social science problems.</p> <p>Level: Advanced Technologies: Python, linearmodels, pandas, Altair Prerequisites: Basic statistics, linear regression</p>"},{"location":"en/ejercicios/08-panel-data/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the structure of panel data (unit x time)</li> <li>Distinguish between Pooled OLS, Fixed Effects, and Random Effects</li> <li>Apply the Hausman Test to choose between FE and RE</li> <li>Implement Two-Way Fixed Effects (unit + time effects)</li> <li>Interpret Odds Ratios in logistic models</li> <li>Calculate marginal effects in nonlinear models</li> </ul>"},{"location":"en/ejercicios/08-panel-data/#module-content","title":"Module Content","text":"<p>The complete module is located at:</p> <pre><code>ejercicios/06_analisis_datos_de_panel/\n\u251c\u2500\u2500 01_analisis_guns.py              # Panel: gun laws and crime\n\u251c\u2500\u2500 02_analisis_fatality.py          # TWFE: beer tax vs mortality\n\u251c\u2500\u2500 03_dashboard_educativo.py        # Interactive dashboard with 4 tabs\n\u251c\u2500\u2500 conceptos_visuales_panel.py      # Conceptual visualizations\n\u251c\u2500\u2500 GUIA_PANEL_DATA.md               # Complete theoretical guide\n\u251c\u2500\u2500 grafico_panel_guns.png           # Analysis result\n\u2514\u2500\u2500 requirements.txt                 # linearmodels, altair, etc.\n</code></pre>"},{"location":"en/ejercicios/08-panel-data/#practical-exercises","title":"Practical Exercises","text":""},{"location":"en/ejercicios/08-panel-data/#01-guns-analysis-gun-laws-and-crime","title":"01 - Guns Analysis: Gun Laws and Crime","text":"<p>Question: Do gun-carry laws reduce violent crime?</p> <ul> <li>Dataset: Guns (Stock &amp; Watson) - 50 U.S. states, 1977-1999</li> <li>Dependent variable: <code>log(violent)</code> - violent crime rate (logarithm)</li> <li>Key variable: <code>law</code> - whether the state has a \"shall-carry\" law (mandatory carry)</li> <li>Controls: income, population, density</li> <li>Models: Pooled OLS vs Fixed Effects vs Random Effects</li> <li>Methodology: Comparison of the 3 models + Hausman Test</li> </ul> <pre><code>python ejercicios/06_analisis_datos_de_panel/01_analisis_guns.py\n</code></pre>"},{"location":"en/ejercicios/08-panel-data/#02-fatality-analysis-beer-tax-and-mortality","title":"02 - Fatality Analysis: Beer Tax and Mortality","text":"<p>Question: Does raising the beer tax reduce traffic accident deaths?</p> <ul> <li>Dataset: Fatalities (AER) - 48 states, 1982-1988</li> <li>Dependent variable: <code>fatality_rate</code> - deaths per 10,000 inhabitants</li> <li>Key variable: <code>beertax</code> - beer tax</li> <li>Controls: minimum drinking age (<code>drinkage</code>), unemployment, income</li> <li>Models: Entity FE vs Two-Way Fixed Effects (state + year)</li> <li>Innovation: TWFE controls for temporal trends (safer cars each year)</li> </ul> <pre><code>python ejercicios/06_analisis_datos_de_panel/02_analisis_fatality.py\n</code></pre>"},{"location":"en/ejercicios/08-panel-data/#03-interactive-educational-dashboard-panel-altair","title":"03 - Interactive Educational Dashboard (Panel + Altair)","text":"<p>Local dashboard with 4 interactive tabs to visually explore the concepts:</p> <ol> <li>Pooled OLS: Heterogeneity slider that shows Simpson's Paradox in action</li> <li>FE vs RE: Explanation and decision table for the Hausman Test</li> <li>Odds Ratios: Sliders to explore probability vs odds vs odds ratio in real time</li> <li>Marginal Effects: Lin-Lin, Log-Lin, Log-Log comparison with dynamic charts</li> </ol> <pre><code>panel serve ejercicios/06_analisis_datos_de_panel/03_dashboard_educativo.py\n# Open http://localhost:5006 in your browser\n</code></pre>"},{"location":"en/ejercicios/08-panel-data/#additional-files","title":"Additional Files","text":"<ul> <li><code>conceptos_visuales_panel.py</code> - Generates static charts explaining panel concepts</li> <li><code>dashboard_educativo_panel.py</code> - Alternative version of the educational dashboard</li> <li><code>GUIA_PANEL_DATA.md</code> - Complete theoretical guide: Pooled OLS, FE, RE, Hausman, TWFE</li> </ul>"},{"location":"en/ejercicios/08-panel-data/#theory-key-concepts","title":"Theory: Key Concepts","text":""},{"location":"en/ejercicios/08-panel-data/#what-is-panel-data","title":"What is panel data?","text":"<p>Data that combines cross-sectional (N units) with time series (T periods):</p> <p></p> <pre><code>| country | year | democracy | gdp_pc |\n|---------|------|-----------|--------|\n| ESP     | 2000 | 0.85      | 24000  |\n| ESP     | 2001 | 0.86      | 24500  |\n| FRA     | 2000 | 0.88      | 28000  |\n| FRA     | 2001 | 0.89      | 28500  |\n</code></pre>"},{"location":"en/ejercicios/08-panel-data/#data-engineering-wide-vs-long-format","title":"Data Engineering: Wide vs Long Format","text":""},{"location":"en/ejercicios/08-panel-data/#pooled-ols-vs-fixed-effects-vs-random-effects","title":"Pooled OLS vs Fixed Effects vs Random Effects","text":"Model Assumption When to use Pooled OLS No individual heterogeneity Rarely appropriate Fixed Effects Heterogeneity correlated with X Social sciences (general rule) Random Effects Heterogeneity NOT correlated with X Random surveys"},{"location":"en/ejercicios/08-panel-data/#the-naive-model-pooled-ols","title":"The Naive Model: Pooled OLS","text":""},{"location":"en/ejercicios/08-panel-data/#unobserved-heterogeneity","title":"Unobserved Heterogeneity","text":""},{"location":"en/ejercicios/08-panel-data/#hausman-test","title":"Hausman Test","text":"<p>Decides between FE and RE:</p> <ul> <li>H0: RE is consistent and efficient (prefer RE)</li> <li>H1: Only FE is consistent (prefer FE)</li> <li>If p-value &lt; 0.05: use Fixed Effects</li> </ul>"},{"location":"en/ejercicios/08-panel-data/#educational-dashboard-local","title":"Educational Dashboard (local)","text":"<p>The main dashboard for this module runs locally with Panel (HoloViz):</p> <pre><code>panel serve ejercicios/06_analisis_datos_de_panel/03_dashboard_educativo.py\n</code></pre> <p>Includes 4 interactive tabs: Pooled OLS, FE vs RE, Odds Ratios, Marginal Effects.</p>"},{"location":"en/ejercicios/08-panel-data/#qog-dashboard-advanced-analysis-github-pages","title":"QoG Dashboard - Advanced Analysis (GitHub Pages)","text":"<p>As a complement, you can explore a dashboard with panel analysis applied to the QoG dataset (4 research lines with Spark + PostgreSQL + ML):</p> <p>View QoG Dashboard - Applied Panel Data</p>"},{"location":"en/ejercicios/08-panel-data/#resources","title":"Resources","text":""},{"location":"en/ejercicios/08-panel-data/#documentation","title":"Documentation","text":"<ul> <li>linearmodels Documentation</li> <li>Altair Documentation</li> </ul>"},{"location":"en/ejercicios/08-panel-data/#theoretical-references","title":"Theoretical References","text":"<ul> <li>Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (2<sup>nd</sup> ed.). MIT Press.</li> <li>Stock, J. H., &amp; Watson, M. W. (2019). Introduction to Econometrics (4<sup>th</sup> ed.). Pearson.</li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic references:</p> <ul> <li>Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data (2<sup>nd</sup> ed.). MIT Press.</li> <li>Stock, J. H., &amp; Watson, M. W. (2019). Introduction to Econometrics (4<sup>th</sup> ed.). Pearson.</li> <li>Baltagi, B. H. (2021). Econometric Analysis of Panel Data (6<sup>th</sup> ed.). Springer.</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/","title":"Module 08: Data Streaming with Apache Kafka","text":""},{"location":"en/ejercicios/08-streaming-kafka/#introduction","title":"Introduction","text":"<p>Data streaming is the continuous processing of data in real time, as opposed to batch processing where data is accumulated and processed periodically. In modern applications such as fraud detection, IoT sensor monitoring, or seismic alerts, response time is critical.</p> <p>Apache Kafka is a distributed streaming platform that allows you to publish, store, and process data streams in real time. Originally developed by LinkedIn and donated to Apache, Kafka has become the de facto standard for streaming data architectures.</p>"},{"location":"en/ejercicios/08-streaming-kafka/#data-formats-for-ingestion","title":"Data Formats for Ingestion","text":""},{"location":"en/ejercicios/08-streaming-kafka/#kafka-architecture","title":"Kafka Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PRODUCTOR  \u2502\u2500\u2500\u2500\u2500&gt;\u2502            KAFKA CLUSTER            \u2502\u2500\u2500\u2500\u2500&gt;\u2502 CONSUMIDOR  \u2502\n\u2502  (Python)   \u2502     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502     \u2502  (Python)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  \u2502  Topic: sismos              \u2502    \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502  \u2502  \u251c\u2500\u2500 Partition 0            \u2502    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502  \u2502  \u251c\u2500\u2500 Partition 1            \u2502    \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PRODUCTOR  \u2502\u2500\u2500\u2500\u2500&gt;\u2502  \u2502  \u2514\u2500\u2500 Partition 2            \u2502    \u2502\u2500\u2500\u2500\u2500&gt;\u2502 CONSUMIDOR  \u2502\n\u2502   (API)     \u2502     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     \u2502  (Spark)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   KRaft Mode    \u2502\n                    \u2502  (sin ZooKeeper)\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"en/ejercicios/08-streaming-kafka/#key-concepts","title":"Key Concepts","text":"Concept Description Broker Kafka server that stores and serves messages Topic Logical channel where messages are published (like a table) Partition Division of a topic for parallelism and scalability Producer Application that sends messages to a topic Consumer Application that reads messages from a topic Consumer Group Set of consumers that share the load of a topic Offset Position of a message within a partition"},{"location":"en/ejercicios/08-streaming-kafka/#kraft-mode-vs-zookeeper","title":"KRaft Mode vs ZooKeeper","text":"<p>Since Kafka 3.x, KRaft (Kafka Raft) mode replaces ZooKeeper for metadata management, simplifying the architecture:</p> <pre><code># Antes (con ZooKeeper) - 2 servicios\nkafka + zookeeper\n\n# Ahora (KRaft mode) - 1 servicio autocontenido\nkafka\n</code></pre>"},{"location":"en/ejercicios/08-streaming-kafka/#delivery-guarantees","title":"Delivery Guarantees","text":"<p>Kafka offers three delivery semantics:</p> Semantics Description Typical use At-most-once Message may be lost, never duplicated Non-critical logs At-least-once Message may be duplicated, never lost Processing with idempotency Exactly-once Message is delivered exactly once Financial transactions"},{"location":"en/ejercicios/08-streaming-kafka/#required-tools","title":"Required Tools","text":"<ul> <li>Docker and Docker Compose: To set up the infrastructure</li> <li>Python 3.9+: Main language</li> <li>confluent-kafka: Official Python client for Kafka</li> <li>requests: To consume external APIs</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#dependency-installation","title":"Dependency Installation","text":"<pre><code>pip install confluent-kafka requests\n</code></pre>"},{"location":"en/ejercicios/08-streaming-kafka/#challenge-1-set-up-kafka-with-docker-compose","title":"Challenge 1: Set Up Kafka with Docker Compose","text":"<p>Objective: Create a functional Kafka cluster on your local machine using Docker.</p> <p>Difficulty: Basic</p>"},{"location":"en/ejercicios/08-streaming-kafka/#instructions","title":"Instructions","text":"<ol> <li> <p>Create a directory for the project:    <pre><code>mkdir kafka-streaming\ncd kafka-streaming\n</code></pre></p> </li> <li> <p>Create a <code>docker-compose.yml</code> file with a Kafka broker in KRaft mode</p> </li> <li> <p>The broker must meet the following requirements:</p> </li> <li>Image: <code>apache/kafka:latest</code></li> <li>Port 9092 exposed</li> <li>KRaft mode enabled (no ZooKeeper)</li> <li> <p>Environment variables correctly configured</p> </li> <li> <p>Start and verify:    <pre><code>docker-compose up -d\ndocker-compose logs -f kafka\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/08-streaming-kafka/#success-criteria","title":"Success Criteria","text":"<ul> <li> The Kafka container is running without errors</li> <li> Logs show \"Kafka Server started\"</li> <li> Port 9092 responds</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#hints","title":"Hints","text":"<ul> <li>Check the documentation for the <code>apache/kafka</code> image on Docker Hub</li> <li>Key variables: <code>KAFKA_NODE_ID</code>, <code>KAFKA_PROCESS_ROLES</code>, <code>KAFKA_LISTENERS</code></li> <li>KRaft mode requires <code>KAFKA_CONTROLLER_QUORUM_VOTERS</code></li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#resources","title":"Resources","text":"<ul> <li>Apache Kafka Docker Image</li> <li>KRaft Mode Documentation</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#challenge-2-your-first-producer","title":"Challenge 2: Your First Producer","text":"<p>Objective: Create a Python script that sends messages to Kafka.</p> <p>Difficulty: Basic</p>"},{"location":"en/ejercicios/08-streaming-kafka/#instructions_1","title":"Instructions","text":"<ol> <li> <p>Create <code>productor_simple.py</code></p> </li> <li> <p>The script must:</p> </li> <li>Connect to <code>localhost:9092</code></li> <li>Send 10 messages to the topic <code>mensajes-test</code></li> <li>Each message must be JSON with: <code>id</code>, <code>texto</code>, <code>timestamp</code></li> <li> <p>Confirm each successful send</p> </li> <li> <p>Base structure:    <pre><code>from confluent_kafka import Producer\nimport json\nfrom datetime import datetime\n\ndef delivery_callback(err, msg):\n    \"\"\"Callback para confirmar envio\"\"\"\n    # Implementa: imprime error o confirmacion\n    pass\n\nconfig = {\n    'bootstrap.servers': 'localhost:9092',\n    'client.id': 'productor-simple'\n}\n\nproducer = Producer(config)\n\n# Implementa: enviar 10 mensajes\n# Usa: producer.produce(topic, key, value, callback=delivery_callback)\n# No olvides: producer.flush()\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/08-streaming-kafka/#success-criteria_1","title":"Success Criteria","text":"<ul> <li> Script runs without errors</li> <li> 10 messages sent successfully</li> <li> Each message has a valid JSON structure</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#hints_1","title":"Hints","text":"<ul> <li><code>json.dumps()</code> to serialize to string</li> <li>The <code>key</code> can be the message ID</li> <li><code>flush()</code> ensures all messages are sent before exiting</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#challenge-3-your-first-consumer","title":"Challenge 3: Your First Consumer","text":"<p>Objective: Create a script that reads messages from Kafka in real time.</p> <p>Difficulty: Basic</p>"},{"location":"en/ejercicios/08-streaming-kafka/#instructions_2","title":"Instructions","text":"<ol> <li> <p>Create <code>consumidor_simple.py</code></p> </li> <li> <p>The script must:</p> </li> <li>Subscribe to the topic <code>mensajes-test</code></li> <li>Read messages in an infinite loop</li> <li>Print each message with its offset and partition</li> <li> <p>Exit cleanly with Ctrl+C</p> </li> <li> <p>Base structure:    <pre><code>from confluent_kafka import Consumer, KafkaError\nimport json\n\nconfig = {\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'mi-grupo-consumidor',\n    'auto.offset.reset': 'earliest'  # Leer desde el inicio\n}\n\nconsumer = Consumer(config)\nconsumer.subscribe(['mensajes-test'])\n\ntry:\n    while True:\n        msg = consumer.poll(timeout=1.0)\n        # Implementa: verificar errores y procesar mensaje\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    consumer.close()\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/08-streaming-kafka/#success-criteria_2","title":"Success Criteria","text":"<ul> <li> Consumer connects successfully</li> <li> Reads the producer's messages</li> <li> Displays: partition, offset, key, value</li> <li> Exits cleanly with Ctrl+C</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#hints_2","title":"Hints","text":"<ul> <li>Check <code>msg is None</code> (timeout with no message)</li> <li>Check <code>msg.error()</code> before processing</li> <li>Use <code>msg.partition()</code>, <code>msg.offset()</code>, <code>msg.key()</code>, <code>msg.value()</code></li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#challenge-4-connect-to-a-real-api-usgs-earthquakes","title":"Challenge 4: Connect to a Real API (USGS Earthquakes)","text":"<p>Objective: Create a producer that consumes earthquake data in real time.</p> <p>Difficulty: Intermediate</p>"},{"location":"en/ejercicios/08-streaming-kafka/#instructions_3","title":"Instructions","text":"<ol> <li> <p>Create <code>productor_sismos.py</code></p> </li> <li> <p>The producer must:</p> </li> <li>Query the USGS API every 30 seconds</li> <li>Parse the GeoJSON response</li> <li>Publish each new earthquake to the topic <code>sismos</code></li> <li> <p>Avoid duplicates (maintain a set of processed IDs)</p> </li> <li> <p>API to use:    <pre><code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson\n</code></pre></p> </li> <li> <p>Message structure to publish:    <pre><code>{\n  \"id\": \"us7000abcd\",\n  \"magnitud\": 4.5,\n  \"lugar\": \"10km SSW of Somewhere\",\n  \"latitud\": -33.45,\n  \"longitud\": -70.66,\n  \"profundidad_km\": 10.0,\n  \"timestamp\": \"2024-01-15T10:30:00.000Z\",\n  \"tsunami\": false\n}\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/08-streaming-kafka/#success-criteria_3","title":"Success Criteria","text":"<ul> <li> Queries the API every 30 seconds</li> <li> Publishes earthquakes to the topic <code>sismos</code></li> <li> Does not publish duplicate earthquakes</li> <li> Handles network errors gracefully</li> <li> Runs continuously</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#hints_3","title":"Hints","text":"<ul> <li>Earthquakes are in <code>response['features']</code></li> <li>The ID is in <code>feature['id']</code></li> <li>Properties are in <code>feature['properties']</code></li> <li>Coordinates are in <code>feature['geometry']['coordinates']</code> (lon, lat, depth)</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#challenge-5-alert-system","title":"Challenge 5: Alert System","text":"<p>Objective: Create a consumer that detects significant earthquakes.</p> <p>Difficulty: Intermediate</p>"},{"location":"en/ejercicios/08-streaming-kafka/#instructions_4","title":"Instructions","text":"<ol> <li> <p>Create <code>consumidor_alertas.py</code></p> </li> <li> <p>The consumer must:</p> </li> <li>Read from the topic <code>sismos</code></li> <li>Filter earthquakes with magnitude &gt;= 4.5</li> <li>Display a highlighted alert in the console</li> <li> <p>Save alerts to <code>alertas.log</code></p> </li> <li> <p>Alert format:    <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         ALERTA SISMICA                 \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Magnitud: 5.2                          \u2551\n\u2551 Lugar: 10km S of Tokyo, Japan          \u2551\n\u2551 Hora: 2024-01-15 10:30:00              \u2551\n\u2551 Coords: (35.6, 139.7)                  \u2551\n\u2551 Profundidad: 10.5 km                   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/08-streaming-kafka/#success-criteria_4","title":"Success Criteria","text":"<ul> <li> Correctly filters by magnitude &gt;= 4.5</li> <li> Alerts visible in the console</li> <li> Alerts saved to log file</li> <li> System runs continuously</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#challenge-6-aggregations-with-spark-structured-streaming","title":"Challenge 6: Aggregations with Spark Structured Streaming","text":"<p>Objective: Process the earthquake stream with Spark to calculate statistics.</p> <p>Difficulty: Advanced</p>"},{"location":"en/ejercicios/08-streaming-kafka/#instructions_5","title":"Instructions","text":"<ol> <li> <p>Add a Spark service to your <code>docker-compose.yml</code></p> </li> <li> <p>Create <code>spark_streaming_sismos.py</code></p> </li> <li> <p>The job must:</p> </li> <li>Read from the topic <code>sismos</code> as a stream</li> <li>Parse the JSON messages</li> <li>Calculate per 5-minute window:<ul> <li>Earthquake count</li> <li>Average magnitude</li> <li>Maximum magnitude</li> </ul> </li> <li> <p>Write results to the console</p> </li> <li> <p>Base structure:    <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nspark = SparkSession.builder \\\n    .appName(\"SismosStreaming\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n    .getOrCreate()\n\n# Define schema del mensaje\nschema = StructType([\n    StructField(\"id\", StringType()),\n    StructField(\"magnitud\", DoubleType()),\n    # ... completa el schema\n])\n\n# Lee del topic\ndf = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"sismos\") \\\n    .load()\n\n# Parsea el JSON\nsismos = df.select(\n    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\n# Implementa: agregaciones por ventana de tiempo\n# Usa: window(), avg(), max(), count()\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/08-streaming-kafka/#success-criteria_5","title":"Success Criteria","text":"<ul> <li> Spark reads from the Kafka topic</li> <li> Correctly parses the messages</li> <li> Calculates aggregations per window</li> <li> Displays results in the console</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#final-challenge-visualization-dashboard","title":"FINAL Challenge: Visualization Dashboard","text":"<p>Objective: Create a web visualization that displays earthquakes in real time.</p> <p>Difficulty: Advanced</p>"},{"location":"en/ejercicios/08-streaming-kafka/#evaluation-criteria","title":"Evaluation Criteria","text":"Criterion Points Interactive map with earthquake locations 20 Automatic updates without reloading 20 Live statistics (total, max, average) 15 Magnitude filters 15 Visual differentiation by magnitude 15 Professional design 15 Total 100"},{"location":"en/ejercicios/08-streaming-kafka/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>HTML5 + vanilla JavaScript (no frameworks)</li> <li>Leaflet.js for maps</li> <li>Fetch API to retrieve data</li> <li>Modern CSS (flexbox/grid)</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#suggestions","title":"Suggestions","text":"<ul> <li>Query the USGS API directly from JavaScript</li> <li>Use <code>setInterval()</code> for automatic updates</li> <li>Implement colors by magnitude (risk scale)</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#submission","title":"Submission","text":"<ul> <li>Self-contained HTML file</li> <li>Screenshot showing it working</li> <li>Brief usage documentation</li> </ul> <p>Reference: You can see an example dashboard at Seismic Observatory, but the challenge is to create your own version with your own style.</p>"},{"location":"en/ejercicios/08-streaming-kafka/#resources-and-references","title":"Resources and References","text":""},{"location":"en/ejercicios/08-streaming-kafka/#official-documentation","title":"Official Documentation","text":"<ul> <li>Apache Kafka Documentation</li> <li>Confluent Python Client</li> <li>Spark Structured Streaming + Kafka</li> <li>USGS Earthquake API</li> </ul>"},{"location":"en/ejercicios/08-streaming-kafka/#earthquake-geojson","title":"Earthquake GeoJSON","text":"<ul> <li>Last hour: <code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson</code></li> <li>Last day: <code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson</code></li> <li>Last week: <code>https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_week.geojson</code></li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic References:</p> <ul> <li>Kreps, J., Narkhede, N., &amp; Rao, J. (2011). Kafka: A distributed messaging system for log processing. Proceedings of the NetDB Workshop.</li> <li>Zaharia, M., et al. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.</li> <li>Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. ISBN: 978-1449373320.</li> <li>Narkhede, N., Shapira, G., &amp; Palino, T. (2017). Kafka: The Definitive Guide. O'Reilly Media. ISBN: 978-1491936160.</li> <li>USGS (2024). Earthquake Hazards Program - Real-time Feeds. United States Geological Survey.</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/","title":"Module 09: Cloud Engineering with LocalStack","text":""},{"location":"en/ejercicios/09-cloud-localstack/#introduction","title":"Introduction","text":"<p>Cloud Computing has revolutionized the way we deploy and scale applications. However, learning AWS, Azure, or GCP has a barrier: cost. A mistake in production can generate unexpected bills.</p> <p>LocalStack solves this problem by simulating AWS services locally. You can learn S3, Lambda, DynamoDB, Kinesis, and more without spending a cent. If it works on LocalStack, it works on real AWS.</p> <p>Terraform is the standard tool for \"Infrastructure as Code\" (IaC). Instead of clicking through web consoles, you define your infrastructure in text files that you can version, review, and reuse.</p>"},{"location":"en/ejercicios/09-cloud-localstack/#fundamental-concepts","title":"Fundamental Concepts","text":""},{"location":"en/ejercicios/09-cloud-localstack/#cloud-computing-the-3-models","title":"Cloud Computing: The 3 Models","text":"Model Description Example IaaS Infrastructure as a Service EC2, VMs PaaS Platform as a Service Elastic Beanstalk, Heroku SaaS Software as a Service Gmail, Salesforce"},{"location":"en/ejercicios/09-cloud-localstack/#key-aws-services-for-big-data","title":"Key AWS Services for Big Data","text":"Service Purpose Open Source Equivalent S3 Object storage (Data Lake) MinIO Lambda Serverless functions OpenFaaS Kinesis Data streaming Kafka DynamoDB NoSQL database MongoDB EventBridge Event orchestration Cron + Kafka IAM Access control -"},{"location":"en/ejercicios/09-cloud-localstack/#data-lake-architecture-medallion","title":"Data Lake Architecture (Medallion)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     DATA LAKE (S3)                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    BRONZE     \u2502      SILVER       \u2502          GOLD           \u2502\n\u2502   (Raw Data)  \u2502   (Cleaned Data)  \u2502   (Business-Ready)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 - Datos crudos\u2502 - Datos limpios   \u2502 - Agregaciones          \u2502\n\u2502 - JSON/CSV    \u2502 - Parquet         \u2502 - KPIs                  \u2502\n\u2502 - Sin esquema \u2502 - Con esquema     \u2502 - Dashboards            \u2502\n\u2502 - Append-only \u2502 - Deduplicados    \u2502 - ML-ready              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"en/ejercicios/09-cloud-localstack/#required-tools","title":"Required Tools","text":"<ul> <li>Docker and Docker Compose: For LocalStack</li> <li>Python 3.9+: Main language</li> <li>Terraform: Infrastructure as code</li> <li>awscli-local: AWS CLI for LocalStack</li> <li>boto3: AWS SDK for Python</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#dependency-installation","title":"Dependency Installation","text":"<pre><code># Python\npip install boto3 requests\n\n# Terraform (Windows con Chocolatey)\nchoco install terraform\n\n# Terraform (Linux/Mac)\nbrew install terraform\n\n# AWS CLI Local\npip install awscli-local\n</code></pre>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-1-set-up-localstack","title":"Challenge 1: Set Up LocalStack","text":"<p>Objective: Create a functional LocalStack environment with Docker.</p> <p>Difficulty: Basic</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions","title":"Instructions","text":"<ol> <li> <p>Create a directory for the project:    <pre><code>mkdir cloud-localstack\ncd cloud-localstack\n</code></pre></p> </li> <li> <p>Create a <code>docker-compose.yml</code> file with LocalStack</p> </li> <li> <p>The service must:</p> </li> <li>Use image <code>localstack/localstack:latest</code></li> <li>Expose port 4566 (unified gateway)</li> <li>Enable services: s3, lambda, dynamodb, events</li> <li> <p>Mount a volume for persistence</p> </li> <li> <p>Start and verify:    <pre><code>docker-compose up -d\ncurl http://localhost:4566/_localstack/health\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria","title":"Success Criteria","text":"<ul> <li> LocalStack container running</li> <li> Health endpoint responds with active services</li> <li> You can list S3 buckets (empty): <code>awslocal s3 ls</code></li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#hints","title":"Hints","text":"<ul> <li>The <code>SERVICES</code> variable defines which services to activate</li> <li><code>LOCALSTACK_HOST=localhost</code> for local connections</li> <li>Port 4566 is the gateway for all services</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#resources","title":"Resources","text":"<ul> <li>LocalStack Documentation</li> <li>Docker Hub - LocalStack</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-2-create-an-s3-bucket-with-terraform","title":"Challenge 2: Create an S3 Bucket with Terraform","text":"<p>Objective: Define an S3 bucket using Terraform and deploy it to LocalStack.</p> <p>Difficulty: Basic</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions_1","title":"Instructions","text":"<ol> <li> <p>Create a <code>main.tf</code> file</p> </li> <li> <p>Configure the AWS provider for LocalStack:    <pre><code>provider \"aws\" {\n  region                      = \"us-east-1\"\n  access_key                  = \"test\"\n  secret_key                  = \"test\"\n  skip_credentials_validation = true\n  skip_metadata_api_check     = true\n  skip_requesting_account_id  = true\n\n  endpoints {\n    s3     = \"http://localhost:4566\"\n    lambda = \"http://localhost:4566\"\n    # ... a\u00f1ade mas endpoints\n  }\n}\n</code></pre></p> </li> <li> <p>Define an S3 bucket:    <pre><code>resource \"aws_s3_bucket\" \"data_lake\" {\n  bucket = \"mi-data-lake\"\n  # ... completa la configuracion\n}\n</code></pre></p> </li> <li> <p>Run Terraform:    <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria_1","title":"Success Criteria","text":"<ul> <li> <code>terraform init</code> downloads the provider</li> <li> <code>terraform plan</code> shows the bucket to be created</li> <li> <code>terraform apply</code> creates the bucket</li> <li> <code>awslocal s3 ls</code> shows \"mi-data-lake\"</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#hints_1","title":"Hints","text":"<ul> <li>Endpoints must point to <code>http://localhost:4566</code></li> <li><code>access_key</code> and <code>secret_key</code> can be any value in LocalStack</li> <li>Use <code>terraform destroy</code> to clean up resources</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-3-your-first-lambda-hello-world","title":"Challenge 3: Your First Lambda (Hello World)","text":"<p>Objective: Create a Lambda function and deploy it with Terraform.</p> <p>Difficulty: Intermediate</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions_2","title":"Instructions","text":"<ol> <li> <p>Create a <code>lambdas/</code> folder with a <code>hello.py</code> file:    <pre><code>def handler(event, context):\n    \"\"\"Tu primera Lambda\"\"\"\n    # Implementa: retorna un saludo con datos del evento\n    return {\n        \"statusCode\": 200,\n        \"body\": \"...\"\n    }\n</code></pre></p> </li> <li> <p>Package the Lambda into a ZIP:    <pre><code>cd lambdas &amp;&amp; zip hello.zip hello.py &amp;&amp; cd ..\n</code></pre></p> </li> <li> <p>Add to <code>main.tf</code>:    <pre><code>resource \"aws_lambda_function\" \"hello\" {\n  filename         = \"lambdas/hello.zip\"\n  function_name    = \"hello-lambda\"\n  role             = \"arn:aws:iam::000000000000:role/lambda-role\"\n  handler          = \"hello.handler\"\n  runtime          = \"python3.9\"\n  # ... completa\n}\n</code></pre></p> </li> <li> <p>Apply and test:    <pre><code>terraform apply\nawslocal lambda invoke --function-name hello-lambda output.json\ncat output.json\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria_2","title":"Success Criteria","text":"<ul> <li> Lambda created in LocalStack</li> <li> Invocation returns statusCode 200</li> <li> The body contains your message</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#hints_2","title":"Hints","text":"<ul> <li>In LocalStack, the role can be a fictitious ARN</li> <li>The handler is <code>filename.function_name</code></li> <li>Use <code>source_code_hash</code> to detect changes in the ZIP</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-4-lambda-that-consumes-an-external-api","title":"Challenge 4: Lambda that Consumes an External API","text":"<p>Objective: Create a Lambda that captures ISS data in real time.</p> <p>Difficulty: Intermediate</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions_3","title":"Instructions","text":"<ol> <li> <p>Create <code>lambdas/capturar_iss.py</code>:    <pre><code>import json\nimport urllib.request\n\nISS_API = \"https://api.wheretheiss.at/v1/satellites/25544\"\n\ndef handler(event, context):\n    \"\"\"Captura posicion actual de la ISS\"\"\"\n    # Implementa:\n    # 1. Hacer request a la API\n    # 2. Parsear el JSON\n    # 3. Extraer: latitude, longitude, altitude, velocity\n    # 4. Retornar los datos formateados\n    pass\n</code></pre></p> </li> <li> <p>Note: Lambda does not have <code>requests</code>, use <code>urllib.request</code>:    <pre><code>with urllib.request.urlopen(ISS_API) as response:\n    data = json.loads(response.read().decode())\n</code></pre></p> </li> <li> <p>Deploy and test the Lambda</p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria_3","title":"Success Criteria","text":"<ul> <li> Lambda deploys correctly</li> <li> Returns the current ISS position</li> <li> Data includes lat, lon, alt, velocity</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#hints_3","title":"Hints","text":"<ul> <li>Lambda has library limitations - use the standard library</li> <li>The default timeout is 3 seconds, it may need more</li> <li>Handle network exceptions</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-5-save-data-to-s3","title":"Challenge 5: Save Data to S3","text":"<p>Objective: Modify the Lambda to save ISS data to S3.</p> <p>Difficulty: Intermediate</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions_4","title":"Instructions","text":"<ol> <li>Modify <code>capturar_iss.py</code> to:</li> <li>Connect to S3 using boto3</li> <li>Save each capture as JSON in the bucket</li> <li> <p>Use path: <code>raw/iss/{date}/{timestamp}.json</code></p> </li> <li> <p>Structure of the saved file:    <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"latitude\": 45.123,\n  \"longitude\": -93.456,\n  \"altitude\": 420.5,\n  \"velocity\": 27580.3\n}\n</code></pre></p> </li> <li> <p>Configure boto3 for LocalStack:    <pre><code>import boto3\n\ns3 = boto3.client('s3',\n    endpoint_url='http://host.docker.internal:4566',\n    aws_access_key_id='test',\n    aws_secret_access_key='test'\n)\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria_4","title":"Success Criteria","text":"<ul> <li> Lambda saves file to S3</li> <li> Path includes date and timestamp</li> <li> File contains valid ISS data</li> <li> You can list files: <code>awslocal s3 ls s3://bucket/raw/iss/ --recursive</code></li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#hints_4","title":"Hints","text":"<ul> <li>In Docker, use <code>host.docker.internal</code> to access LocalStack</li> <li><code>s3.put_object(Bucket, Key, Body)</code> to save</li> <li><code>Body</code> must be a string or bytes</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-6-scheduling-with-eventbridge","title":"Challenge 6: Scheduling with EventBridge","text":"<p>Objective: Schedule the Lambda to run automatically every minute.</p> <p>Difficulty: Advanced</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions_5","title":"Instructions","text":"<ol> <li> <p>Add an EventBridge rule to <code>main.tf</code>:    <pre><code>resource \"aws_cloudwatch_event_rule\" \"cada_minuto\" {\n  name                = \"captura-iss-schedule\"\n  description         = \"Ejecuta Lambda cada minuto\"\n  schedule_expression = \"rate(1 minute)\"\n}\n</code></pre></p> </li> <li> <p>Connect the rule to the Lambda:    <pre><code>resource \"aws_cloudwatch_event_target\" \"lambda_target\" {\n  rule      = aws_cloudwatch_event_rule.cada_minuto.name\n  target_id = \"captura-iss\"\n  arn       = aws_lambda_function.capturar_iss.arn\n}\n</code></pre></p> </li> <li> <p>Add permissions so that EventBridge can invoke the Lambda:    <pre><code>resource \"aws_lambda_permission\" \"allow_eventbridge\" {\n  # ... completa\n}\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria_5","title":"Success Criteria","text":"<ul> <li> EventBridge rule created</li> <li> Lambda runs automatically</li> <li> Files appear in S3 every minute</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#hints_5","title":"Hints","text":"<ul> <li>Use <code>schedule_expression = \"rate(1 minute)\"</code> or cron</li> <li>The permission requires <code>statement_id</code>, <code>action</code>, <code>function_name</code>, <code>principal</code></li> <li>Check logs: <code>awslocal logs tail /aws/lambda/capturar-iss</code></li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#challenge-7-dynamodb-for-metadata","title":"Challenge 7: DynamoDB for Metadata","text":"<p>Objective: Create a DynamoDB table to store capture metadata.</p> <p>Difficulty: Advanced</p>"},{"location":"en/ejercicios/09-cloud-localstack/#instructions_6","title":"Instructions","text":"<ol> <li> <p>Add a DynamoDB table in Terraform:    <pre><code>resource \"aws_dynamodb_table\" \"capturas_metadata\" {\n  name           = \"capturas-iss\"\n  billing_mode   = \"PAY_PER_REQUEST\"\n  hash_key       = \"capture_id\"\n\n  attribute {\n    name = \"capture_id\"\n    type = \"S\"\n  }\n  # ... a\u00f1ade mas atributos si necesitas\n}\n</code></pre></p> </li> <li> <p>Modify the Lambda to save metadata:    <pre><code>dynamodb = boto3.resource('dynamodb',\n    endpoint_url='http://host.docker.internal:4566',\n    # ...\n)\n\ntable = dynamodb.Table('capturas-iss')\ntable.put_item(Item={\n    'capture_id': timestamp,\n    's3_path': f's3://bucket/raw/iss/{fecha}/{timestamp}.json',\n    'latitude': data['latitude'],\n    'longitude': data['longitude'],\n    # ...\n})\n</code></pre></p> </li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#success-criteria_6","title":"Success Criteria","text":"<ul> <li> DynamoDB table created</li> <li> Lambda saves metadata on each execution</li> <li> You can query items: <code>awslocal dynamodb scan --table-name capturas-iss</code></li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#final-challenge-tracker-dashboard","title":"FINAL Challenge: Tracker Dashboard","text":"<p>Objective: Create a web visualization that shows the ISS position in real time.</p> <p>Difficulty: Advanced</p>"},{"location":"en/ejercicios/09-cloud-localstack/#evaluation-criteria","title":"Evaluation Criteria","text":"Criterion Points Map with current ISS position 20 Custom ISS icon 10 Automatic update (every 5-10 sec) 20 Trajectory/movement trail 15 Live data (lat, lon, alt, vel) 15 Flyover predictor for a city 10 Professional design 10 Total 100"},{"location":"en/ejercicios/09-cloud-localstack/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>HTML5 + vanilla JavaScript</li> <li>Leaflet.js for maps</li> <li>Fetch API for live data</li> <li>No Jupyter/Colab dependencies</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#suggestions","title":"Suggestions","text":"<ul> <li>Use the API directly: <code>https://api.wheretheiss.at/v1/satellites/25544</code></li> <li>Nominatim for geocoding cities</li> <li>SVG for the ISS icon</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#submission","title":"Submission","text":"<ul> <li>Self-contained HTML file</li> <li>Screenshot showing it working</li> <li>Brief documentation</li> </ul> <p>Reference: You can see an example of a professional tracker at ISS Tracker, but the challenge is to create your own version.</p>"},{"location":"en/ejercicios/09-cloud-localstack/#from-localstack-to-real-aws","title":"From LocalStack to Real AWS","text":"<p>When you are ready for production:</p>"},{"location":"en/ejercicios/09-cloud-localstack/#required-changes","title":"Required Changes","text":"<ol> <li> <p>Real credentials: <pre><code>aws configure\n# Ingresa Access Key ID y Secret Access Key reales\n</code></pre></p> </li> <li> <p>Remove local endpoints: <pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n  # Sin endpoints personalizados\n}\n</code></pre></p> </li> <li> <p>Real IAM roles:</p> </li> <li>Create roles with minimum required permissions</li> <li> <p>Use managed policies when possible</p> </li> <li> <p>Cost considerations:</p> </li> <li>S3: $0.023/GB/month</li> <li>Lambda: 1M invocations free, then $0.20/1M</li> <li>DynamoDB: Pay-per-request or provisioned</li> </ol>"},{"location":"en/ejercicios/09-cloud-localstack/#resources-and-references","title":"Resources and References","text":""},{"location":"en/ejercicios/09-cloud-localstack/#official-documentation","title":"Official Documentation","text":"<ul> <li>LocalStack Documentation</li> <li>Terraform AWS Provider</li> <li>AWS Lambda Developer Guide</li> <li>boto3 Documentation</li> </ul>"},{"location":"en/ejercicios/09-cloud-localstack/#data-apis","title":"Data APIs","text":"<ul> <li>ISS Position: <code>https://api.wheretheiss.at/v1/satellites/25544</code></li> <li>ISS Astronauts: <code>http://api.open-notify.org/astros.json</code></li> <li>Geocoding: <code>https://nominatim.openstreetmap.org/search</code></li> </ul> <p>Course: Big Data with Python - From Zero to Production Instructor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>Academic References:</p> <ul> <li>Wittig, M., &amp; Wittig, A. (2019). Amazon Web Services in Action (2<sup>nd</sup> ed.). Manning Publications. ISBN: 978-1617295119.</li> <li>Brikman, Y. (2019). Terraform: Up &amp; Running (2<sup>nd</sup> ed.). O'Reilly Media. ISBN: 978-1492046905.</li> <li>Chauhan, A. (2020). Infrastructure as Code (IaC) for Beginners. Medium - Towards Data Science.</li> <li>Jonas, E., et al. (2019). Cloud programming simplified: A Berkeley view on serverless computing. arXiv preprint arXiv:1902.03383.</li> <li>LocalStack Team (2024). LocalStack Documentation. https://docs.localstack.cloud/</li> </ul>"},{"location":"en/entregas/guia-entregas/","title":"How to Submit Your Work","text":"<p>This guide explains step by step how to submit. You don't need to know advanced Git.</p>"},{"location":"en/entregas/guia-entregas/#summary-in-30-seconds","title":"Summary in 30 Seconds","text":"<pre><code>1. You fork the repo (only once)\n2. You work in YOUR fork\n3. You push your changes to YOUR fork\n4. The professor reviews YOUR fork automatically (no PR)\n</code></pre> <p>You do NOT need to create a Pull Request. The automatic system evaluates your PROMPTS.md file directly in your fork.</p>"},{"location":"en/entregas/guia-entregas/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>flowchart TB\n    subgraph Professor[\"PROFESSOR'S REPOSITORY\"]\n        P1[\"github.com/TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;The exercises are here&lt;br/&gt;You do NOT modify anything here\"]\n    end\n\n    subgraph Student[\"YOUR FORK (your copy)\"]\n        A1[\"github.com/YOUR_USERNAME/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;You work and push your code here\"]\n    end\n\n    subgraph Local[\"YOUR PC\"]\n        L1[\"Folder ejercicios-bigdata/&lt;br/&gt;&lt;br/&gt;You code here\"]\n    end\n\n    subgraph System[\"AUTOMATIC SYSTEM\"]\n        S1[\"Professor's script&lt;br/&gt;&lt;br/&gt;Reviews all forks&lt;br/&gt;Generates automatic grades\"]\n    end\n\n    Professor --&gt;|\"1. Fork (copy)\"| Student\n    Student --&gt;|\"2. Clone (download)\"| Local\n    Local --&gt;|\"3. Push (upload)\"| Student\n    Student --&gt;|\"4. Automatic review\"| System\n\n    style Professor fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Student fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style Local fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style System fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px</code></pre>"},{"location":"en/entregas/guia-entregas/#step-1-create-your-fork-only-once","title":"Step 1: Create Your Fork (only once)","text":"<p>A \"fork\" is your personal copy of the repository.</p> <ol> <li>Go to: github.com/TodoEconometria/ejercicios-bigdata</li> <li>Click the \"Fork\" button (top right)</li> <li>Click \"Create fork\"</li> <li>Done! Now you have <code>github.com/YOUR_USERNAME/ejercicios-bigdata</code></li> </ol> <p>You only do this ONCE</p> <p>Your fork is yours forever. All your work goes there.</p>"},{"location":"en/entregas/guia-entregas/#step-2-download-to-your-pc-only-once","title":"Step 2: Download to Your PC (only once)","text":"<pre><code># In your terminal (CMD, PowerShell, or Terminal)\ncd Documentos\ngit clone https://github.com/YOUR_USERNAME/ejercicios-bigdata.git\ncd ejercicios-bigdata\n</code></pre> <p>Replace <code>YOUR_USERNAME</code> with your GitHub username.</p> <p>Now you have the folder</p> <p>Look in <code>Documents/ejercicios-bigdata/</code>. You will always work there.</p>"},{"location":"en/entregas/guia-entregas/#step-3-create-your-submission-folder","title":"Step 3: Create Your Submission Folder","text":"<p>Inside your repository folder, create your personal folder:</p> <pre><code>For the Final Project:\nentregas/trabajo_final/lastname_firstname/\n\nFor database exercises:\nentregas/01_bases_de_datos/1.1_sqlite/lastname_firstname/\n</code></pre> <p>Name format</p> <ul> <li>All in lowercase</li> <li>No accents or spaces</li> <li>Format: <code>lastname_firstname</code></li> <li>Example: <code>garcia_maria</code>, <code>lopez_juan</code></li> </ul>"},{"location":"en/entregas/guia-entregas/#for-the-final-project-copy-the-template","title":"For the Final Project, copy the template:","text":"<pre><code># From the repository folder:\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/your_lastname_firstname/\n</code></pre> <p>This creates all the files you need to complete.</p>"},{"location":"en/entregas/guia-entregas/#step-4-work-and-document-your-prompts","title":"Step 4: Work and Document Your Prompts","text":""},{"location":"en/entregas/guia-entregas/#the-most-important-file-promptsmd","title":"The most important file: PROMPTS.md","text":"<p>Inside your folder you will find <code>PROMPTS.md</code>. This file is WHAT GETS EVALUATED.</p> <pre><code>entregas/trabajo_final/garcia_maria/\n\u251c\u2500\u2500 PROMPTS.md          \u2190 MANDATORY - Your AI prompts\n\u251c\u2500\u2500 docker-compose.yml  \u2190 Your infrastructure\n\u251c\u2500\u2500 pipeline.py         \u2190 Your code\n\u2514\u2500\u2500 ... other files\n</code></pre>"},{"location":"en/entregas/guia-entregas/#what-goes-in-promptsmd","title":"What goes in PROMPTS.md","text":"Section What to include Prompt A, B, C Your REAL prompts copied exactly as-is (with errors and all) Blueprint At the end, ask the AI for a professional summary <p>VERY IMPORTANT</p> <p>DO NOT correct your prompts. If you wrote \"how do i maek sparck read the csv\" with errors, paste THAT. The system detects if you \"cleaned\" your prompts.</p> <p>Perfect prompts in Part 1 = SUSPICIOUS.</p>"},{"location":"en/entregas/guia-entregas/#step-5-upload-your-work","title":"Step 5: Upload Your Work","text":"<p>When you finish (or want to save progress):</p> <pre><code># From the repository folder\ngit add .\ngit commit -m \"Final Project Submission - Garcia Maria\"\ngit push\n</code></pre> <p>What each command does</p> <ul> <li><code>git add .</code> \u2192 Stages all your files</li> <li><code>git commit -m \"...\"</code> \u2192 Saves with a message</li> <li><code>git push</code> \u2192 Uploads to your fork on GitHub</li> </ul>"},{"location":"en/entregas/guia-entregas/#step-6-verify-your-submission","title":"Step 6: Verify Your Submission","text":"<ol> <li>Go to your fork: <code>github.com/YOUR_USERNAME/ejercicios-bigdata</code></li> <li>Navigate to <code>entregas/trabajo_final/your_lastname_firstname/</code></li> <li>Verify that all your files are there</li> </ol> <p>Done!</p> <p>You don't need to do anything else. The automatic system reviews your PROMPTS.md file and generates grades based on your learning process.</p>"},{"location":"en/entregas/guia-entregas/#keep-your-fork-up-to-date","title":"Keep Your Fork Up to Date","text":"<p>The professor adds new exercises. Your fork does NOT update on its own.</p>"},{"location":"en/entregas/guia-entregas/#easy-method-from-github","title":"Easy Method (from GitHub)","text":"<ol> <li>Go to your fork on GitHub</li> <li>If you see a yellow banner \"This branch is X commits behind\", click it</li> <li>Click \"Sync fork\" \u2192 \"Update branch\"</li> <li>On your PC: <code>git pull</code></li> </ol>"},{"location":"en/entregas/guia-entregas/#terminal-method","title":"Terminal Method","text":"<pre><code># Add the professor's repo as \"upstream\" (only once)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Update\ngit fetch upstream\ngit merge upstream/main\ngit push\n</code></pre> <p>When to sync</p> <p>Do it every Monday before class to get the new exercises.</p>"},{"location":"en/entregas/guia-entregas/#submission-structure-final-project","title":"Submission Structure - Final Project","text":"<pre><code>entregas/trabajo_final/lastname_firstname/\n\u2502\n\u251c\u2500\u2500 PROMPTS.md              \u2190 THE MOST IMPORTANT (this is evaluated)\n\u2502\n\u251c\u2500\u2500 01_README.md            \u2190 Your research question\n\u251c\u2500\u2500 02_INFRAESTRUCTURA.md   \u2190 Explanation of your Docker setup\n\u251c\u2500\u2500 03_RESULTADOS.md        \u2190 Graphs and interpretation\n\u251c\u2500\u2500 04_REFLEXION_IA.md      \u2190 3 key moments\n\u251c\u2500\u2500 05_RESPUESTAS.md        \u2190 Comprehension questions\n\u2502\n\u251c\u2500\u2500 docker-compose.yml      \u2190 Your working YAML\n\u251c\u2500\u2500 pipeline.py             \u2190 Your ETL + analysis code\n\u251c\u2500\u2500 requirements.txt        \u2190 Dependencies\n\u2502\n\u2514\u2500\u2500 .gitignore              \u2190 Exclude large data files\n</code></pre>"},{"location":"en/entregas/guia-entregas/#what-not-to-upload","title":"What NOT to Upload","text":"<p>The <code>.gitignore</code> already protects this, but remember:</p> <ul> <li>No data files (<code>.csv</code>, <code>.parquet</code>, <code>.db</code>)</li> <li>No <code>venv/</code> or <code>.venv/</code> folder</li> <li>No <code>__pycache__/</code> folder</li> <li>No <code>.env</code> files with credentials</li> <li>No files larger than 10MB</li> </ul>"},{"location":"en/entregas/guia-entregas/#how-it-is-evaluated-prompts-based-system","title":"How It Is Evaluated (PROMPTS-Based System)","text":"<p>THE MOST IMPORTANT: PROMPTS.md</p> <p>The PROMPTS.md file is what gets evaluated. Not the code, not the YAML, but YOUR documented AI PROMPTS.</p> <p>The automatic system reviews:</p> <pre><code>1. Reads the student list (registered forks)\n2. For each fork:\n   - Verifies that PROMPTS.md exists (MANDATORY)\n   - Analyzes quality and authenticity of prompts\n   - Detects if prompts were \"cleaned\" by AI\n   - Reviews coherence between prompts and submitted code\n   - Calculates automatic grade based on learning process\n3. Generates report with:\n   - Ranking of all students\n   - Outstanding (possible bonus)\n   - Suspicious (require verification)\n</code></pre>"},{"location":"en/entregas/guia-entregas/#automatic-alerts","title":"Automatic Alerts","text":"Alert Meaning OUTSTANDING Exceptional work, review for bonus NORMAL Meets requirements, automatic grade REVIEW Something doesn't add up, professor will verify REJECTED Copy detected or requirements not met"},{"location":"en/entregas/guia-entregas/#common-problems","title":"Common Problems","text":""},{"location":"en/entregas/guia-entregas/#i-dont-have-the-template","title":"\"I don't have the template\"","text":"<pre><code># Update your fork first\ngit fetch upstream\ngit merge upstream/main\n\n# Now copy the template\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/your_lastname/\n</code></pre>"},{"location":"en/entregas/guia-entregas/#git-is-asking-for-username-and-password","title":"\"Git is asking for username and password\"","text":"<p>Use your GitHub account. If it fails, configure:</p> <pre><code>git config --global user.email \"tu@email.com\"\ngit config --global user.name \"Tu Nombre\"\n</code></pre>"},{"location":"en/entregas/guia-entregas/#my-changes-dont-appear-on-github","title":"\"My changes don't appear on GitHub\"","text":"<p>Verify you did all 3 steps:</p> <pre><code>git add .                    # 1. Stage\ngit commit -m \"message\"      # 2. Save\ngit push                     # 3. Upload  \u2190 This one actually uploads\n</code></pre>"},{"location":"en/entregas/guia-entregas/#i-want-to-start-over","title":"\"I want to start over\"","text":"<pre><code># Delete your folder and copy the template again\nrm -rf entregas/trabajo_final/your_lastname/\ncp -r trabajo_final/plantilla/ entregas/trabajo_final/your_lastname/\n</code></pre>"},{"location":"en/entregas/guia-entregas/#dates-and-deadlines","title":"Dates and Deadlines","text":"Submission Deadline Final Project [See course calendar] <p>Late submissions</p> <p>The system reviews on the indicated date. Whatever is not in your fork by that date will not be evaluated.</p>"},{"location":"en/entregas/guia-entregas/#visual-summary","title":"Visual Summary","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    YOUR WORKFLOW                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  1. Fork (once)                                                 \u2502\n\u2502     \u2514\u2500\u2500 Create your copy on GitHub                              \u2502\n\u2502                                                                 \u2502\n\u2502  2. Clone (once)                                                \u2502\n\u2502     \u2514\u2500\u2500 Download to your PC                                     \u2502\n\u2502                                                                 \u2502\n\u2502  3. Copy template                                               \u2502\n\u2502     \u2514\u2500\u2500 cp -r trabajo_final/plantilla/ entregas/.../your_name/  \u2502\n\u2502                                                                 \u2502\n\u2502  4. Work with AI                                                \u2502\n\u2502     \u2514\u2500\u2500 Save prompts in PROMPTS.md (with errors and all)        \u2502\n\u2502                                                                 \u2502\n\u2502  5. Upload changes                                              \u2502\n\u2502     \u2514\u2500\u2500 git add . &amp;&amp; git commit -m \"...\" &amp;&amp; git push            \u2502\n\u2502                                                                 \u2502\n\u2502  6. Verify on GitHub                                            \u2502\n\u2502     \u2514\u2500\u2500 Confirm your files are there                            \u2502\n\u2502                                                                 \u2502\n\u2502  7. Automatic evaluation                                        \u2502\n\u2502     \u2514\u2500\u2500 The professor reviews all forks without you doing       \u2502\n\u2502         anything                                                \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"en/entregas/guia-entregas/#help","title":"Help","text":"<p>If you have problems:</p> <ol> <li>Review this guide again</li> <li>Ask a classmate</li> <li>Ask the professor in class</li> <li>Check the sync guide</li> </ol> <p>Last updated: 2026-02-04</p>"},{"location":"en/entregas/streaming-cloud/","title":"Submissions: Streaming and Cloud (Modules 08-09)","text":"<p>This guide explains how to submit the exercises for Streaming with Kafka and Cloud with LocalStack.</p>"},{"location":"en/entregas/streaming-cloud/#summary","title":"Summary","text":"<p>These modules are optional advanced. You complete them if you want to go beyond the basic Final Project.</p> Module Topic Submission 08 Kafka + Spark Streaming <code>entregas/streaming_cloud/kafka/</code> 09 LocalStack + Terraform <code>entregas/streaming_cloud/localstack/</code>"},{"location":"en/entregas/streaming-cloud/#submission-structure","title":"Submission Structure","text":"<pre><code>entregas/streaming_cloud/lastname_firstname/\n\u2502\n\u251c\u2500\u2500 PROMPTS.md              \u2190 THE MOST IMPORTANT\n\u2502\n\u251c\u2500\u2500 kafka/                  \u2190 Module 08 (if you completed it)\n\u2502   \u251c\u2500\u2500 docker-compose.yml  \u2190 Kafka in KRaft mode\n\u2502   \u251c\u2500\u2500 productor.py        \u2190 Your producer\n\u2502   \u251c\u2500\u2500 consumidor.py       \u2190 Your consumer\n\u2502   \u2514\u2500\u2500 capturas/\n\u2502       \u251c\u2500\u2500 kafka_ui.png\n\u2502       \u2514\u2500\u2500 alertas.png\n\u2502\n\u2514\u2500\u2500 localstack/             \u2190 Module 09 (if you completed it)\n    \u251c\u2500\u2500 docker-compose.yml  \u2190 LocalStack\n    \u251c\u2500\u2500 main.tf             \u2190 Your Terraform\n    \u251c\u2500\u2500 lambdas/\n    \u2502   \u2514\u2500\u2500 capturar.py\n    \u2514\u2500\u2500 capturas/\n        \u251c\u2500\u2500 terraform_apply.png\n        \u2514\u2500\u2500 s3_bucket.png\n</code></pre>"},{"location":"en/entregas/streaming-cloud/#the-promptsmd-file","title":"The PROMPTS.md File","text":"<p>Just like the Final Project, the most important thing is to document your real prompts.</p>"},{"location":"en/entregas/streaming-cloud/#template","title":"Template","text":"<pre><code># PROMPTS - Streaming and Cloud\n\n**Student:** [Your name]\n**Date:** [Submission date]\n**Modules completed:** [08 / 09 / both]\n\n---\n\n## Part 1: My Prompts (EXACTLY as I wrote them)\n\n### Challenge 1: Launch Kafka\n[Paste your real prompt, with errors and all]\n\n**AI Response:**\n[Summary of what it answered]\n\n**Result:**\n- [ ] Worked on the first try\n- [ ] Had to adjust (explain what)\n- [ ] Did not work (explain the error)\n\n### Challenge 2: Producer\n[...]\n\n### Challenge 3: Consumer\n[...]\n\n(continue with each challenge you completed)\n\n---\n\n## Part 2: Screenshots\n\nInclude screenshots of:\n- Kafka UI showing messages (if you did 08)\n- Alerts in console (if you did 08)\n- Successful `terraform apply` (if you did 09)\n- S3 bucket with data (if you did 09)\n\n---\n\n## Part 3: Reflection\n\n### What I learned about streaming/cloud\n[2-3 paragraphs]\n\n### Difference between batch and streaming\n[Your understanding]\n\n### What I would do differently\n[Self-criticism]\n\n---\n\n## Part 4: Blueprint (generated by AI)\n\nAsk your AI:\n&gt; \"Summarize in professional format the previous prompts,\n&gt; highlighting learning patterns and progression\"\n\n[Paste the response here]\n</code></pre>"},{"location":"en/entregas/streaming-cloud/#what-is-evaluated","title":"What Is Evaluated","text":"Criterion Weight Description Prompt authenticity 40% Real prompts, not cleaned Challenges completed 30% How many challenges you finished Screenshots 15% Visual evidence that it works Reflection 15% Conceptual understanding"},{"location":"en/entregas/streaming-cloud/#dashboard-bonus","title":"Dashboard Bonus","text":"<p>If you created your own earthquake dashboard or ISS tracker:</p> Criterion Bonus Working dashboard +10% Live updates +5% Professional design +5%"},{"location":"en/entregas/streaming-cloud/#how-to-submit","title":"How to Submit","text":""},{"location":"en/entregas/streaming-cloud/#step-1-create-your-folder","title":"Step 1: Create your folder","text":"<pre><code># From the root of your fork\nmkdir -p entregas/streaming_cloud/your_lastname_firstname\n</code></pre>"},{"location":"en/entregas/streaming-cloud/#step-2-copy-your-files","title":"Step 2: Copy your files","text":"<pre><code># If you did Kafka\nmkdir -p entregas/streaming_cloud/your_lastname_firstname/kafka\ncp docker-compose.yml productor.py consumidor.py entregas/.../kafka/\n\n# If you did LocalStack\nmkdir -p entregas/streaming_cloud/your_lastname_firstname/localstack\ncp -r *.tf lambdas/ entregas/.../localstack/\n</code></pre>"},{"location":"en/entregas/streaming-cloud/#step-3-create-promptsmd","title":"Step 3: Create PROMPTS.md","text":"<p>Use the template above and document your entire process.</p>"},{"location":"en/entregas/streaming-cloud/#step-4-upload","title":"Step 4: Upload","text":"<pre><code>git add .\ngit commit -m \"Streaming/Cloud Submission - Lastname Firstname\"\ngit push\n</code></pre>"},{"location":"en/entregas/streaming-cloud/#available-challenges","title":"Available Challenges","text":""},{"location":"en/entregas/streaming-cloud/#module-08-kafka","title":"Module 08: Kafka","text":"Challenge Difficulty Description 1 Basic Launch Kafka with Docker 2 Basic Create Python producer 3 Basic Create Python consumer 4 Intermediate Connect USGS API 5 Intermediate Alert system 6 Advanced Spark Structured Streaming Final Advanced Your own dashboard <p>See details: Streaming with Kafka</p>"},{"location":"en/entregas/streaming-cloud/#module-09-localstack","title":"Module 09: LocalStack","text":"Challenge Difficulty Description 1 Basic Launch LocalStack 2 Basic S3 Bucket with Terraform 3 Intermediate Lambda Hello World 4 Intermediate Lambda consumes API 5 Intermediate Save to S3 6 Advanced EventBridge scheduling 7 Advanced DynamoDB metadata Final Advanced Your own ISS Tracker <p>See details: Cloud with LocalStack</p>"},{"location":"en/entregas/streaming-cloud/#reference-examples","title":"Reference Examples","text":"<p>Reference dashboards are available for inspiration:</p> <ul> <li>Global Seismic Observatory - Earthquake visualization example</li> <li>ISS Tracker - Space tracker example</li> </ul> <p>Do not copy</p> <p>These are the professor's examples. Your dashboard should have your own style. The system detects excessive similarity.</p>"},{"location":"en/entregas/streaming-cloud/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"en/entregas/streaming-cloud/#is-it-mandatory","title":"Is it mandatory?","text":"<p>No. These modules are bonus for those who want to go deeper.</p>"},{"location":"en/entregas/streaming-cloud/#can-i-do-only-one","title":"Can I do only one?","text":"<p>Yes. You can submit only Kafka (08) or only LocalStack (09).</p>"},{"location":"en/entregas/streaming-cloud/#how-does-it-affect-my-grade","title":"How does it affect my grade?","text":"<p>Completing correctly = bonus on top of your Final Project grade.</p>"},{"location":"en/entregas/streaming-cloud/#do-i-need-spark-for-module-08","title":"Do I need Spark for module 08?","text":"<p>The Spark Streaming challenge is advanced. You can do challenges 1-5 without Spark.</p>"},{"location":"en/entregas/streaming-cloud/#is-localstack-the-same-as-real-aws","title":"Is LocalStack the same as real AWS?","text":"<p>The code is identical. The difference is that LocalStack runs on your machine at no cost. If your code works on LocalStack, it works on AWS.</p> <p>Course: Big Data with Python - From Zero to Production Professor: Juan Marcelo Gutierrez Miranda | @TodoEconometria Hash ID: 4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c Methodology: Progressive exercises with real data and professional tools</p> <p>References: - Kreps, J., Narkhede, N., &amp; Rao, J. (2011). Kafka: A distributed messaging system for log processing. - Brikman, Y. (2019). Terraform: Up &amp; Running (2<sup>nd</sup> ed.). O'Reilly Media. - LocalStack Team (2024). LocalStack Documentation. https://docs.localstack.cloud/</p>"},{"location":"en/git-github/","title":"Git and GitHub","text":"<p>Guides for working with Git and GitHub in this course.</p>"},{"location":"en/git-github/#what-will-you-find-here","title":"What will you find here?","text":""},{"location":"en/git-github/#fork-and-clone","title":"Fork and Clone","text":"<p>Learn how to create your copy of the repository and clone it to your computer:</p> <ul> <li>What is a Fork and why do you need it</li> <li>How to Fork the repository</li> <li>How to clone your Fork to your PC</li> <li>Configuring remotes (origin and upstream)</li> </ul>"},{"location":"en/git-github/#sync-fork","title":"Sync Fork","text":"<p>Keep your Fork up to date with the professor's new exercises:</p> <ul> <li>Why your Fork does NOT update automatically</li> <li>How to sync from GitHub Web (easy)</li> <li>How to sync from Terminal (complete)</li> <li>Resolving merge conflicts</li> <li>Visual diagrams of the complete workflow</li> </ul>"},{"location":"en/git-github/#useful-commands","title":"Useful Commands","text":"<p>Git cheatsheet for daily use:</p> <ul> <li>Basic commands</li> <li>Advanced commands</li> <li>Useful shortcuts</li> <li>Solving common problems</li> </ul>"},{"location":"en/git-github/#workflow-no-pull-request","title":"Workflow (No Pull Request)","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Clone to PC]\n    B --&gt; C[Work]\n    C --&gt; D[Document PROMPTS.md]\n    D --&gt; E[Commit]\n    E --&gt; F[Push to your Fork]\n    F --&gt; G[Automatic Evaluation]</code></pre> <p>Simplified system</p> <p>You don't need to create a Pull Request. The system evaluates your <code>PROMPTS.md</code> automatically. Just upload your work with <code>git push</code>.</p>"},{"location":"en/git-github/#basic-concepts","title":"Basic Concepts","text":""},{"location":"en/git-github/#git-vs-github","title":"Git vs GitHub","text":"<p>Git</p> <p>Git is a version control system that runs on your computer. It allows you to:</p> <ul> <li>Save versions of your code</li> <li>Revert to previous versions</li> <li>Work on multiple branches</li> <li>Collaborate with others</li> </ul> <p>GitHub</p> <p>GitHub is a cloud platform where you store your code. It allows you to:</p> <ul> <li>Share code publicly</li> <li>Collaborate with other developers</li> <li>Host projects</li> <li>Manage projects and collaboration</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  GIT vs GITHUB                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  GIT (Program on your PC)                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  Your computer                       \u2502                 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502  \u2502  \u2502  \ud83d\udcc1 Folder with your code       \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 exercise1.py               \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 exercise2.py               \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500 .git/  \u2190 Local history     \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                      \u2502                                       \u2502\n\u2502                      \u2502 git push (upload)                    \u2502\n\u2502                      \u2193                                       \u2502\n\u2502  GITHUB (On the Internet)                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502  \ud83c\udf10 github.com                        \u2502                 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502  \u2502  \u2502  \ud83d\udce6 Your online repository      \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2502  (Visible in the browser)       \u2502 \u2502                 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"en/git-github/#first-steps","title":"First Steps","text":"<p>Never used Git?</p> <p>Start with Fork and Clone where we explain everything from scratch.</p> <p>Already have the repository cloned?</p> <p>Learn how to Sync your Fork to get new exercises.</p> <p>Completed an exercise?</p> <p>Just do <code>git push</code> to your fork. Read the Submission Guide.</p>"},{"location":"en/git-github/#help-and-resources","title":"Help and Resources","text":""},{"location":"en/git-github/#common-problems","title":"Common Problems","text":"<p>Check the Useful Commands section where you will find solutions to common problems such as:</p> <ul> <li>\"fatal: not a git repository\"</li> <li>\"Your branch is behind origin/main\"</li> <li>\"CONFLICT (content): Merge conflict\"</li> <li>\"Permission denied (publickey)\"</li> </ul>"},{"location":"en/git-github/#external-resources","title":"External Resources","text":"<ul> <li>Git Handbook</li> <li>GitHub Guides</li> <li>Atlassian Git Tutorial</li> <li>Oh Shit, Git!?! - For when things go wrong</li> </ul>"},{"location":"en/git-github/comandos-utiles/","title":"Useful Git Commands","text":"<p>Git command cheatsheet for daily use in the course.</p>"},{"location":"en/git-github/comandos-utiles/#basic-commands","title":"Basic Commands","text":""},{"location":"en/git-github/comandos-utiles/#initial-configuration","title":"Initial Configuration","text":"<pre><code># Configure name\ngit config --global user.name \"Tu Nombre\"\n\n# Configure email\ngit config --global user.email \"tu@email.com\"\n\n# View configuration\ngit config --list\n\n# View specific configuration\ngit config user.name\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#clone-and-update","title":"Clone and Update","text":"<pre><code># Clone your fork\ngit clone https://github.com/TU_USUARIO/ejercicios-bigdata.git\n\n# Enter the folder\ncd ejercicios-bigdata\n\n# Add upstream (professor's repo)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# View configured remotes\ngit remote -v\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#daily-work","title":"Daily Work","text":""},{"location":"en/git-github/comandos-utiles/#status-and-changes","title":"Status and Changes","text":"<pre><code># View current status\ngit status\n\n# View unsaved changes\ngit diff\n\n# View changes in a specific file\ngit diff archivo.py\n\n# View commit history\ngit log\n\n# View abbreviated history\ngit log --oneline\n\n# View changes of a specific commit\ngit show abc123d\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#save-changes","title":"Save Changes","text":"<pre><code># Add specific file\ngit add archivo.py\n\n# Add all modified files\ngit add .\n\n# Add only Python files\ngit add *.py\n\n# Make a commit\ngit commit -m \"Descriptive message\"\n\n# Commit already tracked files (skip add)\ngit commit -am \"Descriptive message\"\n\n# Amend last commit (before push)\ngit commit --amend -m \"New message\"\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#branches","title":"Branches","text":""},{"location":"en/git-github/comandos-utiles/#create-and-switch","title":"Create and Switch","text":"<pre><code># View local branches\ngit branch\n\n# View all branches (including remote)\ngit branch -a\n\n# Create new branch\ngit branch garcia-ejercicio-01\n\n# Switch to a branch\ngit checkout garcia-ejercicio-01\n\n# Create and switch in one command\ngit checkout -b garcia-ejercicio-01\n\n# Switch to main\ngit checkout main\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#merge-and-delete","title":"Merge and Delete","text":"<pre><code># Merge a branch into the current one\ngit merge branch-name\n\n# Delete local branch\ngit branch -d garcia-ejercicio-01\n\n# Force delete (if it has unmerged changes)\ngit branch -D garcia-ejercicio-01\n\n# Delete remote branch\ngit push origin --delete garcia-ejercicio-01\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#synchronization","title":"Synchronization","text":""},{"location":"en/git-github/comandos-utiles/#download-changes","title":"Download Changes","text":"<pre><code># Download changes from professor (upstream)\ngit fetch upstream\n\n# Download and merge from your fork (origin)\ngit pull origin main\n\n# View differences with upstream\ngit log HEAD..upstream/main\n\n# View commits you don't have\ngit log HEAD..upstream/main --oneline\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#full-fork-sync","title":"Full Fork Sync","text":"<pre><code># Complete workflow to sync\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\ngit push origin main\n\n# Or in a single line\ngit checkout main &amp;&amp; git fetch upstream &amp;&amp; git merge upstream/main &amp;&amp; git push origin main\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#upload-changes","title":"Upload Changes","text":"<pre><code># Push current branch to origin\ngit push origin branch-name\n\n# Push main\ngit push origin main\n\n# Push and set upstream (first time)\ngit push -u origin branch-name\n\n# After that you only need\ngit push\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/git-github/comandos-utiles/#undo-changes","title":"Undo Changes","text":"<pre><code># Discard changes in a file (before add)\ngit checkout -- archivo.py\n\n# Discard all unsaved changes\ngit checkout -- .\n\n# Remove file from staging (after add, before commit)\ngit reset HEAD archivo.py\n\n# Undo last commit (keeps changes)\ngit reset --soft HEAD~1\n\n# Undo last commit (discards changes)\ngit reset --hard HEAD~1\n\n# Go back to a specific commit\ngit reset --hard abc123d\n</code></pre> <p>Be careful with --hard</p> <p><code>git reset --hard</code> permanently deletes changes. Only use it if you are sure.</p>"},{"location":"en/git-github/comandos-utiles/#stash-temporarily-save","title":"Stash (Temporarily Save)","text":"<pre><code># Temporarily save changes\ngit stash\n\n# Save with a message\ngit stash save \"WIP: working on exercise 03\"\n\n# View stash list\ngit stash list\n\n# Apply last stash\ngit stash apply\n\n# Apply and remove last stash\ngit stash pop\n\n# Apply specific stash\ngit stash apply stash@{1}\n\n# Remove stash\ngit stash drop stash@{0}\n\n# Remove all stashes\ngit stash clear\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#conflicts","title":"Conflicts","text":"<pre><code># View files with conflicts\ngit status\n\n# After resolving manually\ngit add resolved-file.py\ngit commit -m \"Resolve conflict in file\"\n\n# Abort merge with conflicts\ngit merge --abort\n\n# View merge tool\ngit mergetool\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#information-and-search","title":"Information and Search","text":""},{"location":"en/git-github/comandos-utiles/#inspect-history","title":"Inspect History","text":"<pre><code># View detailed history\ngit log --graph --decorate --all\n\n# View who modified each line of a file\ngit blame archivo.py\n\n# Search in history\ngit log --grep=\"keyword\"\n\n# View files modified in each commit\ngit log --stat\n\n# View changes from a specific author\ngit log --author=\"Your Name\"\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#search-code","title":"Search Code","text":"<pre><code># Search in tracked files\ngit grep \"keyword\"\n\n# Search in Python files\ngit grep \"keyword\" -- \"*.py\"\n\n# Search showing line number\ngit grep -n \"keyword\"\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#shortcuts-and-aliases","title":"Shortcuts and Aliases","text":""},{"location":"en/git-github/comandos-utiles/#configure-aliases","title":"Configure Aliases","text":"<pre><code># Create alias for status\ngit config --global alias.st status\n\n# Create alias for checkout\ngit config --global alias.co checkout\n\n# Create alias for commit\ngit config --global alias.ci commit\n\n# Create alias for branch\ngit config --global alias.br branch\n\n# Alias for pretty log\ngit config --global alias.lg \"log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\"\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#use-aliases","title":"Use Aliases","text":"<pre><code># Instead of: git status\ngit st\n\n# Instead of: git checkout main\ngit co main\n\n# Instead of: git commit -m \"message\"\ngit ci -m \"message\"\n\n# View pretty log\ngit lg\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#course-workflow","title":"Course Workflow","text":""},{"location":"en/git-github/comandos-utiles/#start-a-new-exercise","title":"Start a New Exercise","text":"<pre><code># 1. Update main\ngit checkout main\ngit pull origin main\ngit fetch upstream\ngit merge upstream/main\n\n# 2. Create branch for exercise\ngit checkout -b garcia-ejercicio-01\n\n# 3. Work...\n# ... edit files ...\n\n# 4. Save work\ngit add .\ngit commit -m \"Implement SQLite data loading\"\n\n# 5. Push to GitHub\ngit push -u origin garcia-ejercicio-01\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#update-exercise-branch","title":"Update Exercise Branch","text":"<pre><code># If the professor added changes while you were working\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\ngit checkout garcia-ejercicio-01\ngit merge main\ngit push origin garcia-ejercicio-01\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#apply-professors-feedback","title":"Apply Professor's Feedback","text":"<pre><code># 1. Make sure you are on your branch\ngit checkout garcia-ejercicio-01\n\n# 2. Make corrections\n# ... edit files ...\n\n# 3. Save and push\ngit add .\ngit commit -m \"Apply feedback: optimize queries\"\ngit push origin garcia-ejercicio-01\n\n# The PR updates automatically\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#advanced-commands","title":"Advanced Commands","text":""},{"location":"en/git-github/comandos-utiles/#cherry-pick","title":"Cherry Pick","text":"<pre><code># Apply a specific commit to the current branch\ngit cherry-pick abc123d\n\n# Apply without automatic commit\ngit cherry-pick -n abc123d\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#rebase","title":"Rebase","text":"<pre><code># Rebase current branch with main\ngit rebase main\n\n# Interactive rebase (last 3 commits)\ngit rebase -i HEAD~3\n\n# Continue rebase after resolving conflicts\ngit rebase --continue\n\n# Abort rebase\ngit rebase --abort\n</code></pre> <p>Be careful with Rebase</p> <p>Do not rebase commits that you have already pushed to GitHub (after push).</p>"},{"location":"en/git-github/comandos-utiles/#reflog","title":"Reflog","text":"<pre><code># View history of all actions\ngit reflog\n\n# Recover a \"lost\" commit\ngit reflog\ngit checkout abc123d\ngit checkout -b recovered-branch\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#tricks-and-tips","title":"Tricks and Tips","text":""},{"location":"en/git-github/comandos-utiles/#useful-configuration","title":"Useful Configuration","text":"<pre><code># Colorize output\ngit config --global color.ui auto\n\n# Default editor (VSCode)\ngit config --global core.editor \"code --wait\"\n\n# Temporarily save credentials\ngit config --global credential.helper cache\n\n# Permanently save credentials (Windows)\ngit config --global credential.helper wincred\n\n# Ignore file permission changes\ngit config core.fileMode false\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#gitignore","title":".gitignore","text":"<p>Create a <code>.gitignore</code> file in the project root:</p> <pre><code># Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\n.venv/\n\n# PyCharm\n.idea/\n\n# VSCode\n.vscode/\n\n# Jupyter\n.ipynb_checkpoints/\n\n# Large data files\n*.csv\n*.db\n*.parquet\ndatos/grandes/\n\n# System\n.DS_Store\nThumbs.db\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#common-errors-and-solutions","title":"Common Errors and Solutions","text":""},{"location":"en/git-github/comandos-utiles/#fatal-not-a-git-repository","title":"\"fatal: not a git repository\"","text":"<pre><code># Solution: Navigate to the project folder\ncd path/to/ejercicios-bigdata\n\n# Verify you are in the correct folder\ngit status\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#your-branch-is-behind-originmain","title":"\"Your branch is behind 'origin/main'\"","text":"<pre><code># Solution: Update your local branch\ngit pull origin main\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#conflict-content-merge-conflict","title":"\"CONFLICT (content): Merge conflict\"","text":"<pre><code># Solution:\n# 1. Open the file with the conflict\n# 2. Look for the markers &lt;&lt;&lt;&lt;&lt;&lt;&lt; ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n# 3. Manually edit, choose which code to keep\n# 4. Remove the markers\n# 5. Save and commit\ngit add resolved-file.py\ngit commit -m \"Resolve conflict\"\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#permission-denied-publickey","title":"\"Permission denied (publickey)\"","text":"<pre><code># Solution: Use HTTPS instead of SSH\ngit remote set-url origin https://github.com/TU_USUARIO/ejercicios-bigdata.git\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#error-failed-to-push-some-refs","title":"\"error: failed to push some refs\"","text":"<pre><code># Cause: Your local branch is behind the remote\n# Solution: Pull first\ngit pull origin your-branch\n# Then push\ngit push origin your-branch\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#emergency-commands","title":"Emergency Commands","text":""},{"location":"en/git-github/comandos-utiles/#recover-lost-work","title":"Recover Lost Work","text":"<pre><code># View all changes\ngit reflog\n\n# Go back to a previous state\ngit reset --hard abc123d\n\n# Recover a deleted file\ngit checkout HEAD -- archivo.py\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#clean-repository","title":"Clean Repository","text":"<pre><code># Remove untracked files (dry run)\ngit clean -n\n\n# Remove untracked files (execute)\ngit clean -f\n\n# Remove untracked files and folders\ngit clean -fd\n\n# Include files ignored in .gitignore\ngit clean -fdx\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#additional-resources","title":"Additional Resources","text":""},{"location":"en/git-github/comandos-utiles/#built-in-help","title":"Built-in Help","text":"<pre><code># General help\ngit help\n\n# Help for a specific command\ngit help commit\ngit commit --help\n\n# Short help version\ngit commit -h\n</code></pre>"},{"location":"en/git-github/comandos-utiles/#useful-links","title":"Useful Links","text":"<ul> <li>Git Cheat Sheet (GitHub)</li> <li>Visualize Git</li> <li>Learn Git Branching</li> <li>Oh Shit, Git!?!</li> </ul>"},{"location":"en/git-github/comandos-utiles/#next-steps","title":"Next Steps","text":"<p>Now that you know the essential commands:</p> <ul> <li>Fork and Clone - Initial project setup</li> <li>Sync Fork - Keep your fork up to date</li> <li>Submission Guide - How to submit exercises</li> </ul>"},{"location":"en/git-github/fork-clone/","title":"Fork and Clone","text":"<p>Complete guide to create your copy of the repository and work with it.</p>"},{"location":"en/git-github/fork-clone/#what-is-git-what-is-github","title":"What is Git? What is GitHub?","text":"<p>Git</p> <p>Git = Version control system (like \"saving versions\" of your code)</p> <p>GitHub</p> <p>GitHub = Cloud where you store your code (like Dropbox, but for code)</p> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph Local[\"\ud83d\udcbb GIT - Your Computer\"]\n        direction TB\n        PC[\"\ud83d\udcc1 Folder with your code&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 exercise1.py&lt;br/&gt;\u251c\u2500\u2500 exercise2.py&lt;br/&gt;\u2514\u2500\u2500 .git/ \u2190 Local history\"]\n    end\n\n    subgraph Cloud[\"\ud83c\udf10 GITHUB - Internet (github.com)\"]\n        direction TB\n        Repo[\"\ud83d\udce6 Your online repository&lt;br/&gt;&lt;br/&gt;Visible in the browser&lt;br/&gt;Backup in the cloud\"]\n    end\n\n    PC --&gt;|git push&lt;br/&gt;Upload changes| Cloud\n    Cloud --&gt;|git pull&lt;br/&gt;Download changes| PC\n\n    style Local fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style Cloud fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style PC fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n    style Repo fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px</code></pre>"},{"location":"en/git-github/fork-clone/#what-is-a-fork","title":"What is a FORK?","text":"<p>A fork is making YOUR OWN COPY of the professor's repository on GitHub.</p> <p>Think of it this way:</p> <ul> <li> The professor has a book (repository)</li> <li> You make a complete photocopy of the book (fork)</li> <li> Now you can write in YOUR copy without affecting the original</li> <li> When you finish, you upload your work with <code>git push</code> (automatic evaluation)</li> </ul> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TD\n    subgraph Original[\"\ud83d\udc68\u200d\ud83c\udfeb ORIGINAL REPOSITORY (Professor)\"]\n        direction TB\n        RepoProf[\"TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\ud83d\udcc1 ejercicio_01/&lt;br/&gt;\ud83d\udcc1 ejercicio_02/&lt;br/&gt;\ud83d\udcc1 datos/&lt;br/&gt;&lt;br/&gt;\ud83d\udd12 You CANNOT modify directly\"]\n    end\n\n    ForkAction{{\"\ud83c\udf74 MAKE FORK&lt;br/&gt;(Click the 'Fork' button)\"}}\n\n    subgraph TuCopia[\"\ud83d\udc64 YOUR FORK (Your Personal Copy)\"]\n        direction TB\n        RepoTuyo[\"YOUR_USERNAME/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\ud83d\udcc1 ejercicio_01/&lt;br/&gt;\ud83d\udcc1 ejercicio_02/&lt;br/&gt;\ud83d\udcc1 datos/&lt;br/&gt;&lt;br/&gt;\u2705 This copy YOU CAN modify\"]\n    end\n\n    Original --&gt; ForkAction\n    ForkAction --&gt;|Creates a complete&lt;br/&gt;and independent copy| TuCopia\n\n    style Original fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style TuCopia fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style ForkAction fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n    style RepoProf fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style RepoTuyo fill:#e8f5e9,stroke:#388e3c,stroke-width:2px</code></pre>"},{"location":"en/git-github/fork-clone/#step-1-fork-the-repository","title":"STEP 1: Fork the Repository","text":""},{"location":"en/git-github/fork-clone/#step-by-step-instructions","title":"Step-by-Step Instructions","text":"<p>1. Go to the professor's repository:</p> <p>Open your browser and navigate to:</p> <pre><code>https://github.com/TodoEconometria/ejercicios-bigdata\n</code></pre> <p>2. Fork (copy to your account):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GitHub - Repository Page              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                          \u2502\n\u2502  [\u2b50 Star]  [\ud83c\udf74 Fork]  [\u2b07 Code]        \u2502\n\u2502              \u2191                           \u2502\n\u2502              \u2514\u2500\u2500 CLICK HERE             \u2502\n\u2502                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Click the \"Fork\" button (top right)</li> <li>Select your GitHub account as the destination</li> <li>Wait a few seconds while GitHub copies everything</li> </ul> <p>3. Verify your fork:</p> <p>You should now be on YOUR copy:</p> <pre><code>https://github.com/YOUR_USERNAME/ejercicios-bigdata\n        \u2191\n        \u2514\u2500\u2500 Your username should appear here\n</code></pre> <p> Done! You now have your personal copy of the repository.</p>"},{"location":"en/git-github/fork-clone/#step-2-clone-your-fork-to-your-computer","title":"STEP 2: Clone YOUR Fork to Your Computer","text":""},{"location":"en/git-github/fork-clone/#what-does-clone-mean","title":"What does \"clone\" mean?","text":"<p>Clone = Download all the code from GitHub to your computer</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \ud83c\udf10 GitHub (Your Fork)                  \u2502\n\u2502  https://github.com/YOUR_USERNAME/...   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 git clone (download)\n                  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \ud83d\udcbb Your PC                              \u2502\n\u2502  \ud83d\udcc1 Folder: ejercicios-bigdata/         \u2502\n\u2502     \u251c\u2500\u2500 ejercicio_01/                   \u2502\n\u2502     \u251c\u2500\u2500 ejercicio_02/                   \u2502\n\u2502     \u2514\u2500\u2500 datos/                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"en/git-github/fork-clone/#step-by-step-instructions_1","title":"Step-by-Step Instructions","text":"<p>1. Open the terminal/cmd:</p> WindowsmacOSLinux <p>Press <code>Win + R</code>, type <code>cmd</code>, Enter</p> <p>Search for \"Terminal\" in Spotlight (<code>Cmd + Space</code>)</p> <p>Press <code>Ctrl + Alt + T</code></p> <p>2. Navigate to the folder where you want to save the project:</p> <pre><code># Example: Go to Documents\ncd Documents\n\n# Or create a new folder for your projects\nmkdir mis-proyectos\ncd mis-proyectos\n</code></pre> <p>3. Clone YOUR fork (replace YOUR_USERNAME):</p> <pre><code>git clone https://github.com/YOUR_USERNAME/ejercicios-bigdata.git\n</code></pre> <p>IMPORTANT</p> <p>Make sure to use YOUR username, not \"TodoEconometria\"</p> <p>4. Enter the folder:</p> <pre><code>cd ejercicios-bigdata\n</code></pre> <p>5. Connect to the professor's original repo:</p> <p>This allows you to receive updates when the professor adds new exercises:</p> <pre><code>git remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n</code></pre> <p>6. Verify everything is set up correctly:</p> <pre><code>git remote -v\n</code></pre> <p>You should see something like this:</p> <pre><code>origin    https://github.com/YOUR_USERNAME/ejercicios-bigdata.git (fetch)\norigin    https://github.com/YOUR_USERNAME/ejercicios-bigdata.git (push)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (fetch)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (push)\n</code></pre> <p> Done! You now have all the code on your computer.</p>"},{"location":"en/git-github/fork-clone/#understanding-origin-and-upstream","title":"Understanding origin and upstream","text":"<p>origin</p> <p>origin = Your fork on GitHub (where you push your changes)</p> <p>upstream</p> <p>upstream = The professor's original repository (where you download updates from)</p> <pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph Upstream[\"\u2b06\ufe0f UPSTREAM (Professor)\"]\n        direction TB\n        UP[\"TodoEconometria/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\u2713 Original repo&lt;br/&gt;\u2713 Read-only for you&lt;br/&gt;\u2713 You download updates from here\"]\n    end\n\n    subgraph Origin[\"\ud83c\udf10 ORIGIN (Your Fork on GitHub)\"]\n        direction TB\n        OR[\"YOUR_USERNAME/ejercicios-bigdata&lt;br/&gt;&lt;br/&gt;\u2713 Your copy on GitHub&lt;br/&gt;\u2713 Read and write&lt;br/&gt;\u2713 You push your changes here\"]\n    end\n\n    subgraph Local[\"\ud83d\udcbb LOCAL (Your PC)\"]\n        direction TB\n        LOC[\"ejercicios-bigdata/&lt;br/&gt;&lt;br/&gt;\u2713 Folder on your computer&lt;br/&gt;\u2713 You work here&lt;br/&gt;\u2713 You make local commits\"]\n    end\n\n    Upstream --&gt;|\"\ud83c\udf74 Fork\"| Origin\n    Origin --&gt;|\"\ud83d\udce5 Clone&lt;br/&gt;(git clone)\"| Local\n    Local --&gt;|\"\ud83d\udce4 Push&lt;br/&gt;(git push origin)\"| Origin\n    Upstream --&gt;|\"\ud83d\udd04 Fetch&lt;br/&gt;(git fetch upstream)\"| Local\n\n    style Upstream fill:#e1f5ff,stroke:#0277bd,stroke-width:3px\n    style Origin fill:#fff9c4,stroke:#f57f17,stroke-width:3px\n    style Local fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style UP fill:#bbdefb,stroke:#1976d2,stroke-width:2px\n    style OR fill:#fff59d,stroke:#f9a825,stroke-width:2px\n    style LOC fill:#c8e6c9,stroke:#43a047,stroke-width:2px</code></pre>"},{"location":"en/git-github/fork-clone/#complete-workflow","title":"Complete Workflow","text":"<pre><code>graph TD\n    A[Professor's Repo&lt;br/&gt;upstream] --&gt;|1. Fork| B[Your Fork&lt;br/&gt;origin]\n    B --&gt;|2. Clone| C[Your PC&lt;br/&gt;local]\n    C --&gt;|3. Work| D[Edit code]\n    D --&gt;|4. Commit| E[Save changes]\n    E --&gt;|5. Push| B\n    B --&gt;|6. Evaluation| F[System evaluates&lt;br/&gt;PROMPTS.md]\n    A --&gt;|7. New exercises| C\n\n    style A fill:#e1f5ff,stroke:#0277bd\n    style B fill:#fff9c4,stroke:#f57f17\n    style C fill:#e8f5e9,stroke:#388e3c\n    style F fill:#f3e5f5,stroke:#7b1fa2</code></pre>"},{"location":"en/git-github/fork-clone/#basic-commands","title":"Basic Commands","text":""},{"location":"en/git-github/fork-clone/#download-changes-from-the-professor","title":"Download changes from the professor","text":"<pre><code># Download changes\ngit fetch upstream\n\n# Apply changes to your main branch\ngit checkout main\ngit merge upstream/main\n\n# Push to your fork\ngit push origin main\n</code></pre>"},{"location":"en/git-github/fork-clone/#upload-your-changes","title":"Upload your changes","text":"<pre><code># See what you changed\ngit status\n\n# Add files\ngit add archivo.py\n\n# Save with a message\ngit commit -m \"Description of the change\"\n\n# Push to your fork\ngit push origin your-branch-name\n</code></pre>"},{"location":"en/git-github/fork-clone/#common-problems","title":"Common Problems","text":"Error: Permission denied (publickey) <p>Cause: You don't have SSH keys configured.</p> <p>Solution: Use HTTPS instead of SSH:</p> <pre><code># Use this URL (HTTPS):\ngit clone https://github.com/YOUR_USERNAME/ejercicios-bigdata.git\n\n# DO NOT use this (SSH):\ngit clone git@github.com:YOUR_USERNAME/ejercicios-bigdata.git\n</code></pre> Error: fatal: not a git repository <p>Cause: You are not in the project folder.</p> <p>Solution:</p> <pre><code># Navigate to the correct folder\ncd path/to/ejercicios-bigdata\n\n# Verify you are in the correct folder\nls -la  # You should see a .git/ folder\n</code></pre> I cloned the professor's repo instead of my fork <p>Cause: You used the professor's URL.</p> <p>Solution:</p> <ol> <li>Delete the cloned folder</li> <li>Fork first on GitHub</li> <li>Clone YOUR fork, not the professor's</li> </ol> <pre><code># \u274c WRONG\ngit clone https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# \u2705 CORRECT\ngit clone https://github.com/YOUR_USERNAME/ejercicios-bigdata.git\n</code></pre>"},{"location":"en/git-github/fork-clone/#next-steps","title":"Next Steps","text":"<p>Now that you have the repository cloned:</p> <ul> <li>Your First Exercise - Start working</li> <li>Sync Fork - Keep your fork up to date</li> <li>Submission Guide - How to submit exercises</li> </ul>"},{"location":"en/git-github/sincronizar-fork/","title":"Sync Your Fork","text":"<p>IMPORTANT</p> <p>Your fork does NOT update automatically. You must sync it manually to get the new exercises the professor adds.</p> <p>Evaluation System</p> <p>Pull Requests are NO longer used. The system evaluates your <code>PROMPTS.md</code> directly in your fork. See the Submission Guide for more details.</p>"},{"location":"en/git-github/sincronizar-fork/#the-problem","title":"The Problem","text":"<p>When you fork, you get a copy at that moment. During the course I will add new exercises, but your fork does NOT update on its own.</p> <p><pre><code>%%{init: {'theme':'base'}}%%\nflowchart TB\n    subgraph S1[\"WEEK 1 - You Forked\"]\n        direction LR\n        Prof1[\"Professor's Repo&lt;br/&gt;[01] [02]\"]\n        Fork1[\"Your Fork&lt;br/&gt;[01] [02]\"]\n        Prof1 -.-&gt;|Fork| Fork1\n    end\n\n    subgraph S3[\"WEEK 3 - Professor added exercises\"]\n        direction LR\n        Prof3[\"Professor's Repo&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        Fork3[\"Your Fork&lt;br/&gt;[01] [02]&lt;br/&gt;You are missing [03] [04] [05]\"]\n    end\n\n    S1 --&gt; S3\n\n    style S1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style S3 fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style Prof1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style Fork1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n    style Prof3 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style Fork3 fill:#ffcdd2,stroke:#d32f2f,stroke-width:3px</code></pre> https://github.com/YOUR_USERNAME/ejercicios-bigdata <pre><code>**Step 2:** Look for the sync banner\n\nWhen there are new changes, you will see a banner like this:\n\n!!! example \"Banner on GitHub\"\n    ```\n    \u26a0\ufe0f This branch is 15 commits behind TodoEconometria:main\n\n    [Sync fork \u25bc]  \u2190 CLICK HERE\n    ```\n\n**Step 3:** Click on \"Sync fork\" \u2192 \"Update branch\"\n\n!!! example \"Sync options\"\n    **Sync fork**\n\n    This will update your branch with the latest changes from TodoEconometria:main\n\n    **[Update branch]** \u2190 CLICK HERE\n    [Discard commits]\n\n**Step 4:** Update your local copy\n\nNow your fork on GitHub is up to date, but your PC is not. Run:\n\n```bash\ngit checkout main\ngit pull origin main\n</code></pre></p> <p>Step 5: Bring changes to your working branch</p> <pre><code># Go to your exercise branch\ngit checkout your-lastname-exercise\n\n# Bring changes from main\ngit merge main\n\n# Push to GitHub\ngit push origin your-lastname-exercise\n</code></pre> <p> Done! You have the new exercises without losing your work.</p>"},{"location":"en/git-github/sincronizar-fork/#visual-diagram-of-the-flow","title":"Visual Diagram of the Flow","text":""},{"location":"en/git-github/sincronizar-fork/#how-synchronization-works","title":"How synchronization works","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\nsequenceDiagram\n    actor You\n    participant Local as \ud83d\udcbb Your PC&lt;br/&gt;(main: 01, 02)\n    participant YourBranch as \ud83d\udcbb Your PC&lt;br/&gt;(your-branch: 01, 02 + YOUR CODE)\n    participant Origin as \ud83c\udf10 Your Fork GitHub&lt;br/&gt;(01, 02)\n    participant Upstream as \ud83d\udc68\u200d\ud83c\udfeb Professor's Repo&lt;br/&gt;(01, 02, 03, 04, 05)\n\n    Note over You,Upstream: INITIAL STATE - Your fork is outdated\n\n    rect rgb(255, 243, 224)\n    Note over You,Upstream: STEP 1: Switch to main branch\n    You-&gt;&gt;Local: git checkout main\n    activate Local\n    Note over Local: Now you are on main\n    end\n\n    rect rgb(232, 245, 233)\n    Note over You,Upstream: STEP 2: Download and merge professor's changes\n    You-&gt;&gt;Upstream: git fetch upstream\n    Upstream--&gt;&gt;Local: Downloads [03, 04, 05]\n    You-&gt;&gt;Local: git merge upstream/main\n    Note over Local: main: 01, 02, 03, 04, 05 \u2705\n    deactivate Local\n    end\n\n    rect rgb(237, 231, 246)\n    Note over You,Upstream: STEP 3: Switch to your working branch\n    You-&gt;&gt;YourBranch: git checkout your-branch\n    activate YourBranch\n    Note over YourBranch: Now you are on your-branch\n    end\n\n    rect rgb(255, 249, 196)\n    Note over You,Upstream: STEP 4: Bring changes to your branch\n    You-&gt;&gt;YourBranch: git merge main\n    Note over YourBranch: your-branch: 01-05 + YOUR CODE \u2705\n    deactivate YourBranch\n    end\n\n    rect rgb(225, 245, 254)\n    Note over You,Upstream: STEP 5: Push everything to GitHub\n    You-&gt;&gt;Origin: git push origin your-branch\n    Note over Origin: your-branch: 01-05 + YOUR CODE \u2705\n    end\n\n    rect rgb(200, 230, 201)\n    Note over You,Upstream: \u2705 RESULT - You have everything without losing your work\n    end</code></pre>"},{"location":"en/git-github/sincronizar-fork/#simplified-view-of-the-process","title":"Simplified View of the Process","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#e1f5ff','primaryTextColor':'#000','primaryBorderColor':'#0277bd','secondaryColor':'#fff9c4','tertiaryColor':'#e8f5e9','noteBkgColor':'#fff3e0','noteTextColor':'#000'}}}%%\nflowchart TB\n    subgraph Before[\"\u274c BEFORE - Outdated\"]\n        direction LR\n        A1[\"\ud83d\udc68\u200d\ud83c\udfeb Professor's Repo&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Exercises:&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        A2[\"\ud83c\udf10 Your Fork&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Your exercises:&lt;br/&gt;[01] [02]&lt;br/&gt;&lt;br/&gt;\u26a0\ufe0f You are missing 3 exercises\"]\n        A3[\"\ud83d\udcbb Your PC&lt;br/&gt;&lt;br/&gt;\ud83d\udcc2 your-branch:&lt;br/&gt;[01] [02] + YOUR CODE\"]\n    end\n\n    subgraph Process[\"\ud83d\udd04 SYNC PROCESS\"]\n        direction TB\n        P1[\"\u2460 git fetch upstream&lt;br/&gt;Download professor's changes\"]\n        P2[\"\u2461 git merge upstream/main&lt;br/&gt;Apply to your local main\"]\n        P3[\"\u2462 git merge main&lt;br/&gt;Bring to your working branch\"]\n        P4[\"\u2463 git push origin your-branch&lt;br/&gt;Push everything to GitHub\"]\n\n        P1 --&gt; P2 --&gt; P3 --&gt; P4\n    end\n\n    subgraph After[\"\u2705 AFTER - Up to date\"]\n        direction LR\n        D1[\"\ud83d\udc68\u200d\ud83c\udfeb Professor's Repo&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Exercises:&lt;br/&gt;[01] [02] [03] [04] [05]\"]\n        D2[\"\ud83c\udf10 Your Fork&lt;br/&gt;&lt;br/&gt;\ud83d\udce6 Your exercises:&lt;br/&gt;[01-05] + YOUR CODE&lt;br/&gt;&lt;br/&gt;\u2705 Fully up to date\"]\n        D3[\"\ud83d\udcbb Your PC&lt;br/&gt;&lt;br/&gt;\ud83d\udcc2 your-branch:&lt;br/&gt;[01-05] + YOUR CODE&lt;br/&gt;&lt;br/&gt;\ud83c\udfaf Ready to work\"]\n    end\n\n    Before --&gt; Process --&gt; After\n\n    style A1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style A2 fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style A3 fill:#fff3e0,stroke:#ef6c00,stroke-width:2px\n\n    style P1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style P2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P3 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style P4 fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n\n    style D1 fill:#e1f5ff,stroke:#0277bd,stroke-width:2px\n    style D2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style D3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px</code></pre>"},{"location":"en/git-github/sincronizar-fork/#detailed-method-terminal","title":"Detailed Method (Terminal)","text":""},{"location":"en/git-github/sincronizar-fork/#situation","title":"Situation","text":"<p>You are working on a branch (example: <code>garcia-ejercicio-1.1</code>) and the professor added new exercises.</p> <p>Goal: Bring the new exercises WITHOUT losing your work.</p>"},{"location":"en/git-github/sincronizar-fork/#step-1-save-your-current-work","title":"STEP 1: Save your current work","text":"<pre><code># See which files you changed\ngit status\n\n# Save your changes\ngit add entregas/01_bases_de_datos/your_lastname_firstname/\ngit commit -m \"Save my progress\"\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#step-2-go-to-your-main-branch","title":"STEP 2: Go to your main branch","text":"<pre><code>git checkout main\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#step-3-download-the-professors-changes","title":"STEP 3: Download the professor's changes","text":"<pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre> <p>Now your local <code>main</code> has the new exercises </p>"},{"location":"en/git-github/sincronizar-fork/#step-4-go-back-to-your-working-branch","title":"STEP 4: Go back to your working branch","text":"<pre><code>git checkout garcia-ejercicio-1.1\n</code></pre> <p>(Replace <code>garcia-ejercicio-1.1</code> with YOUR branch name)</p>"},{"location":"en/git-github/sincronizar-fork/#step-5-bring-the-new-exercises-to-your-branch","title":"STEP 5: Bring the new exercises to your branch","text":"<pre><code>git merge main\n</code></pre> <p>What does this do?</p> <p>It combines the professor's new exercises with your work. It does NOT delete anything of yours.</p>"},{"location":"en/git-github/sincronizar-fork/#step-6-push-to-github","title":"STEP 6: Push to GitHub","text":"<pre><code>git push origin garcia-ejercicio-1.1\n</code></pre> <p> Done! You have the new exercises AND your work intact.</p>"},{"location":"en/git-github/sincronizar-fork/#what-happens-when-the-professor-adds-exercises","title":"What Happens When the Professor Adds Exercises?","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'git0':'#e1f5ff','git1':'#fff9c4','git2':'#ffebee'}}}%%\ngitGraph\n    commit id: \"01: Intro SQLite\" tag: \"Week 1\"\n    commit id: \"02: Data Cleaning\"\n    branch your-fork\n    checkout your-fork\n    commit id: \"\u2705 You Forked\" type: HIGHLIGHT\n\n    checkout main\n    commit id: \"03: Dask &amp; Parquet\" tag: \"Week 3\"\n    commit id: \"04: PySpark\"\n    commit id: \"05: Dashboard\"\n\n    checkout your-fork\n    commit id: \"\u274c Outdated\" type: REVERSE\n    commit id: \"\u26a0\ufe0f Missing 03, 04, 05\" type: REVERSE</code></pre> <p>The fork does NOT update automatically</p> <p>When the professor adds new exercises to the original repository, your fork on GitHub does NOT receive those changes automatically. You must sync it manually following the steps in this guide.</p>"},{"location":"en/git-github/sincronizar-fork/#golden-rule-to-avoid-problems","title":"Golden Rule to Avoid Problems","text":"<pre><code>%%{init: {'theme':'base'}}%%\nflowchart LR\n    subgraph Good[\"\u2705 GOOD - Edit only here\"]\n        direction TB\n        B1[\"\ud83d\udcc1 entregas/01_bases_de_datos/your_lastname_firstname/&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 1.1_sqlite/&lt;br/&gt;\u2502   \u251c\u2500\u2500 ANALISIS_DATOS.md&lt;br/&gt;\u2502   \u251c\u2500\u2500 resumen_eda.md&lt;br/&gt;\u2502   \u2514\u2500\u2500 REFLEXION.md&lt;br/&gt;&lt;br/&gt;\u2705 Make your changes here\"]\n    end\n\n    subgraph Bad[\"\u274c BAD - DO NOT touch this\"]\n        direction TB\n        M1[\"\ud83d\udcc1 ejercicios/01_bases_de_datos/&lt;br/&gt;&lt;br/&gt;\u251c\u2500\u2500 README.md \u2190 DO NOT TOUCH&lt;br/&gt;\u251c\u2500\u2500 eda_exploratorio.py \u2190 Only run&lt;br/&gt;&lt;br/&gt;\ud83d\udd12 Professor's files\"]\n    end\n\n    Good -.-&gt;|No conflicts| OK[\"\ud83c\udf89 Perfect&lt;br/&gt;sync\"]\n    Bad -.-&gt;|Causes conflicts| NOK[\"\u26a0\ufe0f Problems&lt;br/&gt;when syncing\"]\n\n    style Good fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style Bad fill:#ffcdd2,stroke:#c62828,stroke-width:3px\n    style B1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style M1 fill:#ffebee,stroke:#d32f2f,stroke-width:2px\n    style OK fill:#a5d6a7,stroke:#43a047,stroke-width:2px\n    style NOK fill:#ef9a9a,stroke:#e53935,stroke-width:2px</code></pre> <p>Golden Rule</p> <p>If you only edit files in <code>entregas/YOUR_FOLDER/</code>, you will NEVER have conflicts.</p> <p>The professor updates <code>ejercicios/</code>, you work in <code>entregas/</code>. Zero problems.</p>"},{"location":"en/git-github/sincronizar-fork/#what-do-i-do-if-git-says-conflict","title":"What do I do if Git says \"CONFLICT\"?","text":""},{"location":"en/git-github/sincronizar-fork/#step-1-git-will-tell-you-which-file-has-the-conflict","title":"Step 1: Git will tell you which file has the conflict","text":"<pre><code>Auto-merging ejercicio_01.py\nCONFLICT (content): Merge conflict in ejercicio_01.py\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#step-2-open-the-file","title":"Step 2: Open the file","text":"<p>You will see something like this:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nyour code here\n=======\nprofessor's code\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#step-3-decide-what-to-keep","title":"Step 3: Decide what to keep","text":"<ul> <li>If it's a professor's file you should NOT have touched \u2192 Keep the professor's version</li> <li>If it's YOUR submission file \u2192 Keep your version</li> </ul>"},{"location":"en/git-github/sincronizar-fork/#step-4-delete-the-markers","title":"Step 4: Delete the markers","text":"<p>Remove these lines:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#step-5-finish-the-merge","title":"Step 5: Finish the merge","text":"<pre><code>git add filename\ngit commit -m \"Resolve conflict\"\ngit push origin your-branch\n</code></pre> <p>Tip</p> <p>If you only work in <code>entregas/YOUR_FOLDER/</code>, this will never happen to you.</p>"},{"location":"en/git-github/sincronizar-fork/#ultra-quick-summary","title":"Ultra-Quick Summary","text":"<pre><code># 1. Save your work\ngit add .\ngit commit -m \"Save progress\"\n\n# 2. Update main\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\n\n# 3. Go back to your branch and bring changes\ngit checkout your-branch\ngit merge main\n\n# 4. Push\ngit push origin your-branch\n</code></pre> <p>Frequency: Do this every Monday before class.</p>"},{"location":"en/git-github/sincronizar-fork/#sync-best-practices","title":"Sync Best Practices","text":""},{"location":"en/git-github/sincronizar-fork/#1-sync-before-starting-a-new-exercise","title":"1. Sync BEFORE starting a new exercise","text":"<pre><code># \u2705 GOOD - Sync first\ngit fetch upstream &amp;&amp; git merge upstream/main\n# Now start working\n\n# \u274c BAD - Working with old code\n# You start without updating, then you get conflicts\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#2-commit-your-work-before-syncing","title":"2. Commit your work BEFORE syncing","text":"<pre><code># \u2705 GOOD - Save your work first\ngit add .\ngit commit -m \"Progress on exercise 03\"\ngit fetch upstream &amp;&amp; git merge upstream/main\n\n# \u274c BAD - Syncing with unsaved changes\n# You might lose your work\n</code></pre>"},{"location":"en/git-github/sincronizar-fork/#3-recommended-frequency","title":"3. Recommended frequency","text":"<pre><code>%%{init: {'theme':'base'}}%%\ngantt\n    title \ud83d\udcc5 Weekly Sync Schedule\n    dateFormat YYYY-MM-DD\n    section Monday\n    Sync before class :milestone, m1, 2024-01-01, 0d\n    git fetch upstream :active, 2024-01-01, 1h\n    git merge upstream/main :active, 2024-01-01, 30m\n    section Tuesday to Thursday\n    Work on exercises :2024-01-02, 3d\n    Local commits :2024-01-02, 3d\n    section Friday\n    Push your progress :milestone, m2, 2024-01-05, 0d\n    git push origin your-branch :crit, 2024-01-05, 1h\n    section Sunday\n    Check for updates (optional) :done, 2024-01-07, 30m</code></pre> <p>Frequency recommendation</p> <ul> <li>Monday: Sync before class to get the latest exercises</li> <li>During the week: Work normally, make frequent commits</li> <li>Friday: Push your progress to GitHub</li> <li>Sunday (optional): Check if there are new updates</li> </ul>"},{"location":"en/git-github/sincronizar-fork/#check-sync-status","title":"Check Sync Status","text":""},{"location":"en/git-github/sincronizar-fork/#useful-command-to-know-if-you-are-outdated","title":"Useful command to know if you are outdated","text":"<pre><code># See differences between your fork and the professor's repo\ngit fetch upstream\ngit log HEAD..upstream/main --oneline\n</code></pre> <p>If you see new commits:</p> <pre><code>a1b2c3d Add exercise 06\nd4e5f6g Fix typo in exercise 05\ng7h8i9j Add data for exercise 06\n</code></pre> <p>It means you have 3 commits (exercises/updates) that you don't have.</p> <p>If you see nothing:</p> <pre><code>(empty)\n</code></pre> <p>It means you are up to date. </p>"},{"location":"en/git-github/sincronizar-fork/#next-steps","title":"Next Steps","text":"<p>Now that you know how to sync your fork:</p> <ul> <li>Submission Guide - How to submit exercises</li> <li>Useful Commands - Git Cheatsheet</li> <li>Fork and Clone - If you need to review the basic concepts</li> </ul>"},{"location":"en/guia-inicio/","title":"Getting Started","text":"<p>Welcome to the getting started guide for the Big Data with Python course. This section will walk you through everything you need to begin working on the exercises.</p>"},{"location":"en/guia-inicio/#what-youll-find-here","title":"What you'll find here","text":""},{"location":"en/guia-inicio/#tools-installation","title":"Tools Installation","text":"<p>Learn how to install all the tools required for the course:</p> <ul> <li>Python 3.11+</li> <li>Git</li> <li>PyCharm (recommended IDE)</li> <li>Project dependencies</li> </ul>"},{"location":"en/guia-inicio/#your-first-exercise","title":"Your First Exercise","text":"<p>A step-by-step guide to complete your first exercise:</p> <ul> <li>How to navigate the repository</li> <li>How to run the exercises</li> <li>How to submit your work</li> <li>Best practices</li> </ul>"},{"location":"en/guia-inicio/#course-roadmap","title":"Course Roadmap","text":"<p>Overview of all exercises and technologies:</p> <ul> <li>Learning levels</li> <li>Estimated time per exercise</li> <li>Technologies you'll master</li> <li>Recommended study plan</li> </ul>"},{"location":"en/guia-inicio/#where-to-start","title":"Where to start?","text":"<p>First time with Git and Python?</p> <p>Start with Tools Installation where we explain step by step how to install everything you need.</p> <p>Already have everything installed?</p> <p>Go straight to Your First Exercise to start working.</p> <p>Experienced developer?</p> <p>Check the Course Roadmap to see all exercises and choose where to begin.</p>"},{"location":"en/guia-inicio/#help-and-support","title":"Help and Support","text":"<p>Have questions?</p> <ul> <li>In-person course students: Ask during class sessions</li> <li>Self-learners: Create an Issue on GitHub with your question</li> <li>Businesses: Contact directly via email</li> </ul>"},{"location":"en/guia-inicio/instalacion/","title":"Tools Installation","text":"<p>This guide will walk you through the installation of all the tools needed for the course, step by step.</p>"},{"location":"en/guia-inicio/instalacion/#system-requirements","title":"System Requirements","text":"<p>Minimum Requirements</p> <ul> <li>RAM: 8GB</li> <li>Disk space: 20GB</li> <li>Processor: i5 or equivalent</li> <li>Operating System: Windows 10+, macOS 10.14+, or Linux (Ubuntu 20.04+)</li> </ul> <p>Recommended Requirements</p> <ul> <li>RAM: 16GB</li> <li>Disk space: 50GB SSD</li> <li>Processor: i7 or equivalent</li> <li>Operating System: Latest available operating system</li> </ul>"},{"location":"en/guia-inicio/instalacion/#step-1-install-git","title":"Step 1: Install Git","text":"<p>Git is the version control system we will use to manage our code.</p> WindowsmacOSLinux"},{"location":"en/guia-inicio/instalacion/#option-a-with-winget-recommended","title":"Option A: With winget (Recommended)","text":"<pre><code># Open PowerShell or CMD as Administrator\nwinget install Git.Git\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#option-b-graphical-installer","title":"Option B: Graphical installer","text":"<ol> <li>Download from git-scm.com</li> <li>Run the installer</li> <li>Use the default settings (Next, Next, Next...)</li> </ol>"},{"location":"en/guia-inicio/instalacion/#verify-installation","title":"Verify installation","text":"<pre><code>git --version\n# Should show: git version 2.x.x\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#option-a-with-homebrew-recommended","title":"Option A: With Homebrew (Recommended)","text":"<pre><code># If you don't have Homebrew, install it first:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Git\nbrew install git\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#option-b-with-xcode-command-line-tools","title":"Option B: With Xcode Command Line Tools","text":"<pre><code>xcode-select --install\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#verify-installation_1","title":"Verify installation","text":"<pre><code>git --version\n# Should show: git version 2.x.x\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#verify-installation_2","title":"Verify installation","text":"<pre><code>git --version\n# Should show: git version 2.x.x\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#step-2-configure-git","title":"Step 2: Configure Git","text":"<p>Once Git is installed, we need to configure it with your information:</p> <pre><code># Set your name (use your real name)\ngit config --global user.name \"Your Full Name\"\n\n# Set your email (use the same email as GitHub)\ngit config --global user.email \"your@email.com\"\n\n# Verify configuration\ngit config --list\n</code></pre> <p>Tip</p> <p>The name and email you set will appear on all your commits, so use your real name and the email you will use on GitHub.</p>"},{"location":"en/guia-inicio/instalacion/#step-3-create-a-github-account","title":"Step 3: Create a GitHub Account","text":"<p>GitHub is the platform where we will host our code.</p> <ol> <li>Go to github.com</li> <li>Click on \"Sign Up\"</li> <li>Fill in the form:<ul> <li>Username: Choose a professional name (e.g.: <code>juan-garcia</code>, not <code>kitty123</code>)</li> <li>Email: Use the same one you configured in Git</li> <li>Password: Use a strong password</li> </ul> </li> <li>Verify your email</li> <li>Complete your profile (photo, bio optional)</li> </ol> <p>Important</p> <p>Use the same email you configured in Git. This links your commits with your GitHub account.</p>"},{"location":"en/guia-inicio/instalacion/#step-4-install-python","title":"Step 4: Install Python","text":"<p>We need Python 3.11 or higher.</p> WindowsmacOSLinux <p>Note for macOS/Linux</p> <p>On macOS and Linux, use <code>python3</code> and <code>pip3</code> instead of <code>python</code> and <code>pip</code>.</p>"},{"location":"en/guia-inicio/instalacion/#option-a-with-winget-recommended_1","title":"Option A: With winget (Recommended)","text":"<pre><code>winget install Python.Python.3.11\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#option-b-graphical-installer_1","title":"Option B: Graphical installer","text":"<ol> <li>Download from python.org</li> <li>IMPORTANT: Check \"Add Python to PATH\"</li> <li>Run the installer</li> <li>Click on \"Install Now\"</li> </ol>"},{"location":"en/guia-inicio/instalacion/#verify-installation_3","title":"Verify installation","text":"<pre><code>python --version\n# Should show: Python 3.11.x\n\npip --version\n# Should show: pip 23.x.x\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#with-homebrew-recommended","title":"With Homebrew (Recommended)","text":"<pre><code>brew install python@3.11\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#verify-installation_4","title":"Verify installation","text":"<pre><code>python3 --version\n# Should show: Python 3.11.x\n\npip3 --version\n# Should show: pip 23.x.x\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#verify-installation_5","title":"Verify installation","text":"<pre><code>python3 --version\n# Should show: Python 3.11.x\n\npip3 --version\n# Should show: pip 23.x.x\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#step-5-install-pycharm-optional-but-recommended","title":"Step 5: Install PyCharm (Optional but Recommended)","text":"<p>PyCharm is the IDE we recommend for the course.</p>"},{"location":"en/guia-inicio/instalacion/#pycharm-community-edition-free","title":"PyCharm Community Edition (Free)","text":"WindowsmacOSLinux <ol> <li>Download from jetbrains.com/pycharm</li> <li>Choose \"Community Edition\" (free)</li> <li>Run the installer</li> <li>Follow the installer steps</li> </ol> <pre><code>brew install --cask pycharm-ce\n</code></pre> <p>Or download from jetbrains.com/pycharm</p> <p>Download from jetbrains.com/pycharm</p> <p>Or use Snap:</p> <pre><code>sudo snap install pycharm-community --classic\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#alternatives-to-pycharm","title":"Alternatives to PyCharm","text":"<p>If you prefer another editor:</p> <ul> <li>Visual Studio Code: Lightweight and extensible (code.visualstudio.com)</li> <li>Jupyter Lab: For working with notebooks (jupyter.org)</li> <li>Sublime Text: Advanced text editor (sublimetext.com)</li> </ul>"},{"location":"en/guia-inicio/instalacion/#step-6-clone-the-repository","title":"Step 6: Clone the Repository","text":"<p>Now that you have everything installed, clone your fork of the repository:</p> <p>Important</p> <p>First you must Fork the repository on GitHub. Go to the Fork and Clone guide for more details.</p> <pre><code># Navigate to the folder where you want to save the project\ncd Documents  # or whichever folder you prefer\n\n# Clone YOUR fork (replace YOUR_USERNAME)\ngit clone https://github.com/YOUR_USERNAME/ejercicios-bigdata.git\n\n# Enter the folder\ncd ejercicios-bigdata\n\n# Connect to the original repository (upstream)\ngit remote add upstream https://github.com/TodoEconometria/ejercicios-bigdata.git\n\n# Verify that everything is set up correctly\ngit remote -v\n</code></pre> <p>You should see something like this:</p> <pre><code>origin    https://github.com/YOUR_USERNAME/ejercicios-bigdata.git (fetch)\norigin    https://github.com/YOUR_USERNAME/ejercicios-bigdata.git (push)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (fetch)\nupstream  https://github.com/TodoEconometria/ejercicios-bigdata.git (push)\n</code></pre>"},{"location":"en/guia-inicio/instalacion/#step-7-create-a-virtual-environment","title":"Step 7: Create a Virtual Environment","text":"<p>It is a good practice to use virtual environments for each project:</p> <pre><code># Make sure you are in the project folder\ncd ejercicios-bigdata\n\n# Create virtual environment\npython -m venv .venv\n\n# Activate virtual environment\n# Windows:\n.venv\\Scripts\\activate\n\n# macOS/Linux:\nsource .venv/bin/activate\n\n# You should see (.venv) at the beginning of your terminal\n</code></pre> <p>Tip</p> <p>Always activate the virtual environment before working on the project.</p>"},{"location":"en/guia-inicio/instalacion/#step-8-install-dependencies","title":"Step 8: Install Dependencies","text":"<p>With the virtual environment activated, install the project dependencies:</p> <pre><code># Upgrade pip\npip install --upgrade pip\n\n# Install project dependencies\npip install -r requirements.txt\n\n# Verify that everything was installed correctly\npython -c \"import pandas, dask, sqlite3; print('All OK!')\"\n</code></pre> <p>If you see \"All OK!\", you are ready to get started.</p>"},{"location":"en/guia-inicio/instalacion/#final-verification","title":"Final Verification","text":"<p>Run these commands to verify that everything is installed correctly:</p> <pre><code># Git\ngit --version\n\n# Python\npython --version\n\n# Pip\npip --version\n\n# Verify libraries\npython -c \"import pandas; print(f'Pandas {pandas.__version__}')\"\npython -c \"import dask; print(f'Dask {dask.__version__}')\"\n</code></pre> <p>Installation Complete</p> <p>If all the commands above worked, you are ready to start with Your First Exercise!</p>"},{"location":"en/guia-inicio/instalacion/#common-issues","title":"Common Issues","text":"Error: 'python' is not recognized as a command <p>Windows: Python is not in the PATH.</p> <p>Solution:</p> <ol> <li>Reinstall Python</li> <li>Check the \"Add Python to PATH\" option</li> <li>Restart the terminal</li> </ol> <p>macOS/Linux: Use <code>python3</code> instead of <code>python</code></p> Error: Permission denied when installing with pip <p>Cause: Attempting to install packages globally without permissions.</p> <p>Solution: Use a virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\npip install -r requirements.txt\n</code></pre> Git says 'fatal: not a git repository' <p>Cause: You are not in the project folder.</p> <p>Solution:</p> <pre><code># Navigate to the correct folder\ncd path/to/ejercicios-bigdata\n\n# Verify you are in the correct folder\nls -la  # You should see a .git/ folder\n</code></pre> PyCharm does not detect the Python interpreter <p>Solution:</p> <ol> <li>Open PyCharm</li> <li>File \u2192 Settings (Windows/Linux) or PyCharm \u2192 Preferences (macOS)</li> <li>Project \u2192 Python Interpreter</li> <li>Click on the gear icon \u2192 Add</li> <li>Select \"Existing environment\"</li> <li>Browse to <code>.venv/Scripts/python.exe</code> (Windows) or <code>.venv/bin/python</code> (macOS/Linux)</li> </ol>"},{"location":"en/guia-inicio/instalacion/#next-steps","title":"Next Steps","text":"<p>Now that you have everything installed, continue with:</p> <ul> <li>Your First Exercise - Learn the basic workflow</li> <li>Fork and Clone - Understand how to work with Git and GitHub</li> <li>Course Roadmap - See all available exercises</li> </ul>"},{"location":"en/guia-inicio/primer-ejercicio/","title":"Your First Exercise","text":"<p>This guide will take you step by step through the process of completing and submitting your first exercise.</p>"},{"location":"en/guia-inicio/primer-ejercicio/#general-workflow","title":"General Workflow","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Clone to your PC]\n    B --&gt; C[Work on Exercise]\n    C --&gt; D[Document PROMPTS.md]\n    D --&gt; E[Commit Changes]\n    E --&gt; F[Push to your Fork]\n    F --&gt; G[Automatic Evaluation]</code></pre> <p>PROMPTS-Based Evaluation System</p> <p>Pull Requests are NOT used. The system evaluates your <code>PROMPTS.md</code> file directly in your fork. You only need to do <code>git push</code>.</p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-1-open-the-project-in-pycharm","title":"Step 1: Open the Project in PyCharm","text":"<ol> <li>Open PyCharm</li> <li>File \u2192 Open...</li> <li>Select the <code>ejercicios-bigdata/</code> folder</li> <li>Click \"OK\"</li> </ol> <p>First time in PyCharm?</p> <p>PyCharm will ask if you trust the project. Click \"Trust Project\".</p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-2-configure-the-python-interpreter","title":"Step 2: Configure the Python Interpreter","text":"<p>PyCharm should automatically detect the virtual environment. If it doesn't:</p> <ol> <li>File \u2192 Settings (Windows/Linux) or PyCharm \u2192 Preferences (macOS)</li> <li>Project: ejercicios-bigdata \u2192 Python Interpreter</li> <li>Click the gear icon \u2192 Add</li> <li>Select \"Existing environment\"</li> <li>Browse to <code>.venv/Scripts/python.exe</code> (Windows) or <code>.venv/bin/python</code> (macOS/Linux)</li> <li>Click \"OK\"</li> </ol>"},{"location":"en/guia-inicio/primer-ejercicio/#step-3-navigate-to-your-first-exercise","title":"Step 3: Navigate to Your First Exercise","text":"<p>In PyCharm's file explorer:</p> <pre><code>ejercicios-bigdata/\n\u2514\u2500\u2500 ejercicios/\n    \u2514\u2500\u2500 01_cargar_sqlite.py  \u2190 Open this file\n</code></pre> <p>Exercise Structure</p> <p>Each exercise has:</p> <ul> <li>Base code: <code>.py</code> file with instructions</li> <li>Data: <code>datos/</code> folder with datasets</li> <li>README: Detailed explanation of the exercise</li> </ul>"},{"location":"en/guia-inicio/primer-ejercicio/#step-4-read-the-problem-statement","title":"Step 4: Read the Problem Statement","text":"<p>IMPORTANT: Read the ENTIRE file before you start coding.</p> <p>The exercise will have sections like:</p> <pre><code>\"\"\"\nExercise 01: Data Loading with SQLite\n\nOBJECTIVE:\nLearn to load data from CSV into an SQLite database\n\nDATASET:\n- File: datos/muestra_taxi.csv\n- Size: ~10MB\n- Records: ~100,000\n\nTASKS:\n1. Load CSV in chunks into SQLite\n2. Create indexes to optimize queries\n3. Run analysis queries\n4. Export results\n\nESTIMATED TIME: 2-3 hours\n\"\"\"\n</code></pre>"},{"location":"en/guia-inicio/primer-ejercicio/#step-5-create-a-working-branch","title":"Step 5: Create a Working Branch","text":"<p>NEVER work directly on <code>main</code>. Always create a branch:</p> <pre><code># Make sure you are on main and up to date\ngit checkout main\ngit pull origin main\n\n# Create a branch with your last name and exercise number\ngit checkout -b garcia-ejercicio-01\n\n# Verify you are on the correct branch\ngit branch\n# Should show: * garcia-ejercicio-01\n</code></pre> <p>Branch Naming Convention</p> <p>Use the format: <code>your-lastname-exercise-XX</code></p> <p>Examples: - <code>garcia-ejercicio-01</code> - <code>martinez-ejercicio-02</code></p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-6-work-on-the-exercise","title":"Step 6: Work on the Exercise","text":""},{"location":"en/guia-inicio/primer-ejercicio/#edit-the-code","title":"Edit the Code","text":"<p>Open <code>ejercicios/01_cargar_sqlite.py</code> and start working.</p> <p>Code Example</p> <pre><code>import sqlite3\nimport pandas as pd\n\n# Task 1: Load CSV in chunks\ndef cargar_datos_sqlite(csv_path, db_path, chunksize=10000):\n    \"\"\"\n    Loads a large CSV into SQLite in chunks to avoid memory issues\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    # Read CSV in parts\n    chunks = pd.read_csv(csv_path, chunksize=chunksize)\n\n    for i, chunk in enumerate(chunks):\n        chunk.to_sql('trips', conn, if_exists='append', index=False)\n        print(f\"Chunk {i+1} loaded ({len(chunk)} records)\")\n\n    conn.close()\n    print(\"Loading complete!\")\n\n# Execute\nif __name__ == \"__main__\":\n    cargar_datos_sqlite(\n        csv_path='datos/muestra_taxi.csv',\n        db_path='datos/taxi.db'\n    )\n</code></pre>"},{"location":"en/guia-inicio/primer-ejercicio/#test-your-code","title":"Test Your Code","text":"<p>Run your code frequently to verify it works:</p> From PyCharmFrom Terminal <ol> <li>Right-click on the file</li> <li>Run 'ejercicio_01'</li> <li>Or press <code>Shift + F10</code></li> </ol> <pre><code># Make sure the virtual environment is activated\npython ejercicios/01_cargar_sqlite.py\n</code></pre> <p>Debug Frequently</p> <p>Don't write all the code at once. Write a function, test it, and continue.</p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-7-save-your-work-with-git","title":"Step 7: Save Your Work with Git","text":"<p>When you have significant progress (for example, you completed a task):</p> <pre><code># See which files you changed\ngit status\n\n# Add the modified files\ngit add ejercicios/01_cargar_sqlite.py\n\n# Commit with a descriptive message\ngit commit -m \"Implement CSV to SQLite chunk loading\"\n\n# Continue working...\n</code></pre> <p>Good Commit Messages</p> <p>GOOD: - \"Implement CSV to SQLite chunk loading\" - \"Add indexes to optimize queries\" - \"Complete revenue analysis by hour\"</p> <p>BAD: - \"update\" - \"fix\" - \"asdfasdf\"</p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-8-push-to-github","title":"Step 8: Push to GitHub","text":"<p>When you have completed the exercise:</p> <pre><code># Make a final commit\ngit add .\ngit commit -m \"Complete exercise 01: SQLite data loading\"\n\n# Push your branch to GitHub\ngit push origin garcia-ejercicio-01\n</code></pre> <p>First time pushing?</p> <p>Git will ask for authentication. Use your GitHub username and password, or configure SSH keys.</p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-9-verify-your-submission","title":"Step 9: Verify Your Submission","text":"<ol> <li>Go to your fork on GitHub: <code>https://github.com/YOUR_USERNAME/ejercicios-bigdata</code></li> <li>Navigate to your submission folder</li> <li>Verify that all your files are there, especially <code>PROMPTS.md</code></li> </ol> <p>Submission Completed</p> <p>You don't need to do anything else. The system evaluates your <code>PROMPTS.md</code> automatically.</p>"},{"location":"en/guia-inicio/primer-ejercicio/#step-10-the-promptsmd-file","title":"Step 10: The PROMPTS.md File","text":"<p>This is the most important file of your submission.</p> <p>Document your AI prompts as you work:</p> <pre><code># AI Prompts - Exercise 01\n\n## Prompt A: Load data into SQLite\n\n**AI used:** ChatGPT / Claude / etc.\n\n**Exact prompt:**\n&gt; how do i load a large csv into sqlite using python with chunks\n\n---\n\n## Prompt B: Optimize queries\n\n[Same format...]\n\n---\n\n## Final Blueprint\n\n[When finished, ask the AI for a summary of what you built]\n</code></pre> <p>DO NOT clean your prompts</p> <p>Paste your prompts EXACTLY as you wrote them, with errors and all. The system detects if they were \"cleaned\".</p>"},{"location":"en/guia-inicio/primer-ejercicio/#best-practices","title":"Best Practices","text":""},{"location":"en/guia-inicio/primer-ejercicio/#clean-code","title":"Clean Code","text":"<pre><code># \u2705 GOOD - Readable code with comments\ndef calcular_promedio_tarifas(db_path):\n    \"\"\"\n    Calculates the average fare by hour of day\n\n    Args:\n        db_path: Path to the SQLite database\n\n    Returns:\n        DataFrame with average fares by hour\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n\n    query = \"\"\"\n        SELECT\n            strftime('%H', pickup_datetime) as hora,\n            AVG(total_amount) as promedio_tarifa\n        FROM trips\n        GROUP BY hora\n        ORDER BY hora\n    \"\"\"\n\n    resultado = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return resultado\n\n# \u274c BAD - No documentation, confusing names\ndef calc(p):\n    c = sqlite3.connect(p)\n    r = pd.read_sql_query(\"SELECT strftime('%H', pickup_datetime) as h, AVG(total_amount) as t FROM trips GROUP BY h\", c)\n    c.close()\n    return r\n</code></pre>"},{"location":"en/guia-inicio/primer-ejercicio/#atomic-commits","title":"Atomic Commits","text":"<p>Make small and specific commits:</p> <pre><code># \u2705 GOOD - Small and descriptive commits\ngit commit -m \"Add data loading function\"\ngit commit -m \"Implement index creation\"\ngit commit -m \"Add analysis queries\"\n\n# \u274c BAD - One giant commit\ngit commit -m \"Entire exercise\"\n</code></pre>"},{"location":"en/guia-inicio/primer-ejercicio/#test-before-uploading","title":"Test Before Uploading","text":"<pre><code># Always verify it works before pushing\npython ejercicios/01_cargar_sqlite.py\n\n# If it works, then push\ngit push origin garcia-ejercicio-01\n</code></pre>"},{"location":"en/guia-inicio/primer-ejercicio/#exercise-checklist","title":"Exercise Checklist","text":"<p>Before uploading your work (git push), verify:</p> <ul> <li> The code runs without errors</li> <li> All exercise tasks are complete</li> <li> The code is documented (comments, docstrings)</li> <li> Commits have descriptive messages</li> <li> The code follows Python best practices</li> <li> You tested with the complete dataset</li> </ul>"},{"location":"en/guia-inicio/primer-ejercicio/#common-problems","title":"Common Problems","text":"Error: ModuleNotFoundError: No module named 'pandas' <p>Cause: Virtual environment not activated or dependencies not installed.</p> <p>Solution:</p> <pre><code># Activate virtual environment\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate      # Windows\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre> Git says: 'Your branch is behind origin/main' <p>Cause: Your local main branch is outdated.</p> <p>Solution:</p> <pre><code>git checkout main\ngit pull origin main\ngit checkout garcia-ejercicio-01\ngit merge main\n</code></pre> Cannot push: 'Permission denied' <p>Cause: Authentication issues with GitHub.</p> <p>Solution: Configure SSH keys or use a Personal Access Token.</p> <p>See: GitHub Authentication</p> PyCharm can't find the data <p>Cause: Incorrect relative path.</p> <p>Solution: Use relative paths from the project root:</p> <pre><code># \u2705 CORRECT\ncsv_path = 'datos/muestra_taxi.csv'\n\n# \u274c WRONG\ncsv_path = '../datos/muestra_taxi.csv'\n</code></pre>"},{"location":"en/guia-inicio/primer-ejercicio/#next-steps","title":"Next Steps","text":"<p>Once you have completed your first exercise:</p> <ul> <li>Sync Fork - Keep your fork up to date</li> <li>Course Roadmap - See all available exercises</li> <li>Useful Commands - Git Cheatsheet</li> </ul>"},{"location":"en/guia-inicio/roadmap/","title":"Course Roadmap","text":"<p>Complete overview of all exercises, technologies, and the recommended learning plan.</p>"},{"location":"en/guia-inicio/roadmap/#learning-levels","title":"Learning Levels","text":"<pre><code>graph TD\n    A[LEVEL 1: Fundamentals&lt;br/&gt;2-3 weeks] --&gt; B[LEVEL 2: Scaling Up&lt;br/&gt;3-4 weeks]\n    B --&gt; C[LEVEL 3: Real Big Data&lt;br/&gt;4-5 weeks]\n    C --&gt; D[LEVEL 4: Visualization&lt;br/&gt;3-4 weeks]\n\n    A1[SQLite&lt;br/&gt;Pandas&lt;br/&gt;Git/GitHub] --&gt; A\n    B1[Dask&lt;br/&gt;Parquet&lt;br/&gt;Optimization] --&gt; B\n    C1[PySpark&lt;br/&gt;Advanced SQL&lt;br/&gt;Pipelines] --&gt; C\n    D1[Dashboards&lt;br/&gt;APIs&lt;br/&gt;Deploy] --&gt; D</code></pre>"},{"location":"en/guia-inicio/roadmap/#level-1-fundamentals","title":"LEVEL 1: Fundamentals","text":"<p>Duration: 2-3 weeks | Difficulty: Basic</p>"},{"location":"en/guia-inicio/roadmap/#objectives","title":"Objectives","text":"<ul> <li>Master relational databases with SQLite</li> <li>Learn data analysis with Pandas</li> <li>Understand version control with Git/GitHub</li> </ul>"},{"location":"en/guia-inicio/roadmap/#technologies","title":"Technologies","text":"Technology Purpose Resources SQLite Embedded database Official docs Pandas In-memory data analysis Pandas docs Git Version control Git handbook"},{"location":"en/guia-inicio/roadmap/#exercises","title":"Exercises","text":""},{"location":"en/guia-inicio/roadmap/#exercise-01-data-loading-with-sqlite","title":"Exercise 01: Data Loading with SQLite","text":"<p>Details</p> <ul> <li>Estimated time: 2-3 hours</li> <li>Dataset: NYC Taxi (10MB sample)</li> <li>Level: Basic</li> </ul> <p>What you will learn:</p> <ul> <li>Load data from CSV into a database</li> <li>Basic SQL queries (SELECT, WHERE, GROUP BY)</li> <li>Optimization with indexes</li> <li>Export results</li> </ul> <p>Skills:</p> <ul> <li> Load CSV in chunks</li> <li> Create SQLite database</li> <li> Run SQL queries</li> <li> Create indexes</li> <li> Export results to CSV</li> </ul>"},{"location":"en/guia-inicio/roadmap/#exercise-02-cleaning-and-transformation","title":"Exercise 02: Cleaning and Transformation","text":"<p>Details</p> <ul> <li>Estimated time: 3-4 hours</li> <li>Dataset: NYC Taxi (dirty data)</li> <li>Level: Basic</li> </ul> <p>What you will learn:</p> <ul> <li>Detect and handle null values</li> <li>Identify outliers</li> <li>Data transformations</li> <li>Type validation</li> </ul>"},{"location":"en/guia-inicio/roadmap/#level-2-scaling-up","title":"LEVEL 2: Scaling Up","text":"<p>Duration: 3-4 weeks | Difficulty: Intermediate</p>"},{"location":"en/guia-inicio/roadmap/#objectives_1","title":"Objectives","text":"<ul> <li>Process data larger than your RAM</li> <li>Understand parallel processing</li> <li>Optimize performance</li> </ul>"},{"location":"en/guia-inicio/roadmap/#technologies_1","title":"Technologies","text":"Technology Purpose When to Use Dask Parallel processing Data &gt; RAM (5-100GB) Parquet Columnar format Efficient storage Optimization Performance Always"},{"location":"en/guia-inicio/roadmap/#exercises_1","title":"Exercises","text":""},{"location":"en/guia-inicio/roadmap/#exercise-03-processing-with-parquet-and-dask","title":"Exercise 03: Processing with Parquet and Dask","text":"<p>Details</p> <ul> <li>Estimated time: 4-5 hours</li> <li>Dataset: Full NYC Taxi (121MB)</li> <li>Level: Intermediate</li> </ul> <p>What you will learn:</p> <ul> <li>Why Parquet is better than CSV</li> <li>Parallel processing with Dask</li> <li>Lazy evaluation</li> <li>Memory optimization</li> </ul> <p>Format Comparison:</p> Metric CSV Parquet Disk size 121 MB 45 MB Read time 8.5 sec 1.2 sec Compression No Yes Data types Not preserved Preserved"},{"location":"en/guia-inicio/roadmap/#level-3-real-big-data","title":"LEVEL 3: Real Big Data","text":"<p>Duration: 4-5 weeks | Difficulty: Advanced</p>"},{"location":"en/guia-inicio/roadmap/#objectives_2","title":"Objectives","text":"<ul> <li>Master distributed processing</li> <li>Work with massive data (&gt;100GB)</li> <li>Build production pipelines</li> </ul>"},{"location":"en/guia-inicio/roadmap/#technologies_2","title":"Technologies","text":"Technology Purpose Scale PySpark Distributed processing &gt; 100GB Advanced SQL Complex queries Any size ETL Pipelines Automation Production"},{"location":"en/guia-inicio/roadmap/#exercises_2","title":"Exercises","text":""},{"location":"en/guia-inicio/roadmap/#exercise-04-complex-queries-with-pyspark","title":"Exercise 04: Complex Queries with PySpark","text":"<p>Details</p> <ul> <li>Estimated time: 5-6 hours</li> <li>Dataset: NYC Taxi + Weather (multiple sources)</li> <li>Level: Advanced</li> </ul> <p>What you will learn:</p> <ul> <li>Introduction to Spark</li> <li>Distributed DataFrames</li> <li>SQL in Spark</li> <li>Multi-source joins</li> <li>Data partitioning</li> </ul>"},{"location":"en/guia-inicio/roadmap/#exercise-06-complete-etl-pipeline","title":"Exercise 06: Complete ETL Pipeline","text":"<p>Details</p> <ul> <li>Estimated time: 10-12 hours</li> <li>Dataset: Multiple sources</li> <li>Level: Advanced</li> </ul> <p>Pipeline Architecture:</p> <pre><code>graph LR\n    A[CSV 100GB] --&gt;|Extract| B[Dask]\n    B --&gt;|Transform| C[PySpark]\n    C --&gt;|Load| D[Parquet 10GB]\n    D --&gt;|Serve| E[API Flask]\n    E --&gt;|Visualize| F[Dashboard]</code></pre>"},{"location":"en/guia-inicio/roadmap/#level-4-visualization-and-deploy","title":"LEVEL 4: Visualization and Deploy","text":"<p>Duration: 3-4 weeks | Difficulty: Advanced</p>"},{"location":"en/guia-inicio/roadmap/#objectives_3","title":"Objectives","text":"<ul> <li>Create professional dashboards</li> <li>Serve data via API</li> <li>Deploy to production</li> </ul>"},{"location":"en/guia-inicio/roadmap/#technologies_3","title":"Technologies","text":"Technology Purpose Use Flask Web backend APIs and dashboards Chart.js Visualizations Interactive charts Docker Containers Deploy"},{"location":"en/guia-inicio/roadmap/#exercises_3","title":"Exercises","text":""},{"location":"en/guia-inicio/roadmap/#exercise-05-interactive-dashboard","title":"Exercise 05: Interactive Dashboard","text":"<p>Details</p> <ul> <li>Estimated time: 8-10 hours</li> <li>Project: NYC Taxi EDA Dashboard</li> <li>Level: Advanced</li> </ul> <p>Features:</p> <ul> <li> Visualization of 10M+ records</li> <li> Dynamic filters by date/time</li> <li> Heat maps</li> <li> Trend analysis</li> </ul> <p>Tech Stack:</p> <pre><code>Frontend: HTML + Bootstrap + Chart.js\nBackend:  Flask + Pandas/Dask\nData:     SQLite/Parquet\nDeploy:   Docker\n</code></pre>"},{"location":"en/guia-inicio/roadmap/#recommended-study-plan","title":"Recommended Study Plan","text":""},{"location":"en/guia-inicio/roadmap/#for-beginners-10-12-weeks","title":"For Beginners (10-12 weeks)","text":"<pre><code>gantt\n    title Study Plan - Beginners\n    dateFormat YYYY-MM-DD\n    section Fundamentals\n    Exercise 01           :2024-01-01, 1w\n    Exercise 02           :2024-01-08, 1w\n    Fundamentals Practice :2024-01-15, 1w\n    section Scaling Up\n    Exercise 03           :2024-01-22, 2w\n    Personal Project      :2024-02-05, 1w\n    section Big Data\n    Exercise 04           :2024-02-12, 2w\n    Exercise 06           :2024-02-26, 2w\n    section Visualization\n    Exercise 05           :2024-03-11, 2w</code></pre> <p>Dedication: 10-15 hours/week</p>"},{"location":"en/guia-inicio/roadmap/#for-intermediate-learners-6-8-weeks","title":"For Intermediate Learners (6-8 weeks)","text":"<p>Recommendation</p> <p>If you already know Python and Pandas, you can start directly at LEVEL 2.</p> <p>Dedication: 8-10 hours/week</p>"},{"location":"en/guia-inicio/roadmap/#for-advanced-learners-4-5-weeks","title":"For Advanced Learners (4-5 weeks)","text":"<p>Recommendation</p> <p>If you have already worked with Big Data, focus on the PySpark exercises and the final project.</p> <p>Dedication: 5-8 hours/week</p>"},{"location":"en/guia-inicio/roadmap/#technologies-by-exercise","title":"Technologies by Exercise","text":"Exercise SQLite Pandas Dask PySpark Flask Level 01 - SQLite Yes Yes - - - Basic 02 - Cleaning - Yes - - - Basic 03 - Dask - Yes Yes - - Intermediate 04 - PySpark - - Yes Yes - Advanced 05 - Dashboard Yes Yes - - Yes Advanced 06 - Pipeline - - Yes Yes Yes Advanced"},{"location":"en/guia-inicio/roadmap/#technology-comparison","title":"Technology Comparison","text":""},{"location":"en/guia-inicio/roadmap/#when-to-use-each-tool","title":"When to use each tool?","text":"<pre><code>graph TD\n    A[Do you have data?] --&gt; B{How large is it?}\n    B --&gt;|&lt; 5GB| C[Pandas]\n    B --&gt;|5-100GB| D[Dask]\n    B --&gt;|&gt; 100GB| E[PySpark]\n\n    C --&gt; F{Need a DB?}\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt;|Yes, local| G[SQLite]\n    F --&gt;|Yes, production| H[PostgreSQL/MySQL]\n    F --&gt;|No| I[Parquet]</code></pre>"},{"location":"en/guia-inicio/roadmap/#comparison-table","title":"Comparison Table","text":"Data Size Tool Processing Time RAM Required &lt; 1GB Pandas Seconds 2-4x data size 1-5GB Pandas Minutes 2-4x data size 5-50GB Dask Minutes Any RAM 50-500GB Dask/PySpark Minutes-Hours Any RAM &gt; 500GB PySpark Hours Cluster"},{"location":"en/guia-inicio/roadmap/#certification-and-evaluation","title":"Certification and Evaluation","text":""},{"location":"en/guia-inicio/roadmap/#for-in-person-course-students","title":"For In-Person Course Students","text":"<ul> <li> 230-hour certificate</li> <li> Automatic evaluation via PROMPTS.md</li> <li> Integrative final project</li> <li> Direct instructor support</li> </ul>"},{"location":"en/guia-inicio/roadmap/#for-self-learners","title":"For Self-Learners","text":"<ul> <li> Project portfolio on GitHub</li> <li> Code reviewable by employers</li> <li> Experience with real data</li> <li> Learn at your own pace</li> </ul> <p>Your GitHub Is Your Certificate</p> <p>Employers value seeing your code and projects more than a PDF. Make sure to:</p> <ul> <li>Make clear and professional commits</li> <li>Document your code</li> <li>Complete exercises with quality</li> <li>Add a personalized README to your fork</li> </ul>"},{"location":"en/guia-inicio/roadmap/#additional-resources","title":"Additional Resources","text":""},{"location":"en/guia-inicio/roadmap/#official-documentation","title":"Official Documentation","text":"<ul> <li>Pandas Documentation</li> <li>Dask Documentation</li> <li>PySpark Documentation</li> <li>SQLite Tutorial</li> </ul>"},{"location":"en/guia-inicio/roadmap/#complementary-courses","title":"Complementary Courses","text":"<ul> <li>Python for Data Science</li> <li>Big Data with Spark</li> <li>SQL for Data Science</li> </ul>"},{"location":"en/guia-inicio/roadmap/#communities","title":"Communities","text":"<ul> <li>r/datascience</li> <li>Stack Overflow - pandas</li> <li>Dask Discourse</li> </ul>"},{"location":"en/guia-inicio/roadmap/#next-steps","title":"Next Steps","text":"<p>Now that you know the complete roadmap:</p> <ol> <li>Install Tools - If you don't have them yet</li> <li>Your First Exercise - Start practicing</li> <li>Fork and Clone - Set up your work environment</li> </ol>"}]}