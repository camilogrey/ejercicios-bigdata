"""
spark-apps.py - ETL y An√°lisis de Clustering para dataset QoG
Objetivo: Analizar 20 pa√≠ses espec√≠ficos del a√±o 2019 usando K-Means
"""
import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, when, lit, log
from pyspark.sql.types import DoubleType
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib

matplotlib.use('Agg')  # ¬°IMPORTANTE! Para servidor sin display
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

# 1. LISTA DE 20 PA√çSES ESPEC√çFICOS (del enunciado)
PAISES_ESTUDIO = [
    # Am√©rica
    "Canada", "Uruguay", "Haiti", "Chile", "Cuba",
    # Europa
    "Norway", "Poland", "Moldova", "Netherlands", "Belarus",
    # Asia
    "Singapore", "Kuwait", "Japan", "Viet Nam", "India",
    # √Åfrica
    "Botswana", "Rwanda", "South Africa", "Ethiopia", "Mauritius"
]

# 2. VARIABLES PARA EL MODELO (5 variables finales)
VARIABLES_MODELO = [
    'vdem_polyarchy',  # Calidad democr√°tica
    'ti_cpi',  # √çndice de percepci√≥n de corrupci√≥n (Transparency International)
    'wdi_expedu',  # Gasto en educaci√≥n (% del PIB)
    'wdi_gdpcapcon2015',  # PIB per c√°pita (para transformaci√≥n logar√≠tmica)
    'vdem_libdem'  # Democracia liberal (para calcular Liberal_gap)
]

# 3. A√ëO DE ESTUDIO
ANIO_ESTUDIO = 2019

# 4. CONFIGURACI√ìN DE BASE DE DATOS (¬°CORREGIDO! usa 'postgres' no 'localhost')
POSTGRES_CONFIG = {
    'host': 'postgres',  # ¬°CORRECCI√ìN! Nombre del servicio en docker-compose
    'port': 5432,
    'database': 'qog_db',
    'user': 'spark_user',
    'password': 'spark_pass',
    'table_raw': 'qog_raw',
    'table_processed': 'qog_processed',
    'table_results': 'clustering_results'
}

# 5. RUTAS PARA GUARDAR ARCHIVOS EN CONTENEDOR
OUTPUT_PATH = "/opt/spark-apps/"


# ============================================================================
# FUNCI√ìN PRINCIPAL - PIPELINE ETL
# ============================================================================

def crear_spark_session():
    """Crea y configura la SparkSession"""
    spark = SparkSession.builder \
        .appName("QoG_Clustering_Analysis") \
        .master("spark://spark-master:7077") \
        .config("spark.executor.memory", "2g") \
        .config("spark.driver.memory", "2g") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    print("‚úì SparkSession creada exitosamente")
    return spark



def cargar_datos(spark, filepath):
    """Carga el dataset QoG desde CSV"""
    print(f"\nüìÇ Cargando dataset desde: {filepath}")

    # Leer CSV con inferencia de schema
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(filepath)

    print(f"‚úì Dataset cargado: {df.count()} filas, {len(df.columns)} columnas")

    # Mostrar esquema para debugging
    print("Esquema del dataset:")
    df.printSchema()

    return df


def filtrar_datos(df):
    """Filtra los 20 pa√≠ses espec√≠ficos para el a√±o 2019"""
    print(f"\nüîç Filtrando datos para {len(PAISES_ESTUDIO)} pa√≠ses en {ANIO_ESTUDIO}")

    # Asegurar que la columna de a√±o sea num√©rica
    df = df.withColumn("year", col("year").cast("int"))

    # Filtrar por a√±o y pa√≠ses
    df_filtrado = df.filter(
        (col("year") == ANIO_ESTUDIO) &
        (col("cname").isin(PAISES_ESTUDIO))
    )

    # Seleccionar columnas relevantes
    columnas_necesarias = ['cname', 'ccodealp', 'year'] + VARIABLES_MODELO
    columnas_disponibles = [c for c in columnas_necesarias if c in df.columns]
    df_filtrado = df_filtrado.select(columnas_disponibles)

    print(f"‚úì Datos filtrados: {df_filtrado.count()} pa√≠ses encontrados")
    print(f"  Columnas seleccionadas: {columnas_disponibles}")

    # Mostrar pa√≠ses encontrados
    if df_filtrado.count() > 0:
        paises_encontrados = df_filtrado.select("cname").distinct().collect()
        print(f"  Pa√≠ses encontrados: {[row.cname for row in paises_encontrados]}")
    else:
        print("  ‚ö†Ô∏è  ¬°ADVERTENCIA! No se encontraron datos para los pa√≠ses seleccionados")
        # Mostrar pa√≠ses disponibles para debugging
        paises_disponibles = df.filter(col("year") == ANIO_ESTUDIO).select("cname").distinct().limit(10).collect()
        print(f"  Algunos pa√≠ses disponibles en {ANIO_ESTUDIO}: {[row.cname for row in paises_disponibles]}")

    return df_filtrado


def ingenieria_variables(df):
    """Crea variables derivadas para el an√°lisis"""
    print("\n‚öôÔ∏è  Aplicando ingenier√≠a de variables...")

    # Verificar que las columnas necesarias existan
    columnas_requeridas = ['vdem_polyarchy', 'vdem_libdem', 'wdi_gdpcapcon2015']
    columnas_faltantes = [c for c in columnas_requeridas if c not in df.columns]

    if columnas_faltantes:
        print(f"  ‚ö†Ô∏è  Columnas faltantes: {columnas_faltantes}")
        print(f"  Columnas disponibles: {df.columns}")
        raise ValueError(f"Columnas requeridas faltantes: {columnas_faltantes}")

    # 1. Calcular Liberal_gap = (vdem_polyarchy - vdem_libdem)
    df = df.withColumn(
        "Liberal_gap",
        col("vdem_polyarchy") - col("vdem_libdem")
    )

    # 2. Transformaci√≥n logar√≠tmica del PIB (agregar 1 para evitar log(0))
    df = df.withColumn(
        "wdi_gdpcapcon2015_log",
        log(col("wdi_gdpcapcon2015") + 1)
    )

    # 3. Reemplazar valores null
    df = df.fillna(0, subset=["Liberal_gap", "ti_cpi", "wdi_expedu"])

    # 4. Seleccionar variables finales para el modelo
    variables_finales = [
        'cname', 'ccodealp', 'year',
        'vdem_polyarchy',
        'ti_cpi',
        'wdi_expedu',
        'wdi_gdpcapcon2015_log',
        'Liberal_gap'
    ]

    df_final = df.select(variables_finales)

    # 5. Eliminar filas con valores nulos en variables cr√≠ticas
    df_final = df_final.dropna(
        subset=['vdem_polyarchy', 'ti_cpi', 'wdi_gdpcapcon2015_log']
    )

    print("‚úì Variables derivadas creadas:")
    print(f"  - Liberal_gap = vdem_polyarchy - vdem_libdem")
    print(f"  - PIB transformado logar√≠tmicamente (log(PIB+1))")
    print(f"  - Datos finales: {df_final.count()} filas, {len(df_final.columns)} columnas")

    # Mostrar estad√≠sticas descriptivas
    print("  Estad√≠sticas descriptivas:")
    df_final.select('vdem_polyarchy', 'ti_cpi', 'wdi_gdpcapcon2015_log').describe().show()

    return df_final


def guardar_postgres(df, table_name):
    """Guarda DataFrame de Spark en PostgreSQL"""
    print(f"\nüíæ Guardando datos en PostgreSQL ({table_name})...")

    # Convertir a Pandas para facilitar la carga en PostgreSQL
    pandas_df = df.toPandas()

    # Crear conexi√≥n a PostgreSQL
    connection_str = (
        f"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}"
        f"@{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}"
        f"/{POSTGRES_CONFIG['database']}"
    )

    engine = create_engine(connection_str)

    # Verificar conexi√≥n primero
    try:
        with engine.connect() as conn:
            print(f"  ‚úÖ Conexi√≥n a PostgreSQL exitosa en {POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}")
    except Exception as e:
        print(f"  ‚ùå Error conectando a PostgreSQL: {e}")
        print(f"  Cadena de conexi√≥n: {connection_str.replace(POSTGRES_CONFIG['password'], '*****')}")
        raise

    # Guardar en PostgreSQL
    pandas_df.to_sql(
        table_name,
        engine,
        if_exists='replace',
        index=False
    )

    print(f"‚úì Datos guardados en tabla '{table_name}' ({len(pandas_df)} filas)")
    return pandas_df


# ============================================================================
# AN√ÅLISIS DE CLUSTERING (BLOQUE C)
# ============================================================================

def preparar_datos_ml(pandas_df):
    """Prepara datos para Machine Learning"""
    print("\nü§ñ Preparando datos para an√°lisis de clustering...")

    # 1. Variables para el modelo (excluyendo identificadores)
    variables_ml = [
        'vdem_polyarchy',
        'ti_cpi',
        'wdi_expedu',
        'wdi_gdpcapcon2015_log',
        'Liberal_gap'
    ]

    # Verificar que tenemos datos suficientes
    if len(pandas_df) < 3:
        raise ValueError(f"Insuficientes datos para clustering. Solo {len(pandas_df)} muestras disponibles.")

    # 2. Extraer matriz de caracter√≠sticas
    X = pandas_df[variables_ml].values

    # 3. Normalizar con StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print(f"‚úì Datos preparados: {X_scaled.shape[0]} muestras, {X_scaled.shape[1]} caracter√≠sticas")
    return X_scaled, variables_ml, pandas_df


def metodo_del_codo(X_scaled, max_k=8):
    """Aplica el m√©todo del codo para determinar K √≥ptimo y genera gr√°fico INTERACTIVO con Plotly"""
    print("\nüìà Aplicando m√©todo del codo...")

    inertias = []
    K_range = range(1, min(max_k, len(X_scaled)) + 1)

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)

    # Crear figura con Plotly
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=list(K_range),
        y=inertias,
        mode='lines+markers',
        marker=dict(size=10, color='blue'),
        line=dict(width=2, color='blue'),
        name='Inercia'
    ))

    fig.update_layout(
        title='M√©todo del Codo para Determinar K √ìptimo',
        xaxis_title='N√∫mero de Clusters (K)',
        yaxis_title='Inercia',
        hovermode='x',
        template='plotly_white',
        width=800,
        height=500
    )

    # Guardar versi√≥n EST√ÅTICA (PNG) - con matplotlib
    plt.figure(figsize=(10, 6))
    plt.plot(K_range, inertias, 'bo-')
    plt.xlabel('N√∫mero de Clusters (K)')
    plt.ylabel('Inercia')
    plt.title('M√©todo del Codo para Determinar K √ìptimo')
    plt.grid(True, alpha=0.3)
    output_file_png = os.path.join(OUTPUT_PATH, 'elbow_method.png')
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Gr√°fico est√°tico guardado como '{output_file_png}'")

    # Guardar versi√≥n INTERACTIVA (HTML) con Plotly
    output_file_html = os.path.join(OUTPUT_PATH, 'elbow_method_interactivo.html')
    fig.write_html(output_file_html)
    print(f"‚úì Gr√°fico INTERACTIVO guardado como '{output_file_html}'")

    # Sugerir K √≥ptimo
    if len(inertias) > 3:
        diffs = np.diff(inertias)
        if len(diffs) > 2:
            diffs_ratios = diffs[1:] / diffs[:-1]
            k_optimo = np.argmin(diffs_ratios) + 2
            k_optimo = max(2, min(k_optimo, len(X_scaled) // 2))
        else:
            k_optimo = 2
    else:
        k_optimo = 2

    print(f"  K √≥ptimo sugerido: {k_optimo}")
    return k_optimo, inertias


def aplicar_kmeans(X_scaled, n_clusters):
    """Aplica algoritmo K-Means"""
    print(f"\nüéØ Aplicando K-Means con K={n_clusters}...")

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled)

    print(f"‚úì K-Means completado - {n_clusters} clusters creados")
    print(f"  Inercia: {kmeans.inertia_:.2f}")

    return kmeans, clusters


def aplicar_pca_y_visualizar(X_scaled, clusters, pandas_df, kmeans):
    """Aplica PCA y crea visualizaci√≥n 2D INTERACTIVA con Plotly"""
    print("\nüìä Aplicando PCA y creando visualizaci√≥n...")

    # 1. Aplicar PCA para reducir a 2 dimensiones
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    # 2. Crear DataFrame para visualizaci√≥n
    viz_df = pandas_df.copy()
    viz_df['cluster'] = clusters
    viz_df['PC1'] = X_pca[:, 0]
    viz_df['PC2'] = X_pca[:, 1]
    viz_df['cluster_str'] = 'Cluster ' + viz_df['cluster'].astype(str)

    # Calcular varianza explicada
    var_exp = pca.explained_variance_ratio_ * 100

    # 3. Crear gr√°fico con Plotly
    fig = px.scatter(
        viz_df,
        x='PC1',
        y='PC2',
        color='cluster_str',
        text='cname',
        hover_data={
            'cname': True,
            'cluster': True,
            'PC1': ':.3f',
            'PC2': ':.3f',
            'vdem_polyarchy': ':.3f',
            'ti_cpi': ':.1f',
            'wdi_gdpcapcon2015_log': ':.2f',
            'Liberal_gap': ':.3f'
        },
        title=f'Clustering de Pa√≠ses (K={len(np.unique(clusters))})<br>PCA de 5 variables a 2 dimensiones',
        labels={
            'PC1': f'PC1 ({var_exp[0]:.1f}% varianza)',
            'PC2': f'PC2 ({var_exp[1]:.1f}% varianza)',
            'cluster_str': 'Cluster',
            'cname': 'Pa√≠s'
        }
    )

    # Personalizar el gr√°fico
    fig.update_traces(
        textposition='top center',
        marker=dict(size=12, line=dict(width=1, color='DarkSlateGrey'))
    )

    fig.update_layout(
        hovermode='closest',
        template='plotly_white',
        width=900,
        height=700,
        showlegend=True,
        title_x=0.5
    )

    # A√±adir anotaci√≥n con varianza total
    fig.add_annotation(
        xref='paper', yref='paper',
        x=0.5, y=-0.15,
        text=f'Varianza explicada total: {pca.explained_variance_ratio_.sum()*100:.1f}%',
        showarrow=False,
        font=dict(size=12, color='gray')
    )

    # Guardar versi√≥n EST√ÅTICA (PNG) - con matplotlib
    plt.figure(figsize=(14, 10))
    scatter = plt.scatter(
        X_pca[:, 0], X_pca[:, 1],
        c=clusters, cmap='viridis', s=200, alpha=0.8,
        edgecolors='black', linewidth=1
    )
    for i, row in viz_df.iterrows():
        plt.annotate(row['cname'], (row['PC1'], row['PC2']),
                    fontsize=9, ha='center', va='center', fontweight='bold')
    plt.xlabel(f'PC1 ({var_exp[0]:.1f}%)', fontsize=12)
    plt.ylabel(f'PC2 ({var_exp[1]:.1f}%)', fontsize=12)
    plt.title(f'Clustering de Pa√≠ses (K={len(np.unique(clusters))})', fontsize=14)
    plt.colorbar(scatter, label='Cluster')
    plt.grid(True, alpha=0.3)
    plt.figtext(0.5, 0.01, f'Varianza total: {pca.explained_variance_ratio_.sum()*100:.1f}%',
                ha='center', fontsize=10, style='italic')

    output_file_png = os.path.join(OUTPUT_PATH, 'clustering_pca.png')
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Gr√°fico est√°tico guardado como: {output_file_png}")

    # Guardar versi√≥n INTERACTIVA (HTML) con Plotly
    output_file_html = os.path.join(OUTPUT_PATH, 'clustering_pca_interactivo.html')
    fig.write_html(output_file_html)
    print(f"‚úì Gr√°fico INTERACTIVO guardado como: {output_file_html}")

    print(f"  Varianza explicada: PC1={var_exp[0]:.1f}%, PC2={var_exp[1]:.1f}%, "
          f"Total={pca.explained_variance_ratio_.sum()*100:.1f}%")

    return viz_df, X_pca, pca


def analizar_clusters(viz_df, variables_ml):
    """Analiza y describe los clusters encontrados"""
    print("\nüîç Analizando caracter√≠sticas de cada cluster...")

    # Calcular promedios por cluster
    cluster_stats = viz_df.groupby('cluster')[variables_ml].mean()

    # A√±adir conteo de pa√≠ses
    cluster_stats['n_paises'] = viz_df.groupby('cluster').size()
    cluster_stats['paises'] = viz_df.groupby('cluster')['cname'].apply(list)

    print("\nüìä ESTAD√çSTICAS POR CLUSTER:")
    print("=" * 80)

    for cluster_id in sorted(cluster_stats.index):
        stats = cluster_stats.loc[cluster_id]

        print(f"\nCLUSTER {cluster_id} ({stats['n_paises']} pa√≠ses):")
        print(f"Pa√≠ses: {', '.join(stats['paises'])}")
        print("-" * 40)

        # Interpretar caracter√≠sticas
        if stats['vdem_polyarchy'] > 0.7:
            demo_level = "Democracia alta"
        elif stats['vdem_polyarchy'] > 0.4:
            demo_level = "Democracia media"
        else:
            demo_level = "Democracia baja"

        if stats['ti_cpi'] > 70:
            corrup_level = "Baja corrupci√≥n"
        elif stats['ti_cpi'] > 40:
            corrup_level = "Corrupci√≥n media"
        else:
            corrup_level = "Alta corrupci√≥n"

        print(f"‚Ä¢ {demo_level} (vdem_polyarchy: {stats['vdem_polyarchy']:.3f})")
        print(f"‚Ä¢ {corrup_level} (ti_cpi: {stats['ti_cpi']:.1f})")
        print(f"‚Ä¢ PIB per c√°pita (log): {stats['wdi_gdpcapcon2015_log']:.2f}")
        print(f"‚Ä¢ Gasto en educaci√≥n: {stats['wdi_expedu']:.2f}% del PIB")
        print(f"‚Ä¢ Liberal gap: {stats['Liberal_gap']:.3f}")

    print("\n" + "=" * 80)

    # Guardar estad√≠sticas en CSV
    output_file = os.path.join(OUTPUT_PATH, 'cluster_statistics.csv')
    cluster_stats.to_csv(output_file)
    print(f"‚úì Estad√≠sticas guardadas en '{output_file}'")

    return cluster_stats


# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def main():
    """Funci√≥n principal que ejecuta todo el spark-apps"""
    print("=" * 80)
    print("üöÄ INICIANDO PIPELINE ETL + AN√ÅLISIS DE CLUSTERING")
    print("=" * 80)

    try:
        # ====================================================================
        # BLOQUE B: PIPELINE ETL
        # ====================================================================
        print("\n" + "=" * 80)
        print("üì¶ BLOQUE B: PIPELINE ETL")
        print("=" * 80)

        # 1. Crear SparkSession
        spark = crear_spark_session()

        # 2. Cargar datos
        filepath = "/opt/data/qog.csv"  # Ruta dentro del contenedor
        df_raw = cargar_datos(spark, filepath)

        # 3. Filtrar datos
        df_filtrado = filtrar_datos(df_raw)

        if df_filtrado.count() == 0:
            print("\n‚ùå No se encontraron datos para los pa√≠ses seleccionados.")
            print("   Verifica que el archivo qog.csv tenga los pa√≠ses correctos.")
            spark.stop()
            return

        # 4. Ingenier√≠a de variables
        df_final = ingenieria_variables(df_filtrado)

        # 5. Guardar en PostgreSQL
        pandas_df = guardar_postgres(df_final, POSTGRES_CONFIG['table_processed'])

        # ====================================================================
        # BLOQUE C: AN√ÅLISIS DE CLUSTERING
        # ====================================================================
        print("\n" + "=" * 80)
        print("üî¨ BLOQUE C: AN√ÅLISIS DE CLUSTERING")
        print("=" * 80)

        # 6. Preparar datos para ML
        X_scaled, variables_ml, pandas_df = preparar_datos_ml(pandas_df)

        # 7. M√©todo del codo
        k_optimo, inertias = metodo_del_codo(X_scaled, max_k=min(8, len(pandas_df)))
        print(f"\nüí° K seleccionado: {k_optimo}")

        # 8. Aplicar K-Means
        kmeans, clusters = aplicar_kmeans(X_scaled, n_clusters=k_optimo)

        # 9. PCA y visualizaci√≥n
        viz_df, X_pca, pca = aplicar_pca_y_visualizar(X_scaled, clusters, pandas_df, kmeans)

        # 10. Analizar clusters
        cluster_stats = analizar_clusters(viz_df, variables_ml)

        # 11. Guardar resultados finales
        results_file = os.path.join(OUTPUT_PATH, 'clustering_results.csv')
        viz_df.to_csv(results_file, index=False)
        print(f"‚úì Resultados guardados en '{results_file}'")

        # Guardar en PostgreSQL
        spark_df_results = spark.createDataFrame(viz_df)
        guardar_postgres(spark_df_results, POSTGRES_CONFIG['table_results'])

        # ====================================================================
        # RESULTADOS FINALES
        # ====================================================================
        print("\n" + "=" * 80)
        print("‚úÖ PIPELINE COMPLETADO EXITOSAMENTE")
        print("=" * 80)

        print(f"\nüìà RESULTADOS:")
        print(f"‚Ä¢ Pa√≠ses analizados: {len(pandas_df)}")
        print(f"‚Ä¢ Clusters identificados: {len(np.unique(clusters))}")
        print(f"‚Ä¢ Varianza explicada por PCA: {pca.explained_variance_ratio_.sum():.1%}")
        print(f"‚Ä¢ Archivos generados en {OUTPUT_PATH}:")
        print(f"  1. elbow_method.png - M√©todo del codo")
        print(f"  2. clustering_pca.png - Visualizaci√≥n 2D de clusters")
        print(f"  3. clustering_results.csv - Resultados completos")
        print(f"  4. cluster_statistics.csv - Estad√≠sticas por cluster")

        print(f"\nüóÑÔ∏è  Datos guardados en PostgreSQL ({POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}):")
        print(f"  ‚Ä¢ Tabla '{POSTGRES_CONFIG['table_processed']}': Datos procesados")
        print(f"  ‚Ä¢ Tabla '{POSTGRES_CONFIG['table_results']}': Resultados clustering")

        # Detener Spark
        spark.stop()
        print("\nüéâ ¬°An√°lisis completado! Revisa los archivos generados.")

    except Exception as e:
        print(f"\n‚ùå ERROR en el spark-apps: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# ============================================================================
# EJECUCI√ìN
# ============================================================================

if __name__ == "__main__":
    main()